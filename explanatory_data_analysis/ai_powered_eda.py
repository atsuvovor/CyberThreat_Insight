# -*- coding: utf-8 -*-
"""ai_powered_eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nw9bTo5ocFBPd1tdagegiZXrqz_Fjt0R

## **CyberThreat-Insight**  
**Anomalous Behavior Detection in Cybersecurity Analytics using Generative AI**

**Toronto, Septeber 08 2025**  
**Autor : Atsu Vovor**
>Master of Management in Artificial Intelligence    
>Consultant Data Analytics Specialist | Machine Learning |  
Data science | Quantitative Analysis |French & English Bilingual

### Exploratory Data Analysis (EDA) Code Explanation:

This code performs Exploratory Data Analysis (EDA) on a cybersecurity dataset to understand its structure, distributions, and relationships before building threat detection models.

**1. Imports:**
- Imports necessary libraries: `pandas` for data manipulation, `numpy` for numerical operations, `matplotlib.pyplot` and `seaborn` for plotting, `matplotlib.dates` for date formatting, and `MinMaxScaler` for feature scaling.
- Imports `display` from `IPython.display` for formatted DataFrame output in environments like Colab.
- Imports and mounts Google Drive to access the dataset file.

**2. `normalize_numerical_features(p_df)` Function:**
- Takes a DataFrame `p_df` as input.
- Initializes a `MinMaxScaler`.
- Creates a copy of the input DataFrame.
- Applies Min-Max scaling to the numerical columns, transforming values to a range between 0 and 1.
- Returns a new DataFrame with normalized features.

**3. `plot_numerical_features_daily_values(...)` Function:**
- Plots daily trends for specified numerical features against a date column.
- Takes DataFrame, date column name, list of feature columns, and subplot grid dimensions (rows, cols) as input.
- Creates subplots and flattens the axes array for easy iteration.
- Iterates through feature columns and plots each as a time series with markers.
- Sets titles, labels, and grid for each subplot.
- Formats the x-axis to display dates clearly and prevents overlapping labels.
- Hides any unused subplots.
- Displays the plot.

**4. `daily_distribution_of_activity_features_pipeline(df)` Function:**
- A pipeline to visualize the daily distribution of numerical features, both non-normalized and normalized.
- Takes a DataFrame `df` as input.
- Determines the number of rows and columns needed for the subplots based on the number of features.
- Calls `plot_numerical_features_daily_values` twice: once for the original data and once for the normalized data returned by `normalize_numerical_features`.

**5. `plot_histograms(df)` Function:**
- Plots histograms for all features in the DataFrame.
- Defines a color palette for categorical 'Risk Level' and 'Threat Level' features.
- Calculates subplot grid dimensions.
- Iterates through features, plotting histograms. For categorical features with risk levels, it uses the defined palette. For numerical features, it plots a standard histogram with a kernel density estimate (KDE) overlay.
- Calculates and displays basic statistics (mean, standard deviation, skewness, kurtosis) on the plot for numerical features.
- Hides unused subplots.
- Adjusts layout and displays the plot.

**6. `plot_boxplots(df)` Function:**
- Plots boxplots for all features in the DataFrame.
- Defines a color palette for categorical 'Risk Level' and 'Threat Level' features.
- Calculates subplot grid dimensions.
- Iterates through features, plotting boxplots. For categorical features with risk levels, it uses the defined palette. For numerical features, it plots a standard boxplot.
- Calculates and displays basic statistics (mean, standard deviation, skewness, kurtosis) below the plot for numerical features.
- Hides unused subplots.
- Adjusts layout and displays the plot.

**7. `visualize_form_of_activity_features_distribution(df)` Function:**
- A master function to visualize the distribution of activity features using histograms and boxplots.
- Sets the seaborn plot style to "whitegrid".
- Calls `plot_histograms` and `plot_boxplots` to generate the plots.

**8. `plot_scatter(...)` Function:**
- Creates a scatter plot on a specified subplot axis.
- Takes subplot axes, x and y column names, hue column name, DataFrame, color palette, title, axis labels, legend title, and the index of the target axis as input.
- Generates a scatter plot using `seaborn.scatterplot`.
- Sets the title, labels, and legend for the subplot.

**9. `plot_correlation_heatmap(axes, df, features, ax_index)` Function:**
- Creates a heatmap showing the correlation between selected numerical features on a specified subplot axis.
- Takes subplot axes, DataFrame, list of features, and the index of the target axis as input.
- Selects only numerical features from the provided list.
- Calculates the correlation matrix for the numerical features.
- Generates a heatmap using `seaborn.heatmap` with annotations showing correlation values.
- Sets the title for the subplot.

**10. `combines_user_activities_scatter_plots_and_heatmap(scatter_df, df)` Function:**
- Combines scatter plots and a correlation heatmap into a single figure with multiple subplots.
- Takes two DataFrames as input: `scatter_df` for scatter plots and `df` for the heatmap.
- Creates a figure with 1 row and 3 columns for the subplots.
- Calls `plot_scatter` twice to generate two scatter plots (Session Duration vs Data Transfer, Login Attempts vs Data Transfer), using 'User Location' for hue.
- Calls `plot_correlation_heatmap` to generate the correlation heatmap for numerical features in the `df`.
- Adjusts layout and displays the combined figure.

**11. `explaratory_data_analysis_pipeline()` Function:**
- The main function that orchestrates the EDA process.
- Defines the file path to the dataset in Google Drive.
- Defines lists of features for general EDA and for activity-related visualizations.
- Loads the dataset using `pd.read_csv`.
- Defines the reporting frequency for temporal analysis (e.g., 'Quarter').
- Creates a copy of the DataFrame with selected EDA features and sets 'Date Reported' as the index.
- Performs frequency-based grouping and aggregation (mean) on the temporal data.
- Converts the index back to timestamp for plotting.
- Displays the frequency-based aggregated DataFrame.
- Creates copies of the DataFrame for activity features and scatter plot features.
- Calls `daily_distribution_of_activity_features_pipeline` to visualize temporal trends.
- Calls `visualize_form_of_activity_features_distribution` to plot histograms and boxplots.
- Calls `combines_user_activities_scatter_plots_and_heatmap` to display scatter plots and the correlation heatmap.
- Returns the frequency-based aggregated DataFrame.

**12. `if __name__ == "__main__":` Block:**
- This block ensures that the `explaratory_data_analysis_pipeline()` function is executed only when the script is run directly (not when imported as a module).
- Calls the main EDA pipeline function and stores the returned frequency-based DataFrame in the `real_world_normal_and_anomalous_df` variable.
"""

!pip install faiss-cpu sentence-transformers openai rapidfuzz
!pip install streamlit

#Exploratory Data Analysis (EDA) Code
# --------------------------
#   Necessary Imports
# --------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates
import os # Import the os module for path manipulation
from sklearn.preprocessing import MinMaxScaler
from IPython.display import display

# Function to normalize numerical features using Min-Max Scaling
def normalize_numerical_features(p_df):
    scaler = MinMaxScaler()
    p_df_daily = p_df.copy()
    # Apply Min-Max scaling to the DataFrame
    df_normalized = pd.DataFrame(scaler.fit_transform(p_df_daily), columns=p_df_daily.columns.to_list(), index=p_df_daily.index)
    return df_normalized


#------------------------------------------------------------------
# Function to plot daily values of numerical features
def plot_numerical_features_daily_values(df, date_column, feature_columns, rows, cols):

    fig, axes = plt.subplots(rows, cols, figsize=(16, 8))
    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration

    # Iterate through features and plot each one
    for i, column in enumerate(feature_columns):
        ax = axes[i]
        ax.plot(df.index, df[column], marker='o', label=column, color='b')
        ax.set_title(column, fontsize=10)
        ax.set_xlabel("Date Reported", fontsize=8)
        ax.set_ylabel(column, fontsize=8)
        ax.grid(True)
        ax.legend(fontsize=8)

        # Format x-axis to prevent overlapping dates
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=100))  # Show every 100 days
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha="right", fontsize=8)

    # Hide any unused subplots
    for j in range(len(feature_columns), len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()


#------------------------------------------------------------------

# Pipeline to plot daily distribution of numerical features (both non-normalized and normalized)
def daily_distribution_of_activity_features_pipeline(df):
    """
    Pipeline to plot daily distribution of numerical features.
    """
    features = df.columns.tolist()
    n_features = len(features)
    rows = int(n_features/4)
    cols =  int(n_features/2)

    print("Non normalized daily distribution")
    plot_numerical_features_daily_values(df, "Date Reported", features, rows, cols)
    #plot_numerical_features_daily_values(df)
    print("Normalized daily distribution")
    df_normalized = normalize_numerical_features(df)
    #plot_numerical_features_daily_values(df_normalized)
    plot_numerical_features_daily_values(df_normalized, "Date Reported", features, rows, cols)
#-------------------------------------------------------------------------

# Function to plot histograms for features
def plot_histograms(df):
    """
    Plots histograms for all features in the list with risk level and displays basic statistics.
    """
    # Define the risk palette for categorical features
    risk_palette = {
                    'Low': 'green',
                    'Medium': 'yellow',
                    'High': 'orange',
                    'Critical': 'red'
                   }

    features = df.columns.tolist()
    n_features = len(features)
    n_cols = int(n_features/2)
    n_rows = int((n_features + n_cols - 1) // n_cols)  # Calculate rows needed for the grid

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, n_rows * 6))  # Dynamically adjust figure size
    axes = np.array(axes)  # Ensure `axes` is always an array
    axes = axes.flatten()  # Flatten to handle indexing consistently

    # Iterate through features and plot histograms
    for i, feature in enumerate(features):
        # Check if the feature is categorical with risk levels or numerical
        if df[feature].dtype == 'object' and set(df[feature].unique()).issubset(risk_palette.keys()):
            sns.histplot(df[feature], palette=risk_palette, ax=axes[i])
        else:
            sns.histplot(df[feature], bins=30, kde=True, ax=axes[i])

        axes[i].set_title(f'Histogram of {feature}')
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel('Frequency')

        # Calculate and display statistics for numeric features
        if np.issubdtype(df[feature].dtype, np.number):
            mean_return = df[feature].mean()
            std_dev = df[feature].std()
            skewness = df[feature].skew()
            kurtosis = df[feature].kurtosis()

            # Display statistics on the plot
            statistics = (f"Mean: {mean_return:.4f}\n"
                      f"Std Dev: {std_dev:.4f}\n"
                      f"Skewness: {skewness:.4f}\n"
                      f"Kurtosis: {kurtosis:.4f}")
            axes[i].text(0.35, -0.18, statistics, transform=axes[i].transAxes,
                     fontsize=10, verticalalignment='top',
                     bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="lightgrey"))


    # Hide any unused subplots
    for j in range(n_features, len(axes)):
        axes[j].set_visible(False)

    #plt.tight_layout()
    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Add padding to the bottom to make space for statistics
    plt.show()

# Function to plot boxplots for features
def plot_boxplots(df):
    """
    Plots boxplots for all features in the list and displays basic statistics.
    """
    # Define the risk palette for categorical features
    risk_palette = {
                    'Low': 'green',
                    'Medium': 'yellow',
                    'High': 'orange',
                    'Critical': 'red'
                   }

    features  = df.columns.tolist()
    n_features = len(features)
    n_cols = int(n_features/2)
    n_rows = int((n_features + n_cols - 1) // n_cols)  # Calculate rows needed for the grid

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, n_rows * 6))  # Dynamically adjust figure size
    axes = np.array(axes)  # Ensure `axes` is always an array
    axes = axes.flatten()  # Flatten to handle indexing consistently

    # Iterate through features and plot boxplots
    for i, feature in enumerate(features):
        # Check if the feature is categorical with risk levels or numerical
        if df[feature].dtype == 'object' and set(df[feature].unique()).issubset(risk_palette.keys()):
            sns.boxplot(y=df[feature], palette=risk_palette, ax=axes[i])
        else:
            sns.boxplot(y=df[feature], ax=axes[i])

        axes[i].set_title(f'Boxplot of {feature}')
        axes[i].set_ylabel(feature)

        # Calculate and display statistics for numeric features
        if np.issubdtype(df[feature].dtype, np.number):
            mean_return = df[feature].mean()
            std_dev = df[feature].std()
            skewness = df[feature].skew()
            kurtosis = df[feature].kurtosis()

            # Add statistics below the plot
            statistics = (f"Mean: {mean_return:.4f}\n"
                      f"Std Dev: {std_dev:.4f}\n"
                      f"Skewness: {skewness:.4f}\n"
                      f"Kurtosis: {kurtosis:.4f}")
            axes[i].text(0.35, -0.18, statistics, transform=axes[i].transAxes,
                     fontsize=10, verticalalignment='top',
                     bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="lightgrey"))

    # Hide any unused subplots
    for j in range(n_features, len(axes)):
        axes[j].set_visible(False)

    #plt.tight_layout()
    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Add padding to the bottom to make space for statistics
    plt.show()
#-----------------------------------------------------------------------------------------------------

# Master function to visualize the distribution of activity features (histograms and boxplots)
def visualize_form_of_activity_features_distribution(df):
    """
    Master function to plot histograms and boxplots for all features, with statistics.
    """
    sns.set(style="whitegrid") # Set the style for the plots
    print("Plotting histograms...")
    plot_histograms(df)

    print("Plotting boxplots...")
    plot_boxplots(df)

# Function to create a scatter plot on a specified axis
def plot_scatter(axes, x, y, hue, df, palette, title, xlabel, ylabel, legend_title, ax_index):
    """
    Creates a scatter plot on the specified axis.
    """
    sns.scatterplot(x=x, y=y, hue=hue, data=df, palette=palette, ax=axes[ax_index])
    axes[ax_index].set_title(title)
    axes[ax_index].set_xlabel(xlabel)
    axes[ax_index].set_ylabel(ylabel)
    axes[ax_index].legend(title=legend_title)

# Function to create a correlation heatmap for selected features
def plot_correlation_heatmap(axes, df, features, ax_index):
    """
    Creates a heatmap showing the correlation between selected features.
    """
    # Select only numerical features for correlation calculation
    numeric_features = df[features].select_dtypes(include=['number'])

     # Calculate the correlation matrix
    corr_matrix = numeric_features.corr()

    # Plot the heatmap with annotations
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, ax=axes[ax_index])
    axes[ax_index].set_title("Correlation Heatmap of Numerical Features")

    #sns.heatmap(df[features].corr(), annot=True, cmap="coolwarm", fmt=".2f", ax=axes[ax_index])
    #axes[ax_index].set_title("Correlation Heatmap")

# Function to combine scatter plots and a heatmap into a single figure
def combines_user_activities_scatter_plots_and_heatmap(scatter_df, df):
    """
    Combines scatter plots and heatmap into a single figure using subplots.
    """
    fig, axes = plt.subplots(1, 3, figsize=(24, 8))  # Create subplots (1 row, 3 columns)

    # Plot 1: Session Duration vs Data Transfer
    plot_scatter(
        axes=axes,
        x="Session Duration in Second",
        y="Data Transfer MB",
        hue="User Location",
        df=scatter_df,
        palette="Set1",
        title="Session Duration vs Data Transfer (MB) by Location",
        xlabel="Session Duration (seconds)",
        ylabel="Data Transfer (MB)",
        legend_title="User Location",
        ax_index=0
    )

    # Plot 2: Login Attempts vs Data Transfer
    plot_scatter(
        axes=axes,
        x="Login Attempts",
        y="Data Transfer MB",
        hue="User Location",
        df=scatter_df,
        palette="Set2",
        title="Login Attempts vs Data Transfer (MB) by Location",
        xlabel="Login Attempts",
        ylabel="Data Transfer (MB)",
        legend_title="User Location",
        ax_index=1
    )

    # Plot 3: Correlation Heatmap
    plot_correlation_heatmap(
        axes=axes,
        df=df,
        features=df.columns,
        ax_index=2
    )

    # Adjust layout and show plot
    plt.tight_layout()
    plt.show()

#-----------------------------------------Main EDA pipeline------------------------------------------------------
# Main function to execute the exploratory data analysis pipeline
def explaratory_data_analysis_pipeline(df):

    # Define the file path to the dataset in Google Drive
    #google_drive_data_folder = "/content/drive/My Drive/Cybersecurity Data"
    #file_name = "cybersecurity_normal_and_anomalous_dataset_for_google_drive_v1.csv"
    #file_path_to_normal_and_anomalous_google_drive = os.path.join(google_drive_data_folder, file_name)

    # Check if the data file exists in the folder
    #if not os.path.exists(file_path_to_normal_and_anomalous_google_drive):
    #    print(f"Data file not found: {file_name}")
    #    print(f"Please upload the '{file_name}' file to your Google Drive folder: '{google_drive_data_folder}'")
    #    print("You can do this manually through the Google Drive interface or by using the files tab in Colab.")
    #    return None # Exit the function if the file is not found

    # Define the list of features for EDA
    eda_features =  [
    "Date Reported", "Issue Response Time Days", "Impact Score", "Cost",
    "Session Duration in Second", "Num Files Accessed", "Login Attempts",
    "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
    ]

    # Define the list of activity features for visualization
    activity_features = [
    "Risk Level", "Threat Level", "Issue Response Time Days", "Impact Score", "Cost",
    "Session Duration in Second", "Num Files Accessed", "Login Attempts",
    "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
    ]
    # Load the dataset from the specified file path
    #df = pd.read_csv(file_path_to_normal_and_anomalous_google_drive)

    # Define the reporting frequency for temporal analysis
    reporting_frequency = 'Quarter'
    frequency = reporting_frequency[0].upper()
    if reporting_frequency.capitalize() == 'Month' or reporting_frequency.capitalize() == 'Quarter':
        frequency_date_column = reporting_frequency.capitalize() + '_Year'

    # Create a copy of the DataFrame with selected EDA features and set 'Date Reported' as index
    eda_features_df = df[eda_features].copy()
    eda_features_df = eda_features_df.set_index("Date Reported")

    # Create a copy for frequency-based analysis
    freq_eda_features_df = eda_features_df.copy()

    # Convert index to datetime and then to the specified frequency
    freq_eda_features_df.index = pd.to_datetime(freq_eda_features_df.index)
    freq_eda_features_df[frequency_date_column] =  freq_eda_features_df.index.to_period(frequency)

    # Group by frequency and calculate the mean
    freq_eda_features_df = freq_eda_features_df.groupby(frequency_date_column).mean()
    # Convert the index back to timestamp for plotting
    freq_eda_features_df.index = freq_eda_features_df.index.to_timestamp()
    display(freq_eda_features_df)


    # Create a copy of the DataFrame with selected activity features
    activity_features_df = df[activity_features].copy()

    # Create a DataFrame with features for scatter plots
    scatter_plot_features_df = df[["Session Duration in Second", "Login Attempts",
                                  "Data Transfer MB", "User Location"]].copy()

    # Run the daily distribution pipeline for frequency-based data
    daily_distribution_of_activity_features_pipeline(freq_eda_features_df )
    # Visualize the distribution of activity features (histograms and boxplots)
    visualize_form_of_activity_features_distribution(activity_features_df)
    # Combine and display scatter plots and the correlation heatmap
    combines_user_activities_scatter_plots_and_heatmap(scatter_plot_features_df, activity_features_df)
    return freq_eda_features_df

"""# aiagent"""

# ---------------------------------------------
# AI Validator + RAG Q&A Chatbot (dual OpenAI SDK)
# ---------------------------------------------
import os
import io
import json
import numpy as np
import pandas as pd
import faiss
from typing import List, Dict, Optional, Tuple, Any
from google.colab import userdata
from google.colab.userdata import SecretNotFoundError
#-----
import re
from rapidfuzz import fuzz
#----
# Optional Colab/GDrive imports (safe if not available)
try:
    from google.colab import drive, files, userdata  # type: ignore
    IN_COLAB = True
except Exception:
    IN_COLAB = False

# SentenceTransformer for embeddings
from sentence_transformers import SentenceTransformer

# Try to detect OpenAI SDK style
NEW_OPENAI_API = False
try:
    from openai import OpenAI  # >= 1.0.0
    NEW_OPENAI_API = True
except Exception:
    pass

try:
    import openai  # == 0.28 style
except Exception:
    openai = None

from transformers import pipeline
try:
    from huggingface_hub import InferenceClient
except ImportError:
    InferenceClient = None

# --------------------------
#   Expected Schema
# --------------------------
EXPECTED_SCHEMA: Dict[str, str] = {
    "Issue Volume": "int64",
    "Impact Score": "int64",
    "Threat Level": "object",
    "Cost": "float64",
    "Department Affected": "object",
    "Num Files Accessed": "int64",
    "IP Location": "object",
    "CPU Usage %": "float64",
    "Status": "object",
    "Issue Name": "object",
    "Issue ID": "object",
    "User Location": "object",
    "Activity Type": "object",
    "Timestamps": "datetime64[ns]",
    "Category": "object",
    "Date Resolved": "datetime64[ns]",
    "Memory Usage MB": "float64",
    "Date Reported": "datetime64[ns]",
    "Reporters": "object",
    "Threat Score": "float64",
    "Issue Response Time Days": "int64",
    "Defense Action": "object",
    "Remediation Steps": "object",
    "Is Anomaly": "int64",
    "Assignees": "object",
    "Severity": "object",
    "Session Duration in Second": "int64",
    "KPI/KRI": "object",
    "Risk Level": "object",
    "Data Transfer MB": "float64",
    "User ID": "object",
    "Issue Key": "object",
    "Color": "object",
    "Login Attempts": "float64"
}


# -----------------------------------
# Knowledge Base + Sample Questions
# -----------------------------------

DEFAULT_KB: List[str] = [
    "Common data quality issues in cybersecurity datasets include missing values in critical fields like IP addresses, timestamps, or user IDs.",
    "Unexpected data types for security event codes or categorical features can indicate data ingestion problems.",
    "Duplicate entries for unique identifiers such as issue IDs or user IDs can skew analysis and model training.",
    "Anomalies in numerical features like login attempts, session duration, data transfer volume, or CPU/memory usage can signal malicious activity.",
    "Schema validation is crucial to ensure that the dataset structure (columns and data types) matches the expected format.",
    "Missing values in numerical columns might require imputation or removal depending on the analysis context.",
    "Outliers in numerical features can significantly impact statistical measures and model performance.",
    "Inconsistent data formats for dates and timestamps need to be handled to enable time-series analysis.",
    "Categorical features with a high cardinality or inconsistent values require careful cleaning and encoding.",
    "Ensuring data integrity involves checking for inconsistencies and relationships between different columns.",
    "The dataset must contain exactly 33 columns.",
    "Each column should have the correct data type as defined in the schema.",
    "There should be no duplicate rows in the dataset.",
    "Missing values must be identified and reported.",
    "Categorical columns should be validated for consistency.",
    "Numeric columns should be checked for outliers.",
    "Data validation includes checking column names, types, nulls, duplicates, and ranges."
]

#======================================
# EDA Insights Request
#======================================
eda_format_insights_request_dic = {

        "charts_meta": [
            {"title":"Threat Score by Department",
            "summary":"Top 3 depts by avg threat score are Finance, IT, and R&D with notable spikes in April.",
            "metric":"Threat Score"},
            {"title":"Cost over Time",
            "summary":"Cumulative cost accelerates in Q2 with two extreme outliers.",
            "metric":"Cost"}
            ],
        "adhoc_questions": [
            "Which departments drive most critical incidents and cost?",
            "Are there any months with unusually high login attempts or session duration?"
        ]
}


# =============================================
# Knowledge Base Doc Generators (HISTORICAL ONLY)
# =============================================

def create_flat_rag_docs(df: pd.DataFrame, chunk_size: int = 20) -> List[str]:
    cost_cols = ["Cost"]
    date_cols = ["Date Reported", "Date Resolved", "Timestamps"]
    numeric_cols = [
        "Session Duration in Second", "Num Files Accessed", "Login Attempts",
        "Data Transfer MB", "Memory Usage MB", "Threat Score"
    ]
    percent_cols = ["CPU Usage %"]
    defense_col = "Defense Action"
    dept_col = "Department Affected"

    all_columns = set(EXPECTED_SCHEMA.keys())
    rag_docs = []

    # Normalize columns present
    use_cols = [c for c in all_columns if c in df.columns]

    for i in range(0, len(df), chunk_size):
        chunk = df.iloc[i:i + chunk_size]
        lines = []
        for _, row in chunk.iterrows():
            row_vals = []
            for col in use_cols:
                val = row.get(col, None)
                if col in cost_cols and pd.notna(val):
                    row_vals.append(f"{col}: ${float(val):,.2f}")
                elif col in numeric_cols and pd.notna(val):
                    row_vals.append(f"{col}: {val}")
                elif col in percent_cols and pd.notna(val):
                    row_vals.append(f"{col}: {val}%")
                elif col in date_cols and pd.notna(val):
                    row_vals.append(f"{col}: {pd.to_datetime(val, errors='coerce')}")
                elif col == defense_col:
                    row_vals.append(f"{col}: {val if pd.notna(val) else 'General'}")
                elif col == dept_col:
                    row_vals.append(f"{col}: {val if pd.notna(val) else 'None'}")
                else:
                    if pd.notna(val):
                        row_vals.append(f"{col}: {val}")
            lines.append(", ".join(row_vals))
        if lines:
            rag_docs.append("\n".join(lines))
    return rag_docs

def create_grouped_rag_docs(df: pd.DataFrame, group_by: str = "Department Affected", top_k: int = 3) -> List[str]:
    numerical_columns = [
        "Session Duration in Second", "Num Files Accessed", "Login Attempts",
        "Data Transfer MB", "Memory Usage MB", "Threat Score"
    ]
    percentage_columns = ["CPU Usage %"]
    cost_column = "Cost"
    date_columns = ["Date Reported", "Date Resolved", "Timestamps"]
    categorical_columns = [
        "Severity", "Category", "Status", "KPI/KRI", "Activity Type",
        "Threat Level", "Attack Type", "Phase"
    ]

    if group_by not in df.columns:
        group_by = "Department Affected"

    rag_docs = []
    grouped = df.groupby(group_by, dropna=False)

    for group_name, group_df in grouped:
        doc = [f"{group_by}: {group_name}"]
        doc.append(f"- Total Issues: {len(group_df)}")

        if "Threat Level" in group_df.columns:
            critical_issues = group_df[group_df["Threat Level"].astype(str).str.lower() == "critical"]
            doc.append(f"- Critical Issues: {len(critical_issues)}")

        if cost_column in group_df.columns:
            total_cost = pd.to_numeric(group_df[cost_column], errors="coerce").fillna(0).sum()
            avg_cost = pd.to_numeric(group_df[cost_column], errors="coerce").fillna(0).mean()
            min_cost = pd.to_numeric(group_df[cost_column], errors="coerce").fillna(0).min()
            max_cost = pd.to_numeric(group_df[cost_column], errors="coerce").fillna(0).max()
            doc.append(f"- Total Cost: ${total_cost:,.2f}")
            doc.append(f"- Cost (Avg): ${avg_cost:,.2f}, Min: ${min_cost:,.2f}, Max: ${max_cost:,.2f}")

        if "Defense Action" in group_df.columns:
            defense_counts = group_df["Defense Action"].dropna().value_counts().to_dict()
            doc.append(f"- Defense Actions: {defense_counts}")

        for col in numerical_columns:
            if col in group_df.columns:
                s = pd.to_numeric(group_df[col], errors="coerce").dropna()
                if len(s):
                    doc.append(f"- {col}: Total={s.sum():.2f}, Avg={s.mean():.2f}, Min={s.min():.2f}, Max={s.max():.2f}")

        for col in percentage_columns:
            if col in group_df.columns:
                s = pd.to_numeric(group_df[col], errors="coerce").dropna()
                if len(s):
                    doc.append(f"- {col}: Avg={s.mean():.2f}%, Max={s.max():.2f}%")

        for col in date_columns:
            if col in group_df.columns:
                dt = pd.to_datetime(group_df[col], errors="coerce").dropna()
                if len(dt):
                    doc.append(f"- {col}: Earliest={dt.min()}, Latest={dt.max()}")

        for col in categorical_columns:
            if col in group_df.columns:
                counts = group_df[col].astype(str).value_counts().nlargest(top_k).to_dict()
                doc.append(f"- {col} Top-{top_k}: {counts}")

        rag_docs.append("\n".join(doc))

    return rag_docs

def prepare_rag_documents(simulated_df: pd.DataFrame = None,
                          historical_df: pd.DataFrame = None,
                          group_by: str = None,
                          chunk_size: int = 20,
                          use_grouped: bool = True,
                          DEFAULT_KB = DEFAULT_KB) -> List[str]: # Add use_grouped as a parameter here
    """
    HISTORICAL-ONLY per requirement:
    - Do NOT combine with simulated data.
    - 'combined_df' explicitly set to None (unused).
    """

    combined_df = None  # <-- per instruction
    if historical_df is None or len(historical_df) == 0:
        # return at least base kb so downstream code doesn't fail
        return DEFAULT_KB

    group_by_rag_docs = create_grouped_rag_docs(historical_df, group_by=group_by)
    flat_rag_docs = create_flat_rag_docs(historical_df, chunk_size=chunk_size)

    # Merge into one knowledge base
    return  DEFAULT_KB + group_by_rag_docs + flat_rag_docs


# Merge into one knowledge base
#extended_knowledge = DEFAULT_KB + sample_questions
# =============================================
# RAG Vector Store
# =============================================
class RAGStore:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.embedder = SentenceTransformer(model_name)
        self.index = None
        self.docs: List[str] = []

    def fit(self, docs: List[str]):
        self.docs = docs or []
        if not self.docs:
            self.index = None
            return self
        embs = self.embedder.encode(self.docs, convert_to_numpy=True)
        # ensure float32 for faiss
        embs = np.array(embs).astype('float32')
        dim = embs.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embs)
        return self

    def search(self, query: str, k: int = 5) -> List[Tuple[int, float, str]]:
        if not self.docs or self.index is None:
            return []
        q = self.embedder.encode([query], convert_to_numpy=True).astype('float32')
        D, I = self.index.search(q, min(k, len(self.docs)))
        return [(int(i), float(d), self.docs[int(i)]) for i, d in zip(I[0], D[0])]


# =============================================
# Anti-repetition helper
# =============================================
def split_sentences(text: str):
    return [s for s in re.split(r'(?<=[.!?])\s+', text.strip()) if s]

def remove_repetitive_sentences(text: str, threshold: int = 85) -> str:
    sents = split_sentences(text)
    out = []
    for s in sents:
        s = s.strip()
        if not s:
            continue
        if any(fuzz.ratio(s.lower(), kept.lower()) >= threshold for kept in out):
            continue
        out.append(s)
    return " ".join(out)

# -------------------------------------------------
# Utility: build a flexible LLM client
# -------------------------------------------------

def build_llm_client():
    # 1. OpenAI (priority if available)
    api_key = os.environ.get("OPENAI_API_KEY") or os.environ.get("data_analytics_Key")
    if api_key:
        if NEW_OPENAI_API:
            return OpenAI(api_key=api_key), "openai-new"
        elif openai is not None:
            openai.api_key = api_key
            return openai, "openai-old"

    # 2. Hugging Face Inference API
    hf_key = None
    if IN_COLAB:
        try:
            hf_key = userdata.get("data_analytics_hg_key")
        except SecretNotFoundError:
            hf_key = None

    if not hf_key:
        hf_key = os.environ.get("data_analytics_hg_key")

    if hf_key and InferenceClient:
        try:
            client = InferenceClient(token=hf_key)
            return client, "huggingface-api"
        except Exception as e:
            print(f"âš ï¸ HuggingFace client init failed, falling back to local: {e}")

    # 3. Local offline Hugging Face pipeline
    local_pipe = pipeline("text-generation", model="distilgpt2")
    return local_pipe, "huggingface-local"



# --------------------------
#     AI Agent
# --------------------------
class AIValidatorAgent:
    def __init__(
        self,
        expected_schema: Dict[str, str],
        knowledge_base: Optional[List[str]] = None,
        model_name: str = "gpt-3.5-turbo",
        top_k: int = 3
    ):
        # schema + kb
        self.expected_schema = expected_schema
        self.knowledge_base = knowledge_base
        # embeddings + FAISS (for quick KB retrieval)
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        # Build a RAG store attached to the agent (so other methods can call self.rag.search)
        self.rag = RAGStore(model_name="all-MiniLM-L6-v2")
        self.rag.fit(self.knowledge_base)

        # openai client
        self.client, self.sdk_mode = build_llm_client()
        self.model_name = model_name

        # outputs
        self.validation_report: Dict[str, any] = {}
        self.initial_summary: str = ""
        self.refined_summary: Optional[str] = None

    # --------------------------
    # Data loading helpers
    # -------------------------
    def load_data(self, choice: str = None, file_path: str = None, url: str = None) -> pd.DataFrame:
        """
        Entry point to load dataset.
        Shows menu (if no choice passed) and delegates to source-specific loaders.
        """
        if choice is None:
            # fallback to CLI input (non-UI use)
            print("ðŸ“‚ Choose data source:")
            print("1. Google Drive (default)")
            print("2. Upload local file")
            print("3. Load from URL")
            choice = input("Enter choice (1/2/3): ").strip()

        if choice == "1":
            return self.load_from_drive()
        elif choice == "2":
            return self.load_from_upload(file_path=file_path)
        elif choice == "3":
            return self.load_from_url(url=url)
        else:
            raise ValueError("Invalid choice. Use 1/2/3.")

    # --------------------------
    # Source-specific loaders
    # --------------------------

    def load_from_drive(self) -> pd.DataFrame:
        if not IN_COLAB:
            raise RuntimeError("Google Drive option requires Google Colab.")
        from google.colab import drive
        drive.mount('/content/drive')
        folder = "/content/drive/My Drive/Cybersecurity Data"
        file_name = "normal_and_anomalous_cybersecurity_dataset_for_google_drive_kb2.csv"
        path = os.path.join(folder, file_name)
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ File not found in Google Drive: {path}")
        df = pd.read_csv(path)
        return self._postprocess(df)

    def load_from_upload(self, file_path: str = None) -> pd.DataFrame:
        if IN_COLAB and file_path is None:
            from google.colab import files
            up = files.upload()
            fn = next(iter(up.keys()))  # take first uploaded file
            df = pd.read_csv(io.BytesIO(up[fn]))
            print("âœ… Local file uploaded successfully.")
        else:
            if file_path is None:
                file_path = input("Enter local CSV path: ").strip()
            df = pd.read_csv(file_path)
            print(f"âœ… Loaded: {file_path}")
        return self._postprocess(df)

    def load_from_url(self, url: str = None) -> pd.DataFrame:
        import requests
        if url is None:
            url = input("Enter file URL: ").strip()
        resp = requests.get(url)
        resp.raise_for_status()
        df = pd.read_csv(io.StringIO(resp.text))
        print(f"âœ… File loaded from URL: {url}")
        return self._postprocess(df)

    # ----------------------
    # Shared post-processing
    # ----------------------
    def _postprocess(self, df: pd.DataFrame) -> pd.DataFrame:
        # Convert datetime columns if present
        for col in ["Timestamps", "Date Resolved", "Date Reported"]:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors="coerce")
        print("\nðŸ“Š Data Loaded. Shape:", df.shape)
        return df


    # --------------------------
    # Validation
    # --------------------------

    def split_sentences(self, text: str):
        """
        Simple regex-based sentence splitter.
        Splits on ., ?, ! followed by space or end of string.
        """
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        return [s for s in sentences if s]

    def remove_repetitive_sentences(self, text: str, similarity_threshold: int = 85) -> str:
        """
        Removes repetitive or similar sentences from a paragraph.
        """
        sentences = self.split_sentences(text)
        unique_sentences = []

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # Check similarity against existing sentences
            is_duplicate = False
            for kept in unique_sentences:
                if fuzz.ratio(sentence.lower(), kept.lower()) >= similarity_threshold:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_sentences.append(sentence)

        return " \n".join(unique_sentences)

    def validate_dataset(self, df: pd.DataFrame) -> Dict[str, any]:
        report: Dict[str, any] = {}

        expected_cols = set(self.expected_schema.keys())
        actual_cols = set(df.columns)

        missing_cols = sorted(list(expected_cols - actual_cols))
        extra_cols = sorted(list(actual_cols - expected_cols))
        report["shape"] = df.shape
        report["missing_columns"] = missing_cols
        report["extra_columns"] = extra_cols

        # dtype mismatches
        type_mismatches = {}
        for col in expected_cols & actual_cols:
            actual_dtype = self._dtype_string(df[col])
            expected_dtype = str(self.expected_schema[col])
            if actual_dtype != expected_dtype:
                type_mismatches[col] = (actual_dtype, expected_dtype)
        report["type_mismatches"] = type_mismatches

        # missing values
        mv = df.isna().sum()
        report["missing_values"] = mv[mv > 0].to_dict()

        # duplicates
        report["duplicates"] = int(df.duplicated().sum())

        # high-level schema check block (for your original printout)
        report["schema_check"] = {
            "missing_columns": missing_cols,
            "extra_columns": extra_cols
        }

        self.validation_report = report
        self.initial_summary = self._compose_initial_summary(report)

        # refine with LLM (if available), using retrieved knowledge
        self.refined_summary = self.refine_summary_with_llm(
            initial_summary=self.initial_summary,
            validation_report=report
        )
        return report

    def validation_errors(self, report):
        # align names with validate_dataset output
        type_mismatches = report.get("type_mismatches")
        missing_columns = report.get("missing_columns")
        extra_cols = report.get("extra_columns")
        null_values = report.get("missing_values")
        duplicate_rows = report.get("duplicates")

        validation_errors = []
        if type_mismatches:
            validation_errors.append("Type mismatches: " + str(type_mismatches))
        if missing_columns:
            validation_errors.append("Missing columns: " + str(missing_columns))
        if extra_cols:
            validation_errors.append("Extra columns: " + str(extra_cols))
        if null_values:
            validation_errors.append("Null values: " + str(null_values))
        if duplicate_rows:
            validation_errors.append("Duplicate rows: " + str(duplicate_rows))
        if validation_errors:
            return "\n".join(validation_errors)
        # return None if no errors
        return None

    def _dtype_string(self, s: pd.Series) -> str:
        """Normalize dtype str for comparison (pandas often returns e.g., 'int64')."""
        return str(s.dtype)

    def _compose_initial_summary(self, report: Dict[str, any]) -> str:
        parts = []
        shape = report.get("shape")
        if shape:
            parts.append(f"The dataset has {shape[0]} rows and {shape[1]} columns.\n")

        # missing values
        if report.get("missing_values"):
            cols = list(report["missing_values"].keys())
            parts.append(f"Some columns have missing values: {cols}.\n")
        else:
            parts.append("No missing values detected.\n")

        # duplicates
        if report.get("duplicates", 0) > 0:
            parts.append(f"There are {report['duplicates']} duplicate rows.\n")
        else:
            parts.append("No duplicate rows found.\n")

        # type mismatches
        if report.get("type_mismatches"):
            mismatches = [
                f"{col} (found {dtype_pair[0]}, expected {dtype_pair[1]})"
                for col, dtype_pair in report["type_mismatches"].items()
            ]
            parts.append("Type mismatches:\n      " + ";\n      ".join(mismatches) + "\n")

        # schema check
        sc = report.get("schema_check", {})
        if sc:
            if sc.get("missing_columns"):
                parts.append(f"Missing expected columns: {sc['missing_columns']}\n")
            if sc.get("extra_columns"):
                parts.append(f"Extra unexpected columns: {sc['extra_columns']}.\n")
            if not sc.get("missing_columns") and not sc.get("extra_columns"):
                parts.append("Dataset schema matches the expected structure.\n")

        # positive â€œall goodâ€ statement if fully clean
        if (
            not report.get("missing_values") and
            report.get("duplicates", 0) == 0 and
            not report.get("type_mismatches") and
            not report.get("missing_columns") and
            not report.get("extra_columns")
        ):
            parts.append(
                "All validation checks passed: schema matches, dtypes align, "
                "no missing values, and no duplicates."
            )

        return "".join(parts)

    # --------------------------
    # LLM helpers
    # --------------------------
    def _chat(self, messages: List[Dict[str, str]]) -> Optional[str]:
        if self.client is None:
            return None
        try:
            if isinstance(self.sdk_mode, str) and self.sdk_mode.startswith("openai"):
                # OpenAI style
                if self.sdk_mode == "openai-new":
                    resp = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                        max_tokens=800,
                        temperature=0.4,
                        top_p=0.95
                    )
                    return resp.choices[0].message.content.strip()
                else:
                    resp = self.client.ChatCompletion.create(
                        model=self.model_name,
                        messages=messages,
                        max_tokens=800,
                        temperature=0.4,
                        top_p=0.95
                    )
                    return resp["choices"][0]["message"]["content"].strip()

            elif self.sdk_mode == "huggingface-api":
                # Hugging Face Inference API (simple prompt style)
                prompt = "\n".join([m["content"] for m in messages if m["role"] == "user"])
                resp = self.client.text_generation("gpt2", prompt, max_new_tokens=200)
                return resp[0]["generated_text"]

            elif self.sdk_mode == "huggingface-local":
                # Local offline pipeline
                prompt = "\n".join([m["content"] for m in messages if m["role"] == "user"])
                outputs = self.client(prompt, max_new_tokens=200, num_return_sequences=1)
                return outputs[0]["generated_text"]

        except Exception as e:
            print(f"Error interacting with LLM: {e}")
            return None



    def refine_summary_with_llm(self, initial_summary: str, validation_report: Dict[str, any]) -> Optional[str]:
        """
        Use the LLM to rewrite the dataset validation summary into one clear,
        non-repetitive paragraph highlighting key issues and next steps.
        """
        if self.client is None:
            return initial_summary

        # Retrieve relevant knowledge
        kb =self.rag.search(json.dumps(validation_report), k=3)

        # Extract just the string content from the tuples returned by rag.search
        kb_strings = [item[2] for item in kb]

        # Stronger instructions
        hidden_context = (
            "Rewrite the following dataset validation summary into ONE short, clear, and professional paragraph. "
            "Avoid repeating information. "
            "Do not include JSON, do not invent details, and do not list schema objects. "
            "Mention the dataset shape, key validation issues (missing values, duplicates, mismatches), "
            "and overall schema health. "
            "Be factual, concise, and non-repetitive.\n\n"
            "Please rewrite this into one concise, non-repetitive paragraph.\n\n"
            f"Initial Summary:\n{initial_summary}\n\n"
            f"Validation Report: {validation_report}\n\n"
            "Relevant knowledge points: " + "; ".join(kb_strings) # Use kb_strings here
        )

        # Only expose a short version to the "user" so the model doesnâ€™t echo the full context
        visible_prompt = (
            f"Initial Summary:\n{initial_summary}\n"
        )

        messages = [
            {"role": "system", "content": "You are a senior data quality analyst. Use hidden context only for reasoning."},
            {"role": "user", "content": visible_prompt},
            {"role": "system", "content": hidden_context}  # hidden, not echoed
        ]

        refined = self._chat(messages)
        if refined:
            # final cleanup to remove any repetition that slipped through
            return self.remove_repetitive_sentences(refined.strip())
        return initial_summary

    def get_summary(self) -> str:
        return "".join(self.remove_repetitive_sentences(self.refined_summary or self.initial_summary))

    # --------------------------
    # Q&A chatbot
    # --------------------------
    def answer(self, question: str, detail_level: str = "auto") -> str:
        """
        Answers follow-up questions using validation report, refined summary,
        and retrieved knowledge base items.

        detail_level: "auto" | "executive" | "technical"
        - auto: tries to infer from question
        - executive: high-level summary
        - technical: include full validation details
        """

        kb = self.rag.search(question, k=5)
        summary = self.get_summary()
        report = self.validation_report

        # Infer level if auto
        if detail_level == "auto":
            low_words = ["summary", "overview", "high-level", "executive"]
            high_words = ["details", "mismatch", "missing", "duplicates", "dtype", "columns"]
            q_lower = question.lower()
            if any(w in q_lower for w in low_words):
                detail_level = "executive"
            elif any(w in q_lower for w in high_words):
                detail_level = "technical"
            else:
                detail_level = "executive"

        # Prepare context in natural language
        if detail_level == "executive":
            context_text = f"The dataset validation summary is: {summary}. Relevant knowledge: {', '.join([item[2] for item in kb])}." # Extract strings here too
        else:  # technical
            context_text = f"Validation summary: {summary}. Full validation report: {report}. Relevant knowledge: {', '.join([item[2] for item in kb])}." # And here
        # If no LLM, fallback
        if self.client is None:
            fallback = [
                "LLM not configured. Here is a heuristic answer based on available context:",
                f"- Summary: {summary}",
                f"- Validation report: {report}",
                f"- Top knowledge:\n  - " + "\n  - ".join([item[2] for item in kb]) # And here
            ]
            return "\n".join(fallback)

        messages = [
            {"role": "system", "content": "You are a helpful data quality analyst. Always use the provided validation summary and report in your answer."},
            {"role": "user", "content": (
                f"Question: {question}\n\n"
                f"Context: {context_text}\n\n"
                "Answer in one concise, professional paragraph. "
                "Avoid repeating details. Include next-step recommendations if appropriate."
            )}
        ]

        raw_ans = self._chat(messages)
        if raw_ans:
            return self.remove_repetitive_sentences(raw_ans.strip())
        return "I'm unable to answer right now."

    #==================================================
     # ---- Report overview (insightful, non-repetitive) ----
    def report_overview(self, query_hint: str = "overall risk and cost drivers") -> str:
        stats = self._quick_profile()
        retrieved = self.rag.search(query_hint, k=5)
        context_snips = "\n---\n".join([r[2] for r in retrieved])

        system = (
            "You are a senior cybersecurity data analyst. "
            "Use the provided stats and context to craft ONE concise paragraph of insights. "
            "Strict rules: do NOT repeat yourself; do NOT echo the prompt; do NOT list the inputs; "
            "no disclaimers. Provide crisp takeaways and (if useful) a next action."
        )
        user = (
            f"Dataset stats: {json.dumps(stats)}\n\n"
            f"Context from RAG (historical only):\n{context_snips}\n\n"
            "Task: Write a single paragraph highlighting key patterns, risk hotspots, and recommended actions."
        )

        msg = [{"role":"system","content":system},{"role":"user","content":user}]
        ans = self._chat(msg)
        if not ans:
            # Heuristic fallback
            base = [f"Analyzed {stats.get('rows','?')} records across {stats.get('cols','?')} columns."]
            if "cost_total" in stats:
                base.append(f"Total cost {stats['cost_total']} (avg {stats.get('cost_avg','N/A')}).")
            if "top_Department Affected" in stats:
                base.append(f"Most affected departments {stats['top_Department Affected']}.")
            if "top_Threat Level" in stats:
                base.append(f"Threat levels distribution {stats['top_Threat Level']}.")
            ans = " ".join(base)
        return remove_repetitive_sentences(ans)

    # ---- Comment on EDA charts ----
    def comment_on_eda(self, charts: List[Dict[str, str]]) -> List[str]:
        """
        charts: list of dicts with keys like:
          - title: str (e.g., "Threat Score by Department")
          - summary: str (brief description of what the chart shows, e.g., top 3 departments, visible spikes)
          - metric: str (optional metric name)
          - notes: str (optional extra observations)
        Returns: list of one-paragraph comments per chart.
        """
        comments = []
        base_stats = self._quick_profile()

        for ch in charts:
            retrieved = self.rag.search(ch.get("title","chart insight"), k=3)
            ctx = "\n---\n".join([r[2] for r in retrieved])

            system = (
                "You are a senior cybersecurity data analyst. "
                "Provide ONE short paragraph of chart commentary: trend, anomalies/outliers, and a practical action. "
                "Strict rules: no repetition, no generic filler, no listing of inputs, no disclaimers."
            )
            user = (
                f"Dataset quick stats: {json.dumps(base_stats)}\n\n"
                f"Chart title: {ch.get('title','N/A')}\n"
                f"Chart summary: {ch.get('summary','N/A')}\n"
                f"Metric: {ch.get('metric','N/A')}\n"
                f"Notes: {ch.get('notes','')}\n\n"
                f"Relevant historical context:\n{ctx}"
            )

            msg = [{"role":"system","content":system},{"role": "user", "content": user}]
            ans = self._chat(msg)
            if not ans:
                ans = f"{ch.get('title', 'Chart')}: No LLM available; based on summary, highlight the top movers and check anomalies."
            comments.append(remove_repetitive_sentences(ans))
        return comments

    # ---- Ad-hoc Q&A grounded in RAG ----
    def ask(self, question: str) -> str:
        retrieved = self.rag.search(question, k=5)
        ctx = "\n---\n".join([r[2] for r in retrieved])
        system = (
            "You are a helpful cybersecurity data assistant. "
            "Answer with ONE concise paragraph grounded in the provided context and data. "
            "Avoid repetition, no disclaimers, no echoing."
        )
        # Light dataset stats help keep it grounded
        user = (
            f"Question: {question}\n\n"
        )
        msg = [{"role":"system","content":system},{"role":"user","content":user}]
        ans = self._chat(msg)
        if not ans:
            ans = "I don't have a model configured, but you can investigate top departments by cost and critical threat levels first."
        return remove_repetitive_sentences(ans)

    # helper used by report_overview and others
    def _quick_profile(self) -> Dict[str, Any]:
        # Try to summarise current validation_report or return minimal
        r = {"rows": "?", "cols": "?"}
        rep = self.validation_report or {}
        shape = rep.get("shape")
        if shape:
            r["rows"], r["cols"] = int(shape[0]), int(shape[1])
        # try to surface some stats if available in validation_report (e.g., sample top cols)
        return r

    # =====================================================================
    #  Format report insight, build RAG store, create agent, run insights
    # =====================================================================

    def formatting_eda_insights_request(self, df: pd.DataFrame, print_output: bool = True):
        """
        Build RAG docs from historical_df, fit vector store, instantiate CyberInsightsAgent,
        and optionally run example overview / chart commentary / ad-hoc Q&A.

        Returns dict with keys: { 'rag_docs', 'store', 'agent', 'overview', 'comments', 'answers' }
        """
        charts_meta = eda_format_insights_request_dic["charts_meta"]
        adhoc_questions = eda_format_insights_request_dic["adhoc_questions"]
        group_by = "Department Affected"

        # Basic validation
        if df is None or not isinstance(df, pd.DataFrame) or df.shape[0] == 0:
            raise ValueError("historical_df must be a non-empty pandas DataFrame (already validated).")

        rag_docs = prepare_rag_documents(historical_df=df, group_by=group_by)
        insight_agent = AIValidatorAgent(expected_schema=EXPECTED_SCHEMA, knowledge_base=rag_docs)
        store = insight_agent.rag  # RAGStore object

        if print_output:
            print(f"Agent created. LLM mode: {insight_agent.sdk_mode}")

        outputs = {"rag_docs": rag_docs, "store": store, "agent": insight_agent,
                   "overview": None, "comments": [], "answers": []}

        # 4) Get a one-paragraph report overview (RAG-grounded)
        try:
            overview = insight_agent.report_overview()
            outputs["overview"] = overview
            if print_output:
                print("\n=== REPORT OVERVIEW ===\n")
                print(overview)
        except Exception as e:
            if print_output:
                print("Error generating report overview:", e)

        # 5) Get commentary for EDA charts if provided
        if charts_meta:
            try:
                comments = insight_agent.comment_on_eda(charts_meta)
                outputs["comments"] = comments
                if print_output:
                    print("\n=== CHART COMMENTS ===\n")
                    for title, cm in zip([c.get("title","chart") for c in charts_meta], comments):
                        print(f"- {title}:\n  {cm}\n")
            except Exception as e:
                if print_output:
                    print("Error generating chart comments:", e)

        # 6) Run ad-hoc RAG-grounded Q&A if provided
        if adhoc_questions:
            try:
                answers = []
                for q in adhoc_questions:
                    ans = insight_agent.ask(q)
                    answers.append((q, ans))
                    if print_output:
                        print("\nQ>", q)
                        print("A>", ans, "\n")
                outputs["answers"] = answers
            except Exception as e:
                if print_output:
                    print("Error running adhoc Q&A:", e)

        return outputs

# ======================================================
# Executive Agent
# ======================================================
class ExecutiveAgent:
    """
    ExecutiveAgent:
    - Generates high-level, business-friendly insights.
    - Summarizes dashboards, charts, and anomaly trends.
    - Provides concise executive answers from RAG knowledge base.
    """

    def __init__(self, knowledge_base: Optional[List[str]] = None, model_name: str = "gpt-3.5-turbo"):
        self.knowledge_base = knowledge_base or DEFAULT_KB
        self.rag = RAGStore(model_name="all-MiniLM-L6-v2")
        self.rag.fit(self.knowledge_base)
        self.client, self.sdk_mode = build_llm_client()
        self.model_name = model_name

    # ---- Summarize charts / dashboards into executive insights ----
    def summarize_dashboard(self, charts_meta: List[Dict[str, str]]) -> str:
        insights = []
        for chart in charts_meta:
            title = chart.get("title", "Untitled Chart")
            summary = chart.get("summary", "")
            insights.append(f"- {title}: {summary}")
        summary_text = "\n".join(insights)

        if self.client is None:
            return "Executive summary (offline mode):\n" + summary_text

        messages = [
            {"role": "system", "content": "You are an executive data analyst. Write concise, actionable insights for business leaders."},
            {"role": "user", "content": f"Charts Summary:\n{summary_text}\n\nWrite an executive overview."}
        ]
        resp = self._chat(messages)
        return resp or summary_text

    # ---- Answer executive-level questions ----
    def answer(self, question: str) -> str:
        kb = self.rag.search(question, k=5)
        kb_text = "\n".join([f"- {item[2]}" for item in kb])
        messages = [
            {"role": "system", "content": "You are a senior executive advisor. Provide clear, concise, non-technical insights."},
            {"role": "user", "content": f"Question: {question}\nRelevant knowledge:\n{kb_text}"}
        ]
        resp = self._chat(messages)
        return resp or "No executive insight available."

    # ---- Internal chat call ----
    def _chat(self, messages):
        if self.client is None:
            return None
        try:
            if self.sdk_mode == "openai-new":
                resp = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    max_tokens=400
                )
                return resp.choices[0].message.content.strip()
            elif self.sdk_mode == "openai-old":
                resp = self.client.ChatCompletion.create(
                    model=self.model_name,
                    messages=messages,
                    max_tokens=400
                )
                return resp["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"ExecutiveAgent chat error: {e}")
            return None


# ======================================================
# AI Agent Manager
# ======================================================
class AIAgentManager:
    """
    Manages multiple AI agents (Validator, Executive).
    Allows dynamic selection of agent type at runtime.
    """

    def __init__(self):
        self.validator = AIValidatorAgent(expected_schema=EXPECTED_SCHEMA, knowledge_base=DEFAULT_KB)
        self.executive = ExecutiveAgent(knowledge_base=DEFAULT_KB)

    def get_agent(self, agent_type: str):
        """
        Returns the requested agent by type.
        Options: "validator", "executive"
        """
        if agent_type.lower() == "validator":
            return self.validator
        elif agent_type.lower() == "executive":
            return self.executive
        else:
            raise ValueError(f"Unknown agent type: {agent_type}")

"""# **Explanatory Data Analysis Pipeline**"""

# -----------------------------------
#  Explanatory Data Analysis Pipeline
# -----------------------------------
import ipywidgets as widgets
from IPython.display import display, clear_output
#silence all the warning messages
import warnings
warnings.filterwarnings("ignore")

#silence the messages from the sentence-transformers library.
import logging
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)


# ------------------- Data Load UI -------------------

def load_data_ui(data_output, on_confirm_callback):
    choice_widget = widgets.RadioButtons(
        options=[
            ("1. Google Drive (default)", "1"),
            ("2. Upload local file", "2"),
            ("3. Load from URL", "3")
        ],
        value="1",
        layout=widgets.Layout(width="50%")
    )
    confirm_btn = widgets.Button(description="Confirm", button_style="primary")
    box = widgets.VBox([choice_widget, confirm_btn])

    def on_confirm(btn):
        box.close()
        with data_output:
            print(f"ðŸ‘‰ Selected option: {choice_widget.label}")
        on_confirm_callback(choice_widget.value)

    confirm_btn.on_click(on_confirm)
    with data_output:
        display(box)

def eda_main_pipeline():

    data_output = widgets.Output(layout={'border': '1px solid #ddd', 'padding': '10px',
                                        'height': '200px', 'overflow_y': 'auto'})
    display(data_output)
    def after_choice(choice_value):
        # --- Instantiate agent ---
        with data_output:
            print("ðŸ¤– A> Initializing validator agent...\n")
        agent_validator = AIValidatorAgent(expected_schema=EXPECTED_SCHEMA, knowledge_base = prepare_rag_documents())
        # Load data
        df = agent_validator.load_data(choice=choice_value)
        with data_output:
            print("ðŸ¤– A> Loading data...\n")
            report = agent_validator.validate_dataset(df)
            print("ðŸ¤– A> Summary Statement:\n", agent_validator.get_summary() + "\n")
            if not agent_validator.validation_errors(report):
                print("ðŸ¤– A> Data loaded successfully.")
            else:
                print("ðŸ¤– A> Validation issues detected.")
                return
            print("ðŸ¤– A> âœ… Agent built successfully.\n")

            extended_knowledge_base = prepare_rag_documents(df) #
            extended_knowledge = extended_knowledge_base + list(agent_validator.get_summary())

            agent = AIValidatorAgent(expected_schema=EXPECTED_SCHEMA, knowledge_base=extended_knowledge_base)
            # Print the SDK mode for clarity
            print(f"âœ… LLM client initialized in mode: {agent.sdk_mode}")

            #report = agent.validate_dataset(df)
            #print("Summary Statement:", agent.get_summary() + "\n")

            validation_errors = agent.validation_errors(report)
            if not validation_errors:
                print("âœ… Proceeding with EDA pipeline...")

                try:
                    # Check if the function exists before calling
                    if "explaratory_data_analysis_pipeline" in globals():
                        real_world_normal_and_anomalous_df = explaratory_data_analysis_pipeline(df)
                        agent.formatting_eda_insights_request(df)

                        return real_world_normal_and_anomalous_df
                    else:
                        print("âš ï¸ Skipping EDA: 'explaratory_data_analysis_pipeline' is not defined.")
                        return None
                except Exception as e:
                    print(f"âŒ Error running EDA pipeline: {e}")
                    return None
            else:
                print("âš ï¸ Validation issues detected, skipping EDA pipeline.")
                return None

    # Launch data load UI
    load_data_ui(data_output, after_choice)
if __name__ == "__main__":
    eda_main_pipeline()