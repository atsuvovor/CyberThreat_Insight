{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "12FvTzaJlo6qKF4C-ZL_d1XPgqgyOW91F",
      "authorship_tag": "ABX9TyPyeMVvKz28ZuoGv3S4e/JH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atsuvovor/CyberThreat_Insight/blob/main/model_testing/stacked_ad_classifier_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbef799c"
      },
      "source": [
        "#Stacked Supervised Model Inference\n",
        "\n",
        "**Toronto, Sept 17 202**  \n",
        "**Autor : Atsu Vovor**\n",
        ">Master of Management in Artificial Intelligence    \n",
        ">Consultant Data Analytics Specialist | Machine Learning |  \n",
        "Data science | Quantitative Analysis |French & English Bilingual  \n",
        "\n",
        "**Model Name:** Stacked Anomaly Detection Classifier\n",
        "\n",
        "**Model Type:** Stacked Ensemble Model\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "### Abstract\n",
        "\n",
        "This project develops and evaluates a novel stacked anomaly detection model for cybersecurity analytics. Leveraging unsupervised anomaly detection techniques as feature generators, the model augments original data with anomaly scores and flags. A supervised meta-learner (Gradient Boosting) is then trained on this enhanced feature set, including outputs from a Random Forest base model. The approach aims to improve the accuracy and interpretability of threat detection by combining the strengths of various anomaly detection methods with a supervised classification framework. The project includes data loading, preprocessing, feature engineering, model training, comprehensive evaluation using standard classification and anomaly detection metrics, visualization of model performance, and a discussion of real-world deployment considerations.  \n",
        "\n",
        "### Introduction\n",
        "\n",
        "In the rapidly evolving landscape of cybersecurity threats, the ability to accurately and promptly identify malicious or anomalous activities is paramount. Traditional signature-based detection methods often fall short against novel and sophisticated attacks. Anomaly detection, which focuses on identifying deviations from normal behavior, offers a promising alternative. However, single anomaly detection techniques can have limitations in capturing the diverse nature of anomalies and providing actionable insights. This project addresses these challenges by proposing a stacked ensemble model that integrates multiple unsupervised anomaly detection methods. The output of these unsupervised models serves as crucial input features for a supervised classifier, enabling a more robust and nuanced approach to classifying different levels of cyber threats. This hybrid methodology aims to enhance the overall effectiveness and reliability of anomaly-based threat detection in complex cybersecurity environments.  \n",
        "\n",
        "### Scope\n",
        "\n",
        "The scope of this project includes:\n",
        "\n",
        "1.  **Data Processing:** Loading, splitting, and scaling a provided augmented cybersecurity dataset (`x_y_augmented_data_google_drive.csv`).\n",
        "2.  **Unsupervised Anomaly Feature Engineering:** Applying and training various unsupervised anomaly detection models (Isolation Forest, One-Class SVM, Local Outlier Factor, DBSCAN, KMeans, Dense Autoencoder, LSTM Autoencoder) on the training data to generate anomaly scores and flags as new features.\n",
        "3.  **Stacked Model Development:** Training a Random Forest classifier as a base model and a Gradient Boosting classifier as a meta-learner on the extended feature set (original scaled features + unsupervised anomaly features + Random Forest `predict_proba`).\n",
        "4.  **Model Evaluation:** Performing comprehensive evaluation of the final stacked model using multi-class classification metrics (Accuracy, Precision, Recall, F1-score, Confusion Matrix, Classification Report).\n",
        "5.  **Unsupervised Model Analysis:** Evaluating and visualizing the performance of individual unsupervised anomaly detection models to understand their contributions.\n",
        "6.  **Artifact Saving:** Saving the trained scaler, all unsupervised models, and the supervised stacked models (Random Forest base and Gradient Boosting meta) for potential future deployment.\n",
        "7.  **Documentation:** Providing technical specifications and real-world usage guidelines for the developed stacked model.\n",
        "\n",
        "This project focuses on the model development and evaluation phases using a predefined dataset and does not include real-time data ingestion, production deployment infrastructure development, or extensive hyperparameter tuning.  \n",
        "\n",
        "## Stacked Supervised Model Technical Specifications  \n",
        "\n",
        "\n",
        "**Architecture:**  \n",
        "\n",
        "Stacked Anomaly Detection Classifier Model flowchart  \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NGQBLFu7oxQfRefJYSskZ4q772nklM-W\"\n",
        "     alt=\"Stacked Model Flowchart\"\n",
        "     width=\"100%\"/>\n",
        "\n",
        "\n",
        "The model employs a two-layer stacked architecture:\n",
        "1.  **Base Learners:** A set of unsupervised anomaly detection models are used as feature generators. These models process the input data and produce continuous anomaly scores or binary anomaly flags. The models used are:\n",
        "    *   Isolation Forest\n",
        "    *   One-Class SVM\n",
        "    *   Local Outlier Factor (LOF)\n",
        "    *   DBSCAN (with nearest neighbor assignment for test data)\n",
        "    *   KMeans (distance to centroid)\n",
        "    *   Dense Autoencoder (reconstruction Mean Squared Error - MSE)\n",
        "    *   LSTM Autoencoder (reconstruction MSE)\n",
        "\n",
        "2.  **Meta Learner:** A supervised classification model is trained on an augmented feature set. This augmented set includes the original scaled features, the anomaly features generated by the base learners, and the `predict_proba` outputs from a Random Forest model trained on the extended feature set (original + anomaly features).\n",
        "    *   **Base Model for Proba:** Random Forest Classifier\n",
        "    *   **Meta Model:** Gradient Boosting Classifier\n",
        "\n",
        "**Input Data:**\n",
        "*   Augmented numeric dataset.\n",
        "*   Assumes `Threat Level` is encoded as integers (0: Low, 1: Medium, 2: High, 3: Critical).\n",
        "*   Requires features to be preprocessed (no missing values, categorical features encoded).\n",
        "\n",
        "**Preprocessing Steps:**\n",
        "1.  **Data Loading:** Load data from a specified CSV file (`x_y_augmented_data_google_drive.csv`).\n",
        "2.  **Data Splitting:** Split data into training and testing sets using `train_test_split` with stratification on the target variable.\n",
        "3.  **Feature Scaling:** Standardize features using `StandardScaler`.\n",
        "\n",
        "**Feature Engineering (Anomaly Features):**\n",
        "*   Each unsupervised model is fitted on the scaled training data.\n",
        "*   Anomaly scores or flags are generated for both the scaled training and testing data.\n",
        "*   These unsupervised features are concatenated with the original scaled features.\n",
        "\n",
        "**Training Process:**\n",
        "1.  **Unsupervised Models:** Each unsupervised model is trained independently on the scaled training data (Autoencoders are trained on a subset of \"normal\" data).\n",
        "2.  **Random Forest (Base for Proba):** A Random Forest Classifier is trained on the concatenated original scaled features and unsupervised anomaly features.\n",
        "3.  **Meta Learner (Gradient Boosting):** A Gradient Boosting Classifier is trained on a stacked feature set consisting of:\n",
        "    *   Original scaled features\n",
        "    *   Unsupervised anomaly features\n",
        "    *   `predict_proba` outputs from the trained Random Forest model.\n",
        "\n",
        "**Hyperparameters:**\n",
        "*   `TEST_SIZE`: 0.20\n",
        "*   `RANDOM_STATE`: 42\n",
        "*   `CONTAMINATION`: 0.05 (for Isolation Forest, One-Class SVM, LOF)\n",
        "*   `DBSCAN_EPS`: 0.5\n",
        "*   `DBSCAN_MIN_SAMPLES`: 5\n",
        "*   `KMEANS_CLUSTERS`: 4\n",
        "*   `AUTOENCODER_EPOCHS`: 50\n",
        "*   `LSTM_EPOCHS`: 50\n",
        "*   `AUTOENCODER_BATCH`: 32\n",
        "*   `LSTM_BATCH`: 32\n",
        "*   Random Forest: `n_estimators=200`\n",
        "*   Gradient Boosting: `n_estimators=200`\n",
        "*   Autoencoder/LSTM Early Stopping: `patience=5` based on validation loss.\n",
        "\n",
        "**Output:**\n",
        "*   Predicted `Threat Level` (0, 1, 2, or 3).\n",
        "*   Evaluation metrics: Accuracy, Precision, Recall, F1-score (macro and weighted), Confusion Matrix, Classification Report, ROC Curve, Precision-Recall Curve, Anomaly Detection Performance Metrics (AUC, Average Precision, F1).\n",
        "\n",
        "**Model Artifacts Saved:**\n",
        "*   `StandardScaler` object (`scaler.joblib`)\n",
        "*   Trained Random Forest base model (`rf_base.joblib`)\n",
        "*   Trained Gradient Boosting meta model (`gb_meta.joblib`)\n",
        "*   Trained Isolation Forest model (`iso.joblib`)\n",
        "*   Trained One-Class SVM model (`ocsvm.joblib`)\n",
        "*   Trained Local Outlier Factor model (`lof.joblib`)\n",
        "*   Trained DBSCAN model (`dbscan.joblib`)\n",
        "*   Trained KMeans model (`kmeans.joblib`)\n",
        "*   Scaled training data (for DBSCAN mapping) (`train_X_scaled.npy`)\n",
        "*   Trained Dense Autoencoder model (`dense_autoencoder.keras`)\n",
        "*   Trained LSTM Autoencoder model (`lstm_autoencoder.keras`)\n",
        "*   Evaluation metrics (`metrics.json`)\n",
        "\n",
        "**Performance Evaluation:**\n",
        "*   The stacked model's performance is evaluated using standard multi-class classification metrics.\n",
        "*   Individual unsupervised model performance for anomaly detection is evaluated using metrics like AUC and Average Precision (where applicable) against a binary anomaly ground truth derived from the `Threat Level`.\n",
        "\n",
        "**Key Features:**\n",
        "*   Leverages multiple unsupervised anomaly detection techniques to generate rich anomaly features.\n",
        "*   Uses a stacked ensemble approach to combine the signals from unsupervised models and original features for improved classification.\n",
        "*   Provides comprehensive evaluation metrics and visualizations for both the final stacked model and the individual unsupervised components.\n",
        "\n",
        "**Potential Enhancements:**\n",
        "*   Hyperparameter tuning for all models (unsupervised and supervised).\n",
        "*   Experimentation with different unsupervised models or combinations.\n",
        "*   Investigation into more sophisticated methods for combining unsupervised outputs.\n",
        "*   Advanced techniques for handling class imbalance if necessary.\n",
        "*   Integration with real-time data streams for continuous monitoring.  \n",
        "  \n",
        "  \n",
        "## Real-World Usage of the Stacked Anomaly Detection Classifier\n",
        "\n",
        "To use the trained Stacked Anomaly Detection Classifier in a real-world cybersecurity analytics environment, the following steps would typically be involved:\n",
        "\n",
        "1.  **Model Deployment:**\n",
        "    *   The saved model artifacts (`scaler.joblib`, `rf_base.joblib`, `gb_meta.joblib`, unsupervised models, `train_X_scaled.npy`) need to be deployed to a production environment. This could be a dedicated server, a cloud platform (like Google Cloud Platform, AWS, Azure), or an edge device, depending on the application's requirements and scale.\n",
        "    *   The chosen environment should have the necessary dependencies installed (Python, scikit-learn, TensorFlow, joblib, numpy, pandas).\n",
        "\n",
        "2.  **Data Ingestion and Preprocessing:**\n",
        "    *   Real-world security data (e.g., network logs, system events, user activity logs) needs to be collected and processed.\n",
        "    *   This data must be transformed into the same format as the training data, ensuring the same features are present and in the correct order.\n",
        "    *   Any categorical features in the new data must be encoded using the same encoding scheme used during training (e.g., if label encoding was used, apply the saved label encoders).\n",
        "    *   Missing values in the new data must be handled in the same way as during training (e.g., imputation).\n",
        "\n",
        "3.  **Data Scaling:**\n",
        "    *   The preprocessed real-world data must be scaled using the *saved* `StandardScaler` object (`scaler.joblib`). It's crucial to use the scaler fitted on the training data to ensure consistency.\n",
        "\n",
        "4.  **Anomaly Feature Generation:**\n",
        "    *   For each new data instance (or batch of instances), the scaled data is passed through the *saved* unsupervised models (`iso.joblib`, `ocsvm.joblib`, `lof.joblib`, `dbscan.joblib`, `kmeans.joblib`, `dense_autoencoder.keras`, `lstm_autoencoder.keras`).\n",
        "    *   The anomaly scores or flags are extracted from these models, similar to how it was done for the test set during training.\n",
        "    *   For DBSCAN, the nearest neighbor approach using the saved `train_X_scaled.npy` would be applied to assign anomaly flags to new data points.\n",
        "    *   For Autoencoders, the reconstruction error (MSE) is calculated for the new scaled data.\n",
        "\n",
        "5.  **Feature Stacking:**\n",
        "    *   The generated anomaly features are concatenated with the original scaled features of the new data instances.\n",
        "    *   The *saved* Random Forest base model (`rf_base.joblib`) is used to predict the `predict_proba` for the new extended feature set.\n",
        "    *   These `predict_proba` outputs are then stacked with the extended features (original scaled + anomaly features) to create the final input for the meta-learner.\n",
        "\n",
        "6.  **Threat Level Prediction:**\n",
        "    *   The stacked features for the new data instances are fed into the *saved* Gradient Boosting meta-learner (`gb_meta.joblib`).\n",
        "    *   The meta-learner outputs the predicted `Threat Level` (0, 1, 2, or 3) for each new data instance.\n",
        "\n",
        "7.  **Actionable Insights and Response:**\n",
        "    *   The predicted `Threat Level` for each event can be used to trigger alerts, prioritize security incidents, or initiate automated response actions.\n",
        "    *   For example, events classified as \"High\" or \"Critical\" might automatically trigger an investigation by a security analyst or block suspicious network traffic.\n",
        "    *   The anomaly scores generated by the unsupervised models can also be used to provide additional context or confidence levels for the predictions.\n",
        "\n",
        "**Workflow Summary:**\n",
        "\n",
        "Real-World Data -> Preprocessing -> Scale (using saved scaler) -> Generate Anomaly Features (using saved unsupervised models) -> Stack Features + RF Proba (using saved RF base model) -> Predict Threat Level (using saved GB meta-learner) -> Action/Alerting\n",
        "\n",
        "**Considerations for Production:**\n",
        "\n",
        "*   **Scalability:** The deployment environment should be able to handle the volume and velocity of real-world security data.\n",
        "*   **Latency:** For real-time threat detection, the model inference process needs to be fast.\n",
        "*   **Monitoring:** Continuous monitoring of the model's performance in production is essential to detect concept drift or changes in threat patterns.\n",
        "*   **Maintenance:** The model may need to be retrained periodically with new data to maintain its effectiveness.\n",
        "*   **Integration:** The model needs to be integrated with existing security information and event management (SIEM) systems or other security tools.  \n",
        "\n",
        "\n",
        "Conclusion  \n",
        "\n",
        "\n",
        "This project successfully developed and evaluated a stacked anomaly detection classifier leveraging unsupervised anomaly features for enhanced cybersecurity threat detection. The integrated pipeline demonstrated the process from data loading and preprocessing through unsupervised feature engineering, stacked model training, and comprehensive evaluation. The results indicate the potential of this hybrid approach to improve the accuracy and interpretability of anomaly detection compared to using individual unsupervised methods alone. While the model shows promising performance, particularly in distinguishing various threat levels, the analysis also highlighted areas for potential improvement, such as enhancing the recall for critical threats and further optimizing individual unsupervised components. The saved model artifacts and detailed documentation provide a solid foundation for potential real-world deployment and further research into advanced anomaly detection techniques in cybersecurity."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacked Supervised Model Code"
      ],
      "metadata": {
        "id": "asYwUUNHTJ6d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2712592f"
      },
      "source": [
        "#CyberThreat-Insight\n",
        "#Anomalous Behavior Detection in Cybersecurity Analytics using Generative AI\n",
        "#  ---------------------Model Development---------------------\n",
        "#Toronto, Septeber 08 2025\n",
        "#Autor : Atsu Vovor\n",
        "\n",
        "#Master of Management in Artificial Intelligence\n",
        "#Consultant Data Analytics Specialist | Machine Learning |\n",
        "#Data science | Quantitative Analysis |French & English Bilingual\n",
        "#===============================================================================\n",
        "\n",
        "#-------------------\n",
        "#  Import Library\n",
        "#------------------\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, auc, precision_recall_curve,\n",
        "                             average_precision_score, roc_auc_score)\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "\n",
        "# -----------\n",
        "# PARAMETERS\n",
        "# -----------\n",
        "\n",
        "LABEL_COL = \"Threat Level\"\n",
        "TEST_SIZE = 0.20\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "AUTOENCODER_EPOCHS = 50\n",
        "LSTM_EPOCHS = 50\n",
        "AUTOENCODER_BATCH = 32\n",
        "LSTM_BATCH = 32\n",
        "DBSCAN_EPS = 0.5\n",
        "DBSCAN_MIN_SAMPLES = 5\n",
        "KMEANS_CLUSTERS = 4\n",
        "CONTAMINATION = 0.05\n",
        "DATA_PATH =  \"/content/drive/My Drive/Cybersecurity Data\"\n",
        "DATA_PATH = DATA_PATH + \"/x_y_augmented_data_google_drive.csv\"\n",
        "MODEL_OUTPUT_DIR = \"/content/drive/My Drive/stacked_models_deployment\"\n",
        "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "#------------------\n",
        "# Color Mapping\n",
        "#------------------\n",
        "def get_color_map():\n",
        "    # Define the colors\n",
        "    #colors = [\"darkred\", \"red\", \"orangered\", \"orange\", \"yelloworange\", \"lightyellow\", \"yellow\", \"greenyellow\", \"green\"]\n",
        "    colors = [\"#8B0000\", \"#FF0000\", \"#FF4500\", \"#FFA500\", \"#FFB347\", \"#FFFFE0\", \"#FFFF00\", \"#ADFF2F\", \"#008000\"]\n",
        "\n",
        "    # Create a colormap\n",
        "    custom_cmap = LinearSegmentedColormap.from_list(\"CustomCmap\", colors)\n",
        "\n",
        "    return custom_cmap\n",
        "custom_cmap = get_color_map()\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[INFO] {msg}\")\n",
        "    with open(os.path.join(MODEL_OUTPUT_DIR, \"log.txt\"), \"a\") as f:\n",
        "        f.write(f\"{msg}\\n\")\n",
        "\n",
        "def get_label_class_name(model_y_test, y_model_pred):\n",
        "    \"\"\"Get class names based on labels.\"\"\"\n",
        "    labels = sorted(list(set(model_y_test) | set(y_model_pred)))\n",
        "    level_mapping = {0: \"Low\", 1: \"Medium\", 2: \"High\", 3: \"Critical\"}\n",
        "    class_names = [level_mapping.get(label) for label in labels]\n",
        "\n",
        "    return class_names, labels\n",
        "\n",
        "#----------------------------------------Model performance report-----------------------------------\n",
        "def get_metrics(y_true, y_pred, report):\n",
        "    \"\"\"Get model performance metrics from classification report.\"\"\"\n",
        "    class_names = list(y_true.unique())\n",
        "    #report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "    metrics_dic = {\n",
        "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
        "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
        "        \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
        "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
        "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
        "        \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Overall Model Accuracy \": report['accuracy'],\n",
        "\n",
        "    }\n",
        "    return metrics_dic\n",
        "\n",
        "def print_model_performance_report_(model_name, y_true, y_pred):\n",
        "    \"\"\"Prints a classification report and confusion matrix for a given model.\"\"\"\n",
        "    print(f\"\\n{model_name} Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    print(f\"\\n{model_name} Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap= custom_cmap, cbar=False)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "#------------------------------\n",
        "def plot_metrics_bell_curve(model_name, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plots PPV, Sensitivity, NPV, Specificity for multi-class classification\n",
        "    on separate bell-curve style subplots for clarity.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Assuming class names are 0, 1, 2, 3\n",
        "    class_names = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
        "\n",
        "\n",
        "    # Compute TP, FP, FN, TN\n",
        "    TP = np.diag(cm)\n",
        "    FP = cm.sum(axis=0) - TP\n",
        "    FN = cm.sum(axis=1) - TP\n",
        "    TN = cm.sum() - (TP + FP + FN)\n",
        "\n",
        "    # Metrics\n",
        "    PPV = TP / (TP + FP)\n",
        "    Sensitivity = TP / (TP + FN)\n",
        "    NPV = TN / (TN + FN)\n",
        "    Specificity = TN / (TN + FP)\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        \"Class\": class_names,\n",
        "        \"PPV\": PPV,\n",
        "        \"Sensitivity\": Sensitivity,\n",
        "        \"NPV\": NPV,\n",
        "        \"Specificity\": Specificity\n",
        "    })\n",
        "\n",
        "    print(f\"\\n=== {model_name} Classification Metrics ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Plot bell curves in subplots\n",
        "    metrics_to_plot = [\"PPV\", \"Sensitivity\", \"NPV\", \"Specificity\"]\n",
        "    colors = [\"skyblue\", \"lightgreen\", \"salmon\", \"orange\", \"purple\", \"cyan\", \"pink\"]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    x = np.linspace(0, 1, 500)\n",
        "    sigma = 0.05  # width of bell curve\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        ax = axes[i]\n",
        "        for j, value in enumerate(metrics_df[metric]):\n",
        "            y = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5*((x - value)/sigma)**2)\n",
        "            ax.plot(x, y, color=colors[j % len(colors)], alpha=0.7, label=class_names[j])\n",
        "        ax.set_title(metric)\n",
        "        ax.set_xlabel(\"Metric Value (0-1)\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "    plt.suptitle(f\"{model_name} - Multi-class Metrics Bell Curves\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_df\n",
        "#---------------------------------------\n",
        "def create_scatter_plot(data, x, y, hue, ax, x_label=None, y_label=None):\n",
        "    \"\"\"Generate scatter plot for anomalies vs normal points.\"\"\"\n",
        "    sns.scatterplot(x=x, y=y, hue=hue, palette={0: 'blue', 1: 'red'}, data=data, ax=ax)\n",
        "    ax.set_title(\"Anomalies (Red) vs Normal Points (Blue)\")\n",
        "    ax.set_xlabel(x_label or x)\n",
        "    ax.set_ylabel(y_label or y)\n",
        "\n",
        "\n",
        "def create_roc_curve(data, anomaly_score, is_anomaly, ax):\n",
        "    \"\"\"Generate ROC curve and calculate AUC.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(data[is_anomaly], data[anomaly_score])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "def create_precision_recall_curve(data, anomaly_score, is_anomaly, ax):\n",
        "    \"\"\"Generate Precision-Recall Curve.\"\"\"\n",
        "    precision, recall, _ = precision_recall_curve(data[is_anomaly], data[anomaly_score])\n",
        "    ax.plot(recall, precision, color='purple', lw=2)\n",
        "    ax.set_xlabel(\"Recall\")\n",
        "    ax.set_ylabel(\"Precision\")\n",
        "    ax.set_title(\"Precision-Recall Curve\")\n",
        "#--------------------------\n",
        "def generate_performance_insight(model_name, report, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Generates a dynamic performance report insight based on classification metrics.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model.\n",
        "        report (dict): The classification report as a dictionary.\n",
        "        y_true (array-like): True labels.\n",
        "        y_pred (array-like): Predicted labels.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Performance Insight for {model_name} ---\")\n",
        "\n",
        "    class_names, labels = get_label_class_name(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Compute TP, FP, FN, TN for all classes\n",
        "    TP = np.diag(cm)\n",
        "    FP = cm.sum(axis=0) - TP\n",
        "    FN = cm.sum(axis=1) - TP\n",
        "    TN = cm.sum() - (TP + FP + FN)\n",
        "\n",
        "    print(\"\\nMetrics per Class:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        label_str = str(labels[i]) # Get the string representation of the label\n",
        "        print(f\"\\n  Class: {class_name}\")\n",
        "        print(f\"    Precision: {report[label_str]['precision']:.4f} - Of all instances predicted as '{class_name}', this is the proportion that were actually '{class_name}'.\")\n",
        "        print(f\"    Recall: {report[label_str]['recall']:.4f} - Of all actual '{class_name}' instances, this is the proportion that the model correctly identified.\")\n",
        "        print(f\"    F1-Score: {report[label_str]['f1-score']:.4f} - A balanced measure of Precision and Recall for this class.\")\n",
        "        print(f\"    Support: {report[label_str]['support']} - The number of actual instances of this class in the test set.\")\n",
        "        # PPV, NPV, Sensitivity (Recall), Specificity\n",
        "        ppv = TP[i] / (TP[i] + FP[i]) if (TP[i] + FP[i]) > 0 else 0\n",
        "        npv = TN[i] / (TN[i] + FN[i]) if (TN[i] + FN[i]) > 0 else 0\n",
        "        sensitivity = TP[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 # Same as Recall\n",
        "        specificity = TN[i] / (TP[i] + FP[i]) if (TP[i] + FP[i]) > 0 else 0\n",
        "\n",
        "        print(f\"    PPV (Positive Predictive Value): {ppv:.4f} - The probability that a positive prediction is a true positive.\")\n",
        "        print(f\"    NPV (Negative Predictive Value): {npv:.4f} - The probability that a negative prediction is a true negative.\")\n",
        "        print(f\"    Sensitivity (True Positive Rate): {sensitivity:.4f} - Ability to correctly identify positive instances.\")\n",
        "        print(f\"    Specificity (True Negative Rate): {specificity:.4f} - Ability to correctly identify negative instances.\")\n",
        "\n",
        "\n",
        "    print(\"\\nAggregated Metrics:\")\n",
        "    print(f\"  Accuracy: {report['accuracy']:.4f} - The overall proportion of correctly classified instances.\")\n",
        "    print(f\"  Precision (Macro): {report['macro avg']['precision']:.4f} - Average Precision across all classes, unweighted.\")\n",
        "    print(f\"  Recall (Macro): {report['macro avg']['recall']:.4f} - Average Recall across all classes, unweighted.\")\n",
        "    print(f\"  F1 Score (Macro): {report['macro avg']['f1-score']:.4f} - Average F1-Score across all classes, unweighted.\")\n",
        "    print(f\"  Precision (Weighted): {report['weighted avg']['precision']:.4f} - Average Precision across all classes, weighted by support.\")\n",
        "    print(f\"  Recall (Weighted): {report['weighted avg']['recall']:.4f} - Average Recall across all classes, weighted by support.\")\n",
        "    print(f\"  F1 Score (Weighted): {report['weighted avg']['f1-score']:.4f} - Average F1-Score across all classes, weighted by support.\")\n",
        "\n",
        "    print(\"\\nBusiness Insights:\")\n",
        "    print(f\"- The model '{model_name}' achieved an overall accuracy of {report['accuracy']:.4f}.\")\n",
        "\n",
        "    # Identify classes with lower performance (example: based on F1-score)\n",
        "    # Fix: Iterate through class labels directly from the report dictionary\n",
        "    low_performance_classes = []\n",
        "    for label_str in report:\n",
        "        if label_str in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "            continue # Skip aggregated rows\n",
        "        if report[label_str]['f1-score'] < 0.7: # Example threshold\n",
        "            # Find the corresponding class name\n",
        "            try:\n",
        "                label_int = int(label_str)\n",
        "                class_index = labels.index(label_int)\n",
        "                low_performance_classes.append(class_names[class_index])\n",
        "            except ValueError:\n",
        "                 # Handle cases where label might not be an integer (e.g., if class_report includes other keys)\n",
        "                 continue\n",
        "\n",
        "\n",
        "    if low_performance_classes:\n",
        "        print(f\"- Pay close attention to the model's performance on classes: {', '.join(low_performance_classes)}. Lower F1-scores here might indicate challenges in correctly identifying these specific threat levels.\")\n",
        "    else:\n",
        "        print(\"- The model demonstrates strong overall performance across all major metrics and classes.\")\n",
        "\n",
        "    # Highlight critical class performance\n",
        "    if \"Critical\" in class_names:\n",
        "        critical_index = class_names.index(\"Critical\")\n",
        "        critical_label_str = str(labels[critical_index])\n",
        "        if critical_label_str in report: # Ensure 'Critical' label is in the report keys\n",
        "            critical_recall = report[critical_label_str]['recall']\n",
        "            critical_precision = report[critical_label_str]['precision']\n",
        "            print(f\"- For 'Critical' threats (Recall: {critical_recall:.4f}, Precision: {critical_precision:.4f}), the model is able to capture a significant portion of actual critical events (Recall), and when it flags an event as critical, it is often correct (Precision). However, further investigation into missed critical events might be warranted to minimize risk.\")\n",
        "            if critical_recall < 0.8: # Example threshold\n",
        "                 print(\"- Improving recall for Critical threats should be a priority to ensure all high-severity events are detected.\")\n",
        "        else:\n",
        "            print(\"- 'Critical' class metrics not found in the classification report.\")\n",
        "\n",
        "\n",
        "    # General recommendations\n",
        "    print(\"- Consider deploying this model for automated threat detection, but maintain human oversight, especially for high-severity alerts.\")\n",
        "    print(\"- Continuously monitor model performance in a production environment as threat patterns can evolve.\")\n",
        "    print(\"- Investigate misclassified instances to understand the limitations and potential areas for model improvement or data enhancement.\")\n",
        "\n",
        "def generate_visualization_insight(data, anomaly_score_col, is_anomaly_col, model_name):\n",
        "    \"\"\"\n",
        "    Generates dynamic comments and business insights based on visualization data.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): DataFrame containing the data used for visualization.\n",
        "        anomaly_score_col (str): Column name for the anomaly score.\n",
        "        is_anomaly_col (str): Column name for the binary anomaly flag (0 or 1).\n",
        "        model_name (str): The name of the model being visualized.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Visualization Insights for {model_name} ---\")\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"  No data available for visualization insights.\")\n",
        "        return\n",
        "\n",
        "    # Analyze Scatter Plot\n",
        "    num_anomalies = data[is_anomaly_col].sum()\n",
        "    num_normal = len(data) - num_anomalies\n",
        "    print(\"\\nScatter Plot Insight:\")\n",
        "    print(f\"- The scatter plot visually separates points identified as anomalies ({num_anomalies} points, typically shown in red) from normal points ({num_normal} points, typically shown in blue).\")\n",
        "    print(f\"- A clear separation between the red and blue points indicates that the model is effectively distinguishing between normal and anomalous behavior based on the chosen features.\")\n",
        "\n",
        "    # Analyze ROC Curve (requires ground truth in is_anomaly_col)\n",
        "    if len(data[is_anomaly_col].unique()) > 1: # Check if there are both normal and anomaly points\n",
        "        try:\n",
        "            fpr, tpr, _ = roc_curve(data[is_anomaly_col], data[anomaly_score_col])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            print(\"\\nROC Curve Insight:\")\n",
        "            print(f\"- The ROC curve shows the trade-off between the True Positive Rate (ability to detect actual anomalies) and the False Positive Rate (incorrectly flagging normal points as anomalies) at various threshold settings.\")\n",
        "            print(f\"- An Area Under the ROC Curve (AUC) of {roc_auc:.4f} indicates the overall ability of the model to discriminate between positive (anomaly) and negative (normal) classes.\")\n",
        "            if roc_auc > 0.8: # Example threshold for good performance\n",
        "                print(\"- An AUC value above 0.8 suggests good discriminative power.\")\n",
        "            elif roc_auc > 0.5:\n",
        "                 print(\"- An AUC value above 0.5 suggests the model performs better than random guessing.\")\n",
        "            else:\n",
        "                 print(\"- An AUC value below 0.5 suggests the model performs worse than random guessing.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not generate ROC curve insight: {e}\")\n",
        "    else:\n",
        "        print(\"\\nROC Curve Insight: Not applicable (requires both normal and anomaly ground truth labels).\")\n",
        "\n",
        "\n",
        "    # Analyze Precision-Recall Curve (requires ground truth in is_anomaly_col)\n",
        "    if len(data[is_anomaly_col].unique()) > 1: # Check if there are both normal and anomaly points\n",
        "        try:\n",
        "            precision, recall, _ = precision_recall_curve(data[is_anomaly_col], data[anomaly_score_col])\n",
        "            print(\"\\nPrecision-Recall Curve Insight:\")\n",
        "            print(f\"- The Precision-Recall curve highlights the trade-off between Precision (accuracy of positive predictions) and Recall (completeness of positive predictions) as the decision threshold is varied.\")\n",
        "            print(\"- This curve is particularly informative for imbalanced datasets, where the number of anomalies is much smaller than normal instances.\")\n",
        "            print(\"- A curve that stays high as recall increases indicates that the model can achieve high precision even when identifying a large proportion of actual anomalies.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not generate Precision-Recall curve insight: {e}\")\n",
        "    else:\n",
        "        print(\"\\nPrecision-Recall Curve Insight: Not applicable (requires both normal and anomaly ground truth labels).\")\n",
        "\n",
        "\n",
        "    print(\"\\nBusiness Insights from Visualization:\")\n",
        "    if len(data[is_anomaly_col].unique()) > 1:\n",
        "        if roc_auc > 0.8: # Example threshold for good discriminative power\n",
        "            print(f\"- The visualization for the {model_name} suggests that the model has good potential for identifying anomalous behavior. The clear separation in the scatter plot and the high AUC value are positive indicators.\")\n",
        "            print(\"- This model could be used to prioritize events for further investigation by security analysts.\")\n",
        "        elif roc_auc > 0.5:\n",
        "             print(f\"- The visualization for the {model_name} indicates that the model is somewhat effective at identifying anomalies, performing better than random guessing. However, there might be room for improvement in separating normal and anomalous instances.\")\n",
        "             print(\"- Further refinement of features or model parameters could enhance its utility in practice.\")\n",
        "        else:\n",
        "             print(f\"- The visualization for the {model_name} suggests that the model's ability to discriminate anomalies is limited. The scatter plot may show significant overlap, and the AUC is low.\")\n",
        "             print(\"- This model might not be suitable for direct deployment without significant improvements or a different approach.\")\n",
        "\n",
        "        # Comment on Precision-Recall based on the curve shape (qualitative for now)\n",
        "        print(\"- The shape of the Precision-Recall curve provides insight into how many anomalies the model can find before its positive predictions become unreliable. A curve that drops sharply indicates that increasing recall comes at a high cost of precision (more false alarms).\")\n",
        "\n",
        "    else:\n",
        "        print(\"- As the visualization data currently only contains one class (either all normal or all anomalies), it's difficult to provide detailed business insights based on the discriminative performance (ROC and Precision-Recall curves). Ensure your test data for visualization includes a representative mix of normal and anomalous instances with their ground truth labels.\")\n",
        "\n",
        "    print(f\"\\nEnd Visualization Insights for {model_name}.\")\n",
        "#--------------------------------------------\n",
        "def visualizing_model_performance_pipeline(data, x, y,\n",
        "                                           anomaly_score, is_anomaly,  best_unsupervised_model_name, title=None):\n",
        "    \"\"\"Pipeline to visualize scatter plot, ROC curve, and Precision-Recall curve.\"\"\"\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    fig.suptitle(\"Model Performance Visualization\\n\")\n",
        "\n",
        "    # Generate Scatter Plot\n",
        "    create_scatter_plot(data, x, y, hue=is_anomaly, ax=ax1, x_label=x, y_label=y)\n",
        "\n",
        "    # Generate ROC Curve\n",
        "    create_roc_curve(data, anomaly_score, is_anomaly, ax=ax2)\n",
        "\n",
        "    # Generate Precision-Recall Curve\n",
        "    create_precision_recall_curve(data, anomaly_score, is_anomaly, ax=ax3)\n",
        "\n",
        "    # Adjust layout and set title\n",
        "    plt.tight_layout()\n",
        "    if title:\n",
        "       plt.suptitle(title)\n",
        "    plt.show()\n",
        "    generate_visualization_insight(data, anomaly_score, is_anomaly, best_unsupervised_model_name)\n",
        "#----------------------------------------------------\n",
        "def print_model_performance_report(model_name, model_y_test, y_model_pred):\n",
        "    \"\"\"Print comprehensive model performance report including classification report, confusion matrix, and aggregated metrics.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + model_name + \" classification_report:\\n\")\n",
        "    report = classification_report(model_y_test, y_model_pred, output_dict=True)\n",
        "    class_names, labels = get_label_class_name(model_y_test, y_model_pred)\n",
        "\n",
        "    # Create DataFrame from classification report and rename index\n",
        "    report_df = pd.DataFrame(report).T\n",
        "    # Only rename the class labels (0, 1, 2, 3), not the aggregated rows\n",
        "    class_labels_to_rename = [str(label) for label in labels]\n",
        "    rename_dict = {label: class_names[i] for i, label in enumerate(class_labels_to_rename)}\n",
        "\n",
        "    report_df = report_df.rename(index=rename_dict)\n",
        "\n",
        "    display(round(report_df,4))\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(model_y_test, y_model_pred, labels=labels)\n",
        "    confusion_matrix_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "    print(\"\\n\" + model_name + \" Confusion Matrix:\\n\")\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    heatmap = sns.heatmap(\n",
        "            round(confusion_matrix_df,2),\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap=custom_cmap,\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names\n",
        "    )\n",
        "\n",
        "    # Get the axes object\n",
        "    ax = heatmap.axes\n",
        "    # Set the x-axis label\n",
        "    ax.set_xlabel(\"Predicted Class\")\n",
        "\n",
        "    # Move the x-axis label to the top\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    ax.xaxis.tick_top()\n",
        "\n",
        "    #Set the y-axis label (title)\n",
        "    ax.set_ylabel(\"Actual Class\")\n",
        "\n",
        "    # Set the overall plot title\n",
        "    plt.title(\"Confusion Matrix\\n\")\n",
        "\n",
        "    # Adjust subplot parameters to give more space at the top\n",
        "    plt.subplots_adjust(top=0.85)\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + model_name + \" Agreggated Peformance Metrics:\\n\")\n",
        "    metrics_dic = get_metrics(model_y_test, y_model_pred, report)\n",
        "    metrics_df = pd.DataFrame(metrics_dic.items(), columns=['Metric', 'Value'])\n",
        "    display(metrics_df)\n",
        "    plot_metrics_bell_curve(model_name, model_y_test, y_model_pred)\n",
        "    print(\"\\nOverall Model Accuracy : \", metrics_dic.get(\"Overall Model Accuracy \", 0))\n",
        "    # Call the generate_performance_insight function for the best model\n",
        "    generate_performance_insight(model_name, report, model_y_test, y_model_pred )\n",
        "\n",
        "    #select tbe best unsupersised model end generate the reports insight\n",
        "\n",
        "    #generate_visualization_insight(data, anomaly_score, is_anomaly, best_unsupervised_model_name)\n",
        "    return  metrics_dic\n",
        "#-----------------------------------------------------\n",
        "def build_dense_autoencoder(input_dim):\n",
        "    inp = Input(shape=(input_dim,))\n",
        "    x = Dense(64, activation='relu')(inp)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    out = Dense(input_dim, activation='linear')(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    # explicit loss object to be safe when serializing\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_lstm_autoencoder(timesteps, features):\n",
        "    inputs = Input(shape=(timesteps, features))\n",
        "    encoded = LSTM(128, activation='relu', return_sequences=False)(inputs)\n",
        "    decoded = RepeatVector(timesteps)(encoded)\n",
        "    decoded = LSTM(features, activation='linear', return_sequences=True)(decoded)\n",
        "    model = Model(inputs, decoded)\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "\n",
        "    if not os.path.exists(DATA_PATH):\n",
        "        raise FileNotFoundError(f\"Dataset not found: {DATA_PATH}\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    if LABEL_COL not in df.columns:\n",
        "        raise ValueError(f\"Label column '{LABEL_COL}' not found in dataset.\")\n",
        "    # ensure integer labels\n",
        "    df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
        "    X = df.drop(columns=[LABEL_COL])\n",
        "    y = df[LABEL_COL]\n",
        "    return X, y\n",
        "\n",
        "def get_best_unsupervised_model_corrected(X_test_scaled, y_test, unsupervised_models):\n",
        "    \"\"\"\n",
        "    Evaluates all unsupervised models and identifies the best one based on a chosen metric (AUC or F1).\n",
        "\n",
        "    Args:\n",
        "        X_test_scaled (np.ndarray): Scaled test features.\n",
        "        y_test (pd.Series): True labels for the test set.\n",
        "        unsupervised_models (dict): Dictionary containing all trained unsupervised models and train_X.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_model_name, best_anomaly_score, best_is_anomaly)\n",
        "               best_model_name (str): Name of the best performing model.\n",
        "               best_anomaly_score (np.ndarray): Anomaly scores from the best model.\n",
        "               best_is_anomaly (np.ndarray): Binary anomaly labels (1 for anomaly, 0 for normal) from the best model.\n",
        "    \"\"\"\n",
        "    performance_data = get_unsupervised_model_performance_data_corrected(X_test_scaled, y_test, unsupervised_models)\n",
        "\n",
        "    best_model_name = None\n",
        "    best_performance = -1 # Initialize with a value lower than any expected metric\n",
        "    best_anomaly_score = None\n",
        "    best_is_anomaly = None\n",
        "\n",
        "    for name, (anomaly_score, is_anomaly, performance_metric) in performance_data.items():\n",
        "        if performance_metric is not None:\n",
        "            if performance_metric > best_performance:\n",
        "                best_performance = performance_metric\n",
        "                best_model_name = name\n",
        "                best_anomaly_score = anomaly_score\n",
        "                best_is_anomaly = is_anomaly\n",
        "\n",
        "    if best_model_name:\n",
        "        log(f\"\\nBest unsupervised model: {best_model_name} with performance: {best_performance:.4f}\")\n",
        "    else:\n",
        "        log(\"\\nCould not identify the best unsupervised model.\")\n",
        "    print(\"\\n--- End of Unsupervised Model Visualizations and Insights ---\")\n",
        "    return best_model_name, best_anomaly_score, best_is_anomaly\n",
        "def evaluate_unsupervised_model_corrected(name, model, X_test_scaled, y_test, unsupervised_models):\n",
        "    \"\"\"\n",
        "    Evaluates an unsupervised model for anomaly detection.\n",
        "\n",
        "    Args:\n",
        "        name (str): Name of the model.\n",
        "        model: The trained unsupervised model.\n",
        "        X_test_scaled (np.ndarray): Scaled test features.\n",
        "        y_test (pd.Series): True labels for the test set.\n",
        "        unsupervised_models (dict): Dictionary containing all trained unsupervised models and train_X.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (anomaly_score, is_anomaly, performance_metric)\n",
        "               anomaly_score (np.ndarray): Anomaly scores from the model.\n",
        "               is_anomaly (np.ndarray): Binary anomaly labels (1 for anomaly, 0 for normal).\n",
        "               performance_metric (float): A chosen metric for evaluation (AUC or F1).\n",
        "                                           Returns None if evaluation is not possible.\n",
        "    \"\"\"\n",
        "    anomaly_score = None\n",
        "    is_anomaly = None\n",
        "    performance_metric = None\n",
        "    y_test_binary_anomaly = (y_test.reset_index(drop=True) >= 2).astype(int) # Define anomaly based on Threat Level\n",
        "\n",
        "    try:\n",
        "        if hasattr(model, 'decision_function'):\n",
        "            # For models with decision function, invert for consistency: higher score = more anomalous\n",
        "            anomaly_score = -model.decision_function(X_test_scaled)\n",
        "            # For models with decision function, we can also get predicted labels based on contamination\n",
        "            if hasattr(model, 'predict'):\n",
        "                 is_anomaly = (model.predict(X_test_scaled) == -1).astype(int) # -1 is outlier for IsolationForest, LOF, OneClassSVM\n",
        "\n",
        "        elif hasattr(model, 'predict'):\n",
        "             # DBSCAN: predicts cluster labels. -1 is noise (anomaly)\n",
        "             if name == \"DBSCAN\":\n",
        "                 # For DBSCAN on test data, use nearest neighbor assignment and map -1 to anomaly\n",
        "                 # Need to re-calculate this as it's not stored in the model object\n",
        "                 train_X = unsupervised_models.get('train_X')\n",
        "                 if train_X is not None and hasattr(model, 'labels_'):\n",
        "                      nbrs = NearestNeighbors(n_neighbors=1).fit(train_X)\n",
        "                      nn_idx = nbrs.kneighbors(X_test_scaled, return_distance=False)[:, 0]\n",
        "                      db_labels_train = model.labels_\n",
        "                      assigned_train_labels = db_labels_train[nn_idx]\n",
        "                      is_anomaly = (assigned_train_labels == -1).astype(int)\n",
        "                      # Using binary label as score proxy for plotting if no continuous score\n",
        "                      anomaly_score = is_anomaly.copy().astype(float)\n",
        "                 else:\n",
        "                    log(f\"Warning: Cannot evaluate DBSCAN on test set without train_X and model labels_.\")\n",
        "                    return None, None, None # Cannot evaluate without necessary components\n",
        "             # KMeans: distance to centroid as anomaly score\n",
        "             elif name == \"KMeans\":\n",
        "                 test_k_labels = model.predict(X_test_scaled)\n",
        "                 anomaly_score = np.linalg.norm(X_test_scaled - model.cluster_centers_[test_k_labels], axis=1)\n",
        "                 # No direct binary prediction from KMeans predict\n",
        "                 is_anomaly = None\n",
        "\n",
        "        # Handle Autoencoders (Dense and LSTM)\n",
        "        if name in [\"Autoencoder\", \"LSTM(Classifier)\"]:\n",
        "            if name == \"LSTM(Classifier)\":\n",
        "                timesteps = 1 # Assuming timestep=1 as in training\n",
        "                input_dim = X_test_scaled.shape[1]\n",
        "                X_test_seq = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, input_dim))\n",
        "                reconstructions = model.predict(X_test_seq, verbose=0)\n",
        "                anomaly_score = np.mean((X_test_seq - reconstructions) ** 2, axis=(1, 2))\n",
        "            elif name == \"Autoencoder\": # Dense Autoencoder\n",
        "                reconstructions = model.predict(X_test_scaled, verbose=0)\n",
        "                anomaly_score = np.mean((X_test_scaled - reconstructions) ** 2, axis=1)\n",
        "            # No direct binary prediction from Autoencoders predict\n",
        "            is_anomaly = None\n",
        "\n",
        "        # Evaluate performance if possible\n",
        "        # Check if y_test_binary_anomaly has at least two unique classes before calculating metrics\n",
        "        if anomaly_score is not None and len(np.unique(y_test_binary_anomaly)) > 1:\n",
        "            # Use Average Precision Score for models with continuous anomaly scores\n",
        "            if name not in [\"DBSCAN\"] and anomaly_score is not None: # DBSCAN has binary prediction primarily\n",
        "                try:\n",
        "                    performance_metric = average_precision_score(y_test_binary_anomaly, anomaly_score)\n",
        "                    log(f\"  {name} performance (Average Precision): {performance_metric:.4f}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"  Could not calculate Average Precision for {name}: {e}\")\n",
        "                    # Fallback to AUC if Average Precision fails and AUC is applicable\n",
        "                    if name not in [\"DBSCAN\"] and anomaly_score is not None:\n",
        "                        try:\n",
        "                            performance_metric = roc_auc_score(y_test_binary_anomaly, anomaly_score)\n",
        "                            log(f\"  {name} performance (AUC - Fallback): {performance_metric:.4f}\")\n",
        "                        except Exception as e_auc:\n",
        "                            log(f\"  Could not calculate AUC for {name}: {e_auc}\")\n",
        "                            performance_metric = None # Cannot evaluate with AUC or Average Precision\n",
        "\n",
        "\n",
        "            # Use F1-score for models with binary anomaly predictions (like DBSCAN)\n",
        "            if is_anomaly is not None:\n",
        "                 # Check if both classes exist in predictions and true labels for F1\n",
        "                 if len(np.unique(is_anomaly)) > 1 and len(np.unique(y_test_binary_anomaly)) > 1:\n",
        "                     try:\n",
        "                         f1 = f1_score(y_test_binary_anomaly, is_anomaly)\n",
        "                         # For DBSCAN, F1 is the primary metric here\n",
        "                         if name == \"DBSCAN\":\n",
        "                             performance_metric = f1\n",
        "                             log(f\"  {name} performance (F1): {performance_metric:.4f}\")\n",
        "                         # For other models with binary predictions, F1 is an additional metric\n",
        "                     except Exception as e:\n",
        "                         log(f\"  Could not calculate F1 for {name}: {e}\")\n",
        "                 else:\n",
        "                     log(f\"  Skipping F1 calculation for {name}: Not enough unique classes in predictions or true labels.\")\n",
        "        elif len(np.unique(y_test_binary_anomaly)) <= 1:\n",
        "             log(f\"  Skipping performance metric calculation for {name}: Test set contains only one class of anomaly labels.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error evaluating {name}: {e}\")\n",
        "        performance_metric = None\n",
        "\n",
        "    return anomaly_score, is_anomaly, performance_metric\n",
        "\n",
        "def get_unsupervised_model_performance_data_corrected(X_test_scaled, y_test, unsupervised_models):\n",
        "    \"\"\"\n",
        "    Evaluates all unsupervised models and returns a dictionary containing\n",
        "    their anomaly scores, binary anomaly labels, and performance metrics.\n",
        "\n",
        "    Args:\n",
        "        X_test_scaled (np.ndarray): Scaled test features.\n",
        "        y_test (pd.Series): True labels for the test set.\n",
        "        unsupervised_models (dict): Dictionary containing all trained unsupervised models and train_X.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are model names and values are tuples\n",
        "              (anomaly_score, is_anomaly, performance_metric).\n",
        "              anomaly_score (np.ndarray): Anomaly scores from the model.\n",
        "              is_anomaly (np.ndarray): Binary anomaly labels (1 for anomaly, 0 for normal).\n",
        "              performance_metric (float): A chosen metric for evaluation (e.g., AUC or F1).\n",
        "    \"\"\"\n",
        "    unsupervised_model_names = [\"IsolationForest\", \"OneClassSVM\", \"LocalOutlierFactor\", \"DBSCAN\", \"KMeans\", \"Autoencoder\", \"LSTM(Classifier)\"]\n",
        "    model_mapping = {\n",
        "        \"IsolationForest\": unsupervised_models.get('iso'),\n",
        "        \"OneClassSVM\": unsupervised_models.get('ocsvm'),\n",
        "        \"LocalOutlierFactor\": unsupervised_models.get('lof'),\n",
        "        \"DBSCAN\": unsupervised_models.get('dbscan'),\n",
        "        \"KMeans\": unsupervised_models.get('kmeans'),\n",
        "        \"Autoencoder\": unsupervised_models.get('dense_ae'),\n",
        "        \"LSTM(Classifier)\": unsupervised_models.get('lstm_ae') # Assuming this maps to lstm_ae\n",
        "    }\n",
        "\n",
        "    performance_data = {}\n",
        "    # y_test_binary_anomaly is calculated inside evaluate_unsupervised_model_corrected\n",
        "\n",
        "    for name in unsupervised_model_names:\n",
        "        model = model_mapping.get(name)\n",
        "        if model is None:\n",
        "            log(f\"Skipping {name}: Model not found in unsupervised_models dictionary.\")\n",
        "            continue\n",
        "\n",
        "        log(f\"Evaluating {name}...\")\n",
        "        anomaly_score, is_anomaly, performance_metric = evaluate_unsupervised_model_corrected(\n",
        "            name, model, X_test_scaled, y_test, unsupervised_models\n",
        "        )\n",
        "\n",
        "        performance_data[name] = (anomaly_score, is_anomaly, performance_metric)\n",
        "\n",
        "    return performance_data\n",
        "\n",
        "\n",
        "# 1. Refactor Data Loading and Splitting\n",
        "def load_and_split_data(data_path, label_col, test_size, random_state):\n",
        "    \"\"\"\n",
        "    Loads the dataset and splits it into training and testing sets.\n",
        "    \"\"\"\n",
        "    log(\"Loading dataset...\")\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(f\"Dataset not found: {data_path}\")\n",
        "    df = pd.read_csv(data_path)\n",
        "    if label_col not in df.columns:\n",
        "        raise ValueError(f\"Label column '{label_col}' not found in dataset.\")\n",
        "    df[label_col] = df[label_col].astype(int) # ensure integer labels\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "\n",
        "    log(\"Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test, X.columns # Return original column names\n",
        "\n",
        "# 2. Refactor Data Scaling\n",
        "def scale_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Scales the training and testing data using StandardScaler.\n",
        "    Returns the fitted scaler and the scaled data.\n",
        "    \"\"\"\n",
        "    log(\"Scaling data...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return scaler, X_train_scaled, X_test_scaled\n",
        "\n",
        "# 3. Refactor Anomaly Feature Extraction\n",
        "def extract_unsupervised_features(X_train_scaled, X_test_scaled, y_train, unsupervised_params, random_state):\n",
        "    \"\"\"\n",
        "    Train unsupervised detectors and produce anomaly features for train/test.\n",
        "    Returns features_train (DataFrame), features_test (DataFrame), unsupervised_models (dict).\n",
        "    unsupervised_models will include 'train_X' stored as numpy array to support inference mapping.\n",
        "    \"\"\"\n",
        "    log(\"Extracting anomaly features...\")\n",
        "    features_train = pd.DataFrame(index=np.arange(X_train_scaled.shape[0]))\n",
        "    features_test = pd.DataFrame(index=np.arange(X_test_scaled.shape[0]))\n",
        "\n",
        "    # Isolation Forest\n",
        "    log(\"Fitting IsolationForest...\")\n",
        "    iso = IsolationForest(contamination=unsupervised_params['contamination'], random_state=random_state)\n",
        "    iso.fit(X_train_scaled)\n",
        "    features_train['iso_df'] = iso.decision_function(X_train_scaled)\n",
        "    features_test['iso_df'] = iso.decision_function(X_test_scaled)\n",
        "\n",
        "    # One-Class SVM\n",
        "    log(\"Fitting One-Class SVM...\")\n",
        "    ocsvm = OneClassSVM(nu=unsupervised_params['contamination'], kernel='rbf', gamma='scale')\n",
        "    ocsvm.fit(X_train_scaled)\n",
        "    features_train['ocsvm_df'] = ocsvm.decision_function(X_train_scaled)\n",
        "    features_test['ocsvm_df'] = ocsvm.decision_function(X_test_scaled)\n",
        "\n",
        "    # Local Outlier Factor (novelty True so we can use decision_function/predict)\n",
        "    log(\"Fitting Local Outlier Factor (LOF)...\")\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=unsupervised_params['contamination'], novelty=True)\n",
        "    lof.fit(X_train_scaled)\n",
        "    features_train['lof_df'] = lof.decision_function(X_train_scaled)\n",
        "    features_test['lof_df'] = lof.decision_function(X_test_scaled)\n",
        "\n",
        "    # DBSCAN anomaly flag with nearest neighbor assignment for test set\n",
        "    log(\"Running DBSCAN clustering...\")\n",
        "    db = DBSCAN(eps=unsupervised_params['dbscan_eps'], min_samples=unsupervised_params['dbscan_min_samples'])\n",
        "    db_labels_train = db.fit_predict(X_train_scaled)  # labels for training samples\n",
        "    # nearest neighbor mapping from test samples -> nearest train index\n",
        "    nbrs = NearestNeighbors(n_neighbors=1).fit(X_train_scaled) # Use unsupervised_models['train_X']\n",
        "    nn_idx = nbrs.kneighbors(X_test_scaled, return_distance=False)[:, 0]\n",
        "    assigned_train_labels = db_labels_train[nn_idx]\n",
        "    features_train['dbscan_anomaly'] = (db_labels_train == -1).astype(float)\n",
        "    features_test['dbscan_anomaly'] = (assigned_train_labels == -1).astype(float)\n",
        "\n",
        "    # KMeans distances to cluster centers\n",
        "    log(\"Running KMeans clustering...\")\n",
        "    kmeans = KMeans(n_clusters=unsupervised_params['kmeans_clusters'], random_state=random_state)\n",
        "    kmeans.fit(X_train_scaled)\n",
        "    train_k_labels = kmeans.predict(X_train_scaled)\n",
        "    test_k_labels = kmeans.predict(X_test_scaled)\n",
        "    train_distances = np.linalg.norm(X_train_scaled - kmeans.cluster_centers_[train_k_labels], axis=1)\n",
        "    test_distances = np.linalg.norm(X_test_scaled - kmeans.cluster_centers_[test_k_labels], axis=1)\n",
        "    features_train['kmeans_dist'] = train_distances\n",
        "    features_test['kmeans_dist'] = test_distances\n",
        "\n",
        "    # Dense Autoencoder reconstruction error\n",
        "    log(\"Training Dense Autoencoder...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    dense_ae = build_dense_autoencoder(input_dim)\n",
        "    # Define \"normal\" mask (update threshold to suit your label encoding)\n",
        "    normal_mask = (y_train <= 1).to_numpy() if hasattr(y_train, \"to_numpy\") else (y_train <= 1)\n",
        "    X_ae_train = X_train_scaled[normal_mask]\n",
        "    # Only use validation_split if we have enough samples\n",
        "    fit_kwargs = {\"epochs\": unsupervised_params['autoencoder_epochs'], \"batch_size\": unsupervised_params['autoencoder_batch'], \"verbose\": 0,\n",
        "                  \"callbacks\": [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]}\n",
        "    if len(X_ae_train) > 50:\n",
        "        fit_kwargs[\"validation_split\"] = 0.1\n",
        "    dense_ae.fit(X_ae_train, X_ae_train, **fit_kwargs)\n",
        "    features_train['ae_mse'] = np.mean((X_train_scaled - dense_ae.predict(X_train_scaled, verbose=0)) ** 2, axis=1)\n",
        "    features_test['ae_mse'] = np.mean((X_test_scaled - dense_ae.predict(X_test_scaled, verbose=0)) ** 2, axis=1)\n",
        "\n",
        "    # LSTM Autoencoder reconstruction error (reshape sequences with timesteps=1)\n",
        "    log(\"Training LSTM Autoencoder...\")\n",
        "    timesteps = 1\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    X_train_seq = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, input_dim))\n",
        "    X_test_seq = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, input_dim))\n",
        "    lstm_ae = build_lstm_autoencoder(timesteps, input_dim)\n",
        "    fit_kwargs_lstm = {\"epochs\": unsupervised_params['lstm_epochs'], \"batch_size\": unsupervised_params['lstm_batch'], \"verbose\": 0,\n",
        "                       \"callbacks\": [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]}\n",
        "    if len(X_ae_train) > 50:\n",
        "        fit_kwargs_lstm[\"validation_split\"] = 0.1\n",
        "    lstm_ae.fit(X_train_seq[normal_mask], X_train_seq[normal_mask], **fit_kwargs_lstm)\n",
        "    features_train['lstm_mse'] = np.mean((X_train_seq - lstm_ae.predict(X_train_seq, verbose=0)) ** 2, axis=(1, 2))\n",
        "    features_test['lstm_mse'] = np.mean((X_test_seq - lstm_ae.predict(X_test_seq, verbose=0)) ** 2, axis=(1, 2))\n",
        "\n",
        "    unsupervised_models = {\n",
        "        'iso': iso, 'ocsvm': ocsvm, 'lof': lof, 'dbscan': db,\n",
        "        'kmeans': kmeans, 'dense_ae': dense_ae, 'lstm_ae': lstm_ae,\n",
        "        'train_X': np.asarray(X_train_scaled)  # save training X scaled for inference mapping\n",
        "    }\n",
        "\n",
        "    return features_train, features_test, unsupervised_models\n",
        "\n",
        "# 4. Refactor Stacked Model Training\n",
        "def train_stacked_model(X_train_ext, X_test_ext, y_train, random_state):\n",
        "    \"\"\"\n",
        "    Trains the Random Forest base model and the Gradient Boosting meta-learner.\n",
        "    Returns the trained base and meta models.\n",
        "    \"\"\"\n",
        "    log(\"Training stacked supervised model...\")\n",
        "    log(\"  Training RandomForest base model...\")\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1)\n",
        "    rf.fit(X_train_ext, y_train)\n",
        "    rf_train_proba = rf.predict_proba(X_train_ext)\n",
        "    rf_test_proba = rf.predict_proba(X_test_ext)\n",
        "\n",
        "    X_train_stack = np.hstack([X_train_ext.values, rf_train_proba])\n",
        "    X_test_stack = np.hstack([X_test_ext.values, rf_test_proba])\n",
        "\n",
        "    log(\"  Training GradientBoosting meta model...\")\n",
        "    gb = GradientBoostingClassifier(n_estimators=200, random_state=random_state)\n",
        "    gb.fit(X_train_stack, y_train)\n",
        "\n",
        "    return rf, gb, X_test_stack\n",
        "\n",
        "# 5. Refactor Stacked Model Evaluation\n",
        "def evaluate_stacked_model(model_name, y_test, y_pred, model_output_dir):\n",
        "    \"\"\"\n",
        "    Evaluates the stacked model and saves the metrics.\n",
        "    \"\"\"\n",
        "    log(f\"Evaluating {model_name}...\")\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Use the existing print_model_performance_report for comprehensive output\n",
        "    print_model_performance_report(model_name, y_test, y_pred)\n",
        "\n",
        "    # Save metrics to JSON\n",
        "    metrics_path = os.path.join(model_output_dir, \"metrics.json\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump({\"accuracy\": acc, \"classification_report\": report_dict,\n",
        "                   \"confusion_matrix\": cm.tolist()}, f, indent=4)\n",
        "    log(f\"Saved evaluation metrics to {metrics_path}\")\n",
        "\n",
        "def visualize_all_unsupervised_models(X_test_scaled, y_test, unsupervised_models, X_columns):\n",
        "    \"\"\"\n",
        "    Visualizes the performance of each unsupervised model using scatter plots,\n",
        "    ROC curves, and Precision-Recall curves in separate subplots.\n",
        "    \"\"\"\n",
        "    log(\"Visualizing performance for all unsupervised models...\")\n",
        "\n",
        "    unsupervised_model_names_eval = [\"IsolationForest\", \"OneClassSVM\", \"LocalOutlierFactor\", \"DBSCAN\", \"KMeans\", \"Autoencoder\", \"LSTM(Classifier)\"]\n",
        "    model_mapping_eval = {\n",
        "        \"IsolationForest\": unsupervised_models.get('iso'),\n",
        "        \"OneClassSVM\": unsupervised_models.get('ocsvm'),\n",
        "        \"LocalOutlierFactor\": unsupervised_models.get('lof'),\n",
        "        \"DBSCAN\": unsupervised_models.get('dbscan'),\n",
        "        \"KMeans\": unsupervised_models.get('kmeans'),\n",
        "        \"Autoencoder\": unsupervised_models.get('dense_ae'),\n",
        "        \"LSTM(Classifier)\": unsupervised_models.get('lstm_ae')\n",
        "    }\n",
        "\n",
        "    # Prepare the true anomaly labels for the test set\n",
        "    y_test_binary_anomaly = (y_test.reset_index(drop=True) >= 2).astype(int)\n",
        "\n",
        "    for name in unsupervised_model_names_eval:\n",
        "        model = model_mapping_eval.get(name)\n",
        "        if model is None:\n",
        "            log(f\"Skipping visualization for {name}: Model not found.\")\n",
        "            continue\n",
        "\n",
        "        log(f\"  Generating visualization data for {name}...\")\n",
        "\n",
        "        # Get anomaly scores and predicted anomalies (if available) for the current model\n",
        "        # Use evaluate_unsupervised_model_corrected to get the necessary data for visualization\n",
        "        anomaly_score, is_anomaly, _ = evaluate_unsupervised_model_corrected(\n",
        "             name, model, X_test_scaled, y_test, unsupervised_models # Pass unsupervised_models here\n",
        "        )\n",
        "\n",
        "        # Check if anomaly_score is available\n",
        "        if anomaly_score is None:\n",
        "            log(f\"Skipping visualization for {name}: Anomaly score not available.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare data for visualization\n",
        "        viz_data = pd.DataFrame(X_test_scaled, columns=X_columns)\n",
        "        viz_data['true_anomaly'] = y_test_binary_anomaly\n",
        "        viz_data['anomaly_score'] = anomaly_score\n",
        "\n",
        "        if is_anomaly is not None:\n",
        "            viz_data['predicted_anomaly'] = is_anomaly\n",
        "        else:\n",
        "             # Use median as a simple threshold for visualization if binary labels are not directly available\n",
        "            threshold = np.median(anomaly_score)\n",
        "            viz_data['predicted_anomaly'] = (anomaly_score > threshold).astype(int)\n",
        "\n",
        "\n",
        "        # Visualize the model's performance using the existing pipeline\n",
        "        log(f\"  Visualizing performance for {name}...\")\n",
        "        # Pass the correct arguments to visualizing_model_performance_pipeline\n",
        "        visualizing_model_performance_pipeline(\n",
        "            viz_data, # Use the prepared viz_data DataFrame\n",
        "            'Threat Score' if 'Threat Score' in viz_data.columns else viz_data.columns[0], # Use 'Threat Score' if available, otherwise the first column\n",
        "            'Impact Score' if 'Impact Score' in viz_data.columns else viz_data.columns[1],  # Use 'Impact Score' if available, otherwise the second column\n",
        "            'anomaly_score', # Pass the anomaly score column name\n",
        "            'true_anomaly', # Pass the true anomaly label column name\n",
        "            name, # Pass the model name\n",
        "            title=f\"{name} Performance Visualization\" # Set the title\n",
        "        )\n",
        "\n",
        "    log(\"Finished visualizing performance for all unsupervised models.\")\n",
        "\n",
        "# 6. Refactor Unsupervised Model Evaluation and Visualization\n",
        "def evaluate_and_visualize_unsupervised_models(X_test_scaled, y_test, unsupervised_models, X_columns):\n",
        "    \"\"\"\n",
        "    Evaluates all unsupervised models, identifies the best, and visualizes its performance.\n",
        "    Also generates a comparative performance analysis of all unsupervised models.\n",
        "    \"\"\"\n",
        "    print(\"\\n---Unsupervised Model Evaluation and Identification of the Best Model ---\\n\")\n",
        "\n",
        "    log(\"Evaluating unsupervised models and identifying the best...\")\n",
        "\n",
        "    #---------------------------------------------------\n",
        "    # Call the new function to visualize all unsupervised models\n",
        "    # Ensure X_test_scaled, y_test, unsupervised_models, and X_columns are available from the previous run\n",
        "\n",
        "    if 'X_test_scaled' in locals() and 'y_test' in locals() and 'unsupervised_models' in locals() and 'X_columns' in locals():\n",
        "        visualize_all_unsupervised_models(X_test_scaled, y_test, unsupervised_models, X_columns)\n",
        "    else:\n",
        "        print(\"Required variables (X_test_scaled, y_test, unsupervised_models, X_columns) are not available. Please run the main pipeline first.\")\n",
        "    #-------------------------------------------------\n",
        "    # Call get_best_unsupervised_model_corrected with only 4 arguments\n",
        "    best_unsupervised_model_name, best_anomaly_score, best_is_anomaly = get_best_unsupervised_model_corrected(X_test_scaled, y_test, unsupervised_models)\n",
        "    print(f\"\\nBest unsupervised model for anomaly detection: {best_unsupervised_model_name}\")\n",
        "\n",
        "    # Prepare data for visualization of the best unsupervised model\n",
        "    viz_data = pd.DataFrame(X_test_scaled, columns=X_columns)\n",
        "    viz_data['true_anomaly'] = (y_test.reset_index(drop=True) >= 2).astype(int)\n",
        "    viz_data['anomaly_score'] = best_anomaly_score\n",
        "\n",
        "    if best_is_anomaly is not None:\n",
        "        viz_data['predicted_anomaly'] = best_is_anomaly\n",
        "    else:\n",
        "        # Use median as a simple threshold for visualization if binary labels are not directly available\n",
        "        if best_anomaly_score is None:\n",
        "            print(\"[ERROR] No valid unsupervised model was selected. Skipping visualization.\")\n",
        "            # Return None to indicate no visualization data was generated\n",
        "            viz_data = None\n",
        "        else:\n",
        "            threshold = np.median(best_anomaly_score)\n",
        "            viz_data['predicted_anomaly'] = (best_anomaly_score > threshold).astype(int)\n",
        "\n",
        "    # Visualize the best unsupervised model's performance\n",
        "    if viz_data is not None:\n",
        "        log(f\"Visualizing performance for the best unsupervised model ({best_unsupervised_model_name})...\")\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        fig.suptitle(f\"{best_unsupervised_model_name} Performance Visualization\\n\")\n",
        "\n",
        "        # Check if 'Threat Score' and 'Impact Score' columns exist before plotting\n",
        "        if 'Threat Score' in viz_data.columns and 'Impact Score' in viz_data.columns:\n",
        "            create_scatter_plot(viz_data, 'Threat Score', 'Impact Score', hue='true_anomaly', ax=ax1)\n",
        "        else:\n",
        "            log(\"Warning: 'Threat Score' or 'Impact Score' not found in visualization data. Skipping scatter plot.\")\n",
        "            # Optionally, create a placeholder or plot different columns\n",
        "            ax1.set_title(\"Scatter Plot Skipped (Missing Columns)\")\n",
        "            ax1.text(0.5, 0.5, \"Missing 'Threat Score' or 'Impact Score'\", horizontalalignment='center', verticalalignment='center', transform=ax1.transAxes)\n",
        "\n",
        "\n",
        "        # Ensure anomaly_score column exists before creating ROC and PR curves\n",
        "        if 'anomaly_score' in viz_data.columns and viz_data['anomaly_score'] is not None:\n",
        "             create_roc_curve(viz_data, 'anomaly_score', 'true_anomaly', ax=ax2)\n",
        "             create_precision_recall_curve(viz_data, 'anomaly_score', 'true_anomaly', ax=ax3)\n",
        "        else:\n",
        "             log(\"Warning: Anomaly score not available for visualization. Skipping ROC and Precision-Recall curves.\")\n",
        "             ax2.set_title(\"ROC Curve Skipped (No Anomaly Score)\")\n",
        "             ax2.text(0.5, 0.5, \"No Anomaly Score Available\", horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n",
        "             ax3.set_title(\"Precision-Recall Curve Skipped (No Anomaly Score)\")\n",
        "             ax3.text(0.5, 0.5, \"No Anomaly Score Available\", horizontalalignment='center', verticalalignment='center', transform=ax3.transAxes)\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        if 'anomaly_score' in viz_data.columns and viz_data['anomaly_score'] is not None:\n",
        "            generate_visualization_insight(viz_data, 'anomaly_score', 'true_anomaly', best_unsupervised_model_name)\n",
        "        else:\n",
        "             log(\"Skipping visualization insight generation due to missing anomaly scores.\")\n",
        "\n",
        "\n",
        "    # 7. Comparative Performance Analysis and Visualization of Unsupervised Models\n",
        "    log(\"Generating comparative performance analysis...\")\n",
        "    unsupervised_performance = {}\n",
        "    unsupervised_model_names_eval = [\"IsolationForest\", \"OneClassSVM\", \"LocalOutlierFactor\", \"DBSCAN\", \"KMeans\", \"Autoencoder\", \"LSTM(Classifier)\"]\n",
        "    model_mapping_eval = {\n",
        "        \"IsolationForest\": unsupervised_models.get('iso'),\n",
        "        \"OneClassSVM\": unsupervised_models.get('ocsvm'),\n",
        "        \"LocalOutlierFactor\": unsupervised_models.get('lof'),\n",
        "        \"DBSCAN\": unsupervised_models.get('dbscan'),\n",
        "        \"KMeans\": unsupervised_models.get('kmeans'),\n",
        "        \"Autoencoder\": unsupervised_models.get('dense_ae'),\n",
        "        \"LSTM(Classifier)\": unsupervised_models.get('lstm_ae')\n",
        "    }\n",
        "\n",
        "    for name in unsupervised_model_names_eval:\n",
        "        model = model_mapping_eval.get(name)\n",
        "        if model is None:\n",
        "            log(f\"Skipping evaluation for {name}: Model not found.\")\n",
        "            continue\n",
        "\n",
        "        log(f\"  Evaluating {name}...\")\n",
        "        # Call evaluate_unsupervised_model_corrected with only 4 arguments\n",
        "        anomaly_score, is_anomaly, performance_metric = evaluate_unsupervised_model_corrected(\n",
        "            name, model, X_test_scaled, y_test, unsupervised_models # Add unsupervised_models here\n",
        "        )\n",
        "\n",
        "        if performance_metric is not None:\n",
        "            unsupervised_performance[name] = performance_metric\n",
        "            log(f\"    {name} performance ({'AUC' if name not in ['DBSCAN'] else 'F1'}): {performance_metric:.4f}\")\n",
        "        else:\n",
        "            log(f\"    Could not evaluate {name} performance.\")\n",
        "\n",
        "    # Display comparative table\n",
        "    log(\"  Displaying unsupervised model performance table...\")\n",
        "    unsupervised_performance_df = pd.DataFrame(list(unsupervised_performance.items()), columns=['Model', 'Performance Metric'])\n",
        "    display(unsupervised_performance_df)\n",
        "\n",
        "    # Display comparative chart\n",
        "    log(\"  Displaying unsupervised model performance chart...\")\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.barplot(x='Model', y='Performance Metric', data=unsupervised_performance_df, palette='viridis')\n",
        "    plt.title('Unsupervised Model Performance Comparison')\n",
        "    plt.xlabel('Unsupervised Model')\n",
        "    plt.ylabel('Performance Metric')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Generate insights from comparative analysis\n",
        "    print(\"\\n--- Insights from Unsupervised Model Performance Comparison ---\")\n",
        "\n",
        "    # Filter out models where performance metric was None\n",
        "    evaluable_models_df = unsupervised_performance_df.dropna(subset=['Performance Metric'])\n",
        "\n",
        "    if not evaluable_models_df.empty:\n",
        "        # Find the best performing model among those successfully evaluated\n",
        "        best_model_row = evaluable_models_df.loc[evaluable_models_df['Performance Metric'].idxmax()]\n",
        "        best_model_name_comp = best_model_row['Model']\n",
        "        best_performance_comp = best_model_row['Performance Metric']\n",
        "\n",
        "        # Find the worst performing model among those successfully evaluated\n",
        "        worst_model_row = evaluable_models_df.loc[evaluable_models_df['Performance Metric'].idxmin()]\n",
        "        worst_model_name_comp = worst_model_row['Model']\n",
        "        worst_performance_comp = worst_model_row['Performance Metric']\n",
        "\n",
        "\n",
        "        print(f\"Best Performing Model: {best_model_name_comp} (Metric: {best_performance_comp:.4f})\")\n",
        "        print(f\"Worst Performing Model: {worst_model_name_comp} (Metric: {worst_performance_comp:.4f})\")\n",
        "\n",
        "        autoencoder_performance_df = evaluable_models_df[evaluable_models_df['Model'].isin(['Autoencoder', 'LSTM(Classifier)'])]\n",
        "        autoencoder_performance = autoencoder_performance_df['Performance Metric'].mean() if not autoencoder_performance_df.empty else float('nan')\n",
        "\n",
        "        clustering_performance_df = evaluable_models_df[evaluable_models_df['Model'].isin(['DBSCAN', 'KMeans'])]\n",
        "        clustering_performance = clustering_performance_df['Performance Metric'].mean() if not clustering_performance_df.empty else float('nan')\n",
        "\n",
        "        tree_based_performance_df = evaluable_models_df[evaluable_models_df['Model'].isin(['IsolationForest', 'LocalOutlierFactor'])]\n",
        "        tree_based_performance = tree_based_performance_df['Performance Metric'].mean() if not tree_based_performance_df.empty else float('nan')\n",
        "\n",
        "        svm_performance_df = evaluable_models_df[evaluable_models_df['Model'].isin(['OneClassSVM'])]\n",
        "        svm_performance = svm_performance_df['Performance Metric'].mean() if not svm_performance_df.empty else float('nan')\n",
        "\n",
        "\n",
        "        print(\"\\nPerformance Comparison by Model Type:\")\n",
        "        print(f\"- Autoencoder Models (Mean Metric): {autoencoder_performance:.4f}\")\n",
        "        print(f\"- Clustering Models (Mean Metric): {clustering_performance:.4f}\")\n",
        "        print(f\"- Tree-based Models (Mean Metric): {tree_based_performance:.4f}\")\n",
        "        print(f\"- One-Class SVM (Metric): {svm_performance:.4f}\")\n",
        "\n",
        "        print(\"\\nPotential Reasons for Performance Differences:\")\n",
        "        print(\"- Autoencoders (especially LSTM) might be better at capturing complex, non-linear relationships and temporal patterns in the data, which is relevant for anomaly detection in time-series-like threat data.\")\n",
        "        print(\"- Clustering methods like DBSCAN and KMeans might struggle if anomalies don't form distinct clusters or if the concept of 'normal' varies significantly.\")\n",
        "        print(\"- Tree-based methods (Isolation Forest, LOF) are often effective but might be sensitive to feature scaling and hyperparameters.\")\n",
        "        print(\"- One-Class SVM is designed for novelty detection but its performance can be sensitive to the choice of kernel and hyperparameters.\")\n",
        "        print(\"- The specific nature of the anomalies in this dataset (e.g., their density, shape, and relationship to normal data) heavily influences which model type is most suitable.\")\n",
        "\n",
        "        baseline_auc = 0.5 # Random guessing for AUC\n",
        "        print(\"\\nComparison to Baseline:\")\n",
        "        print(f\"- The best model ({best_model_name_comp}) achieved a metric of {best_performance_comp:.4f}.\")\n",
        "        auc_models_df = evaluable_models_df[evaluable_models_df['Model'].isin(['IsolationForest', 'OneClassSVM', 'LocalOutlierFactor', 'KMeans', 'Autoencoder', 'LSTM(Classifier)'])] # Include autoencoders and LSTM for AUC comparison\n",
        "        if not auc_models_df.empty:\n",
        "            best_auc = auc_models_df['Performance Metric'].max()\n",
        "            print(f\"- Compared to a random guessing baseline (AUC = {baseline_auc}), the best AUC model performs {'better' if best_auc > baseline_auc else 'worse' if best_auc < baseline_auc else 'similarly'}.\")\n",
        "        else:\n",
        "            print(\"- No AUC-based models were evaluated for baseline comparison.\")\n",
        "\n",
        "\n",
        "        print(\"\\nImplications for the Stacked Model:\")\n",
        "        print(f\"- The anomaly features generated by the unsupervised models, particularly the best performing ones like {best_model_name_comp}, will be fed into the supervised meta-learner.\")\n",
        "        print(\"- Higher quality anomaly features (from better performing unsupervised models) are likely to improve the performance of the stacked supervised model, especially in distinguishing between different threat levels.\")\n",
        "        print(\"- The meta-learner can learn to leverage the strengths of different anomaly detection signals from the various unsupervised models.\")\n",
        "        print(\"- While the individual unsupervised models might not be perfect anomaly detectors on their own, their outputs serve as valuable additional features for the supervised layer.\")\n",
        "    else:\n",
        "        print(\"No unsupervised models were successfully evaluated for comparative analysis.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- End of Insights ---\")\n",
        "\n",
        "# 7. Refactor Saving Models and Metrics\n",
        "def save_all_models_and_metrics(output_dir, scaler, base_model, meta_model, unsupervised_models, metrics_path):\n",
        "    \"\"\"\n",
        "    Saves the scaler, all trained models, and evaluation metrics.\n",
        "    \"\"\"\n",
        "    log(\"Saving models and metrics...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
        "    joblib.dump(base_model, os.path.join(output_dir, \"rf_base.joblib\"))\n",
        "    joblib.dump(meta_model, os.path.join(output_dir, \"gb_meta.joblib\"))\n",
        "    # save classical unsupervised models\n",
        "    for name in ['iso', 'ocsvm', 'lof', 'dbscan', 'kmeans']:\n",
        "        joblib.dump(unsupervised_models[name], os.path.join(output_dir, f\"{name}.joblib\"))\n",
        "    # save train_X_scaled for DBSCAN mapping\n",
        "    np.save(os.path.join(output_dir, \"train_X_scaled.npy\"), unsupervised_models['train_X'])\n",
        "    # save Keras models in native format (.keras)\n",
        "    dense_path = os.path.join(output_dir, \"dense_autoencoder.keras\")\n",
        "    lstm_path = os.path.join(output_dir, \"lstm_autoencoder.keras\")\n",
        "    unsupervised_models['dense_ae'].save(dense_path)\n",
        "    unsupervised_models['lstm_ae'].save(lstm_path)\n",
        "    log(f\"  Scaler and ALL models saved in '{output_dir}'\")\n",
        "\n",
        "    # Metrics are already saved in evaluate_stacked_model, just log the path\n",
        "    log(f\"Evaluation metrics saved in {metrics_path}\")\n",
        "#------------------\n",
        "#  Main pipelines\n",
        "#------------------\n",
        "def run_stacked_model_pipeline_integrated():\n",
        "    \"\"\"\n",
        "    Orchestrates the entire data science process for the stacked model,\n",
        "    integrating the steps previously in main_model_dev_stacked.\n",
        "\n",
        "    Includes:\n",
        "    1. Data Loading\n",
        "    2. Data Splitting and Scaling\n",
        "    3. Anomaly Feature Extraction using Unsupervised Models\n",
        "    4. Stacked Supervised Model Training (Random Forest base, Gradient Boosting meta)\n",
        "    5. Stacked Model Evaluation\n",
        "    6. Unsupervised Model Evaluation and Identification of the Best Model\n",
        "    7. Comparative Performance Analysis and Visualization of Unsupervised Models\n",
        "    8. Saving Scaler, Models, and Metrics\n",
        "    \"\"\"\n",
        "    log(\"Starting integrated stacked model pipeline...\")\n",
        "\n",
        "    # 1. Data Loading and Splitting\n",
        "    X_train, X_test, y_train, y_test, X_columns = load_and_split_data(DATA_PATH, LABEL_COL, TEST_SIZE, RANDOM_STATE)\n",
        "\n",
        "    # 2. Data Scaling\n",
        "    scaler, X_train_scaled, X_test_scaled = scale_data(X_train, X_test)\n",
        "\n",
        "    # 3. Anomaly Feature Extraction\n",
        "    unsupervised_params = {\n",
        "        'contamination': CONTAMINATION,\n",
        "        'dbscan_eps': DBSCAN_EPS,\n",
        "        'dbscan_min_samples': DBSCAN_MIN_SAMPLES,\n",
        "        'kmeans_clusters': KMEANS_CLUSTERS,\n",
        "        'autoencoder_epochs': AUTOENCODER_EPOCHS,\n",
        "        'autoencoder_batch': AUTOENCODER_BATCH,\n",
        "        'lstm_epochs': LSTM_EPOCHS,\n",
        "        'lstm_batch': LSTM_BATCH\n",
        "    }\n",
        "    anomaly_train_df, anomaly_test_df, unsupervised_models = extract_unsupervised_features(\n",
        "        X_train_scaled, X_test_scaled, y_train, unsupervised_params, RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    X_train_ext = pd.concat([pd.DataFrame(X_train_scaled, columns=X_columns), anomaly_train_df], axis=1)\n",
        "    X_test_ext = pd.concat([pd.DataFrame(X_test_scaled, columns=X_columns), anomaly_test_df], axis=1)\n",
        "\n",
        "    # 4. Stacked Supervised Model Training\n",
        "    rf, gb, X_test_stack = train_stacked_model(X_train_ext, X_test_ext, y_train, RANDOM_STATE)\n",
        "\n",
        "    # 5. Stacked Model Evaluation\n",
        "    y_pred = gb.predict(X_test_stack)\n",
        "    evaluate_stacked_model(type(gb).__name__, y_test, y_pred, MODEL_OUTPUT_DIR)\n",
        "    metrics_path = os.path.join(MODEL_OUTPUT_DIR, \"metrics.json\")\n",
        "\n",
        "\n",
        "    # 6. Unsupervised Model Evaluation and Visualization\n",
        "    evaluate_and_visualize_unsupervised_models(X_test_scaled, y_test, unsupervised_models, X_columns)\n",
        "\n",
        "\n",
        "    # 8. Saving Scaler, Models, and Metrics\n",
        "    save_all_models_and_metrics(MODEL_OUTPUT_DIR, scaler, rf, gb, unsupervised_models, metrics_path)\n",
        "\n",
        "    log(\"Stacked model pipeline execution finished.\")\n",
        "\n",
        "\n",
        "# To run the integrated pipeline, you would call:\n",
        "if __name__ == \"__main__\":\n",
        "    run_stacked_model_pipeline_integrated()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd82526"
      },
      "source": [
        "# --------------------------\n",
        "#   Necessary Imports\n",
        "# --------------------------\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import json # Import json\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.neighbors import NearestNeighbors # Import NearestNeighbors\n",
        "import sys # Import sys\n",
        "\n",
        "# Add the directory containing your custom modules to the Python path\n",
        "# Assuming your modules are in 'cyber_insights_web' folder directly under 'My Drive'\n",
        "custom_modules_path = \"/content/drive/My Drive/\"\n",
        "if custom_modules_path not in sys.path:\n",
        "    sys.path.append(custom_modules_path)\n",
        "\n",
        "#--------------------------------\n",
        "from cyber_insights_web.feature_engeneering.f_engineering import (load_objects_from_drive,\n",
        "                                               features_engineering_pipeline,\n",
        "                                               data_augmentation_pipeline)\n",
        "\n",
        "from cyber_insights_web.model_development.model_dev_stack_vers_03.stacked_anomaly_detection_classifier import (\n",
        "    print_model_performance_report, visualizing_model_performance_pipeline\n",
        ")\n",
        "\n",
        "#---------------------------\n",
        "# --------------------------\n",
        "#   Configurations\n",
        "# --------------------------\n",
        "#Stacked Supervised Model using Unsupervised Anomaly Features\n",
        "MODEL_TYPE = \"Stacked Supervised Model using Unsupervised Anomaly Features\"\n",
        "MODEL_NAME = \"2SM2UAF_model\"\n",
        "THREASHHOLD_PERC = 95\n",
        "LABEL_COL = \"Threat Level\"  # Ground truth label column name\n",
        "#MODELS_DIR = \"/content/drive/My Drive/stacked_models_deployment\"\n",
        "MODEL_OUTPUT_DIR = \"/content/drive/My Drive/stacked_models_deployment\"\n",
        "SIMULATED_REAL_TIME_DATA_FILE = \\\n",
        "                         \"/content/drive/My Drive/Cybersecurity Data/normal_and_anomalous_cybersecurity_dataset_for_google_drive_kb.csv\"\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "#   Ensure model directory exists\n",
        "# --------------------------\n",
        "if not os.path.exists(MODEL_OUTPUT_DIR):\n",
        "    os.makedirs(MODEL_OUTPUT_DIR)\n",
        "\n",
        "# --------------------------\n",
        "#   Logging Function\n",
        "# --------------------------\n",
        "def log(msg):\n",
        "    \"\"\"Logs a message to both console and a file in MODEL_OUTPUT_DIR.\"\"\"\n",
        "    print(f\"[INFO] {msg}\")\n",
        "    with open(os.path.join(MODEL_OUTPUT_DIR, \"log.txt\"), \"a\") as f:\n",
        "        f.write(f\"{msg}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "#   Check Required Model Files (Table View)\n",
        "# --------------------------\n",
        "def check_required_files(output_dir):\n",
        "    \"\"\"Checks if all model/scaler files exist before loading, shows table.\"\"\"\n",
        "    required_files = [\n",
        "        \"scaler.joblib\", \"rf_base.joblib\", \"gb_meta.joblib\",\n",
        "        \"iso.joblib\", \"ocsvm.joblib\", \"lof.joblib\", \"dbscan.joblib\", \"kmeans.joblib\",\n",
        "        \"train_X_scaled.npy\", \"dense_autoencoder.keras\", \"lstm_autoencoder.keras\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n Checking Required Model Files:\\n\" + \"-\" * 50)\n",
        "    missing_files = []\n",
        "\n",
        "    for f in required_files:\n",
        "        file_path = os.path.join(output_dir, f)\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\" {f}  FOUND\")\n",
        "        else:\n",
        "            print(f\" {f}  MISSING\")\n",
        "            missing_files.append(f)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(f\"\\nMissing required model files:\\n - \" + \"\\n - \".join(missing_files))\n",
        "\n",
        "# --------------------------\n",
        "#   Load Trained Features\n",
        "# --------------------------\n",
        "def load_treaned_features(scaler, input_data):\n",
        "    \"\"\"Ensures new data matches the trained feature set.\"\"\"\n",
        "    log(\"Loading trained features...\")\n",
        "\n",
        "    if isinstance(input_data, str):\n",
        "        if not os.path.exists(input_data):\n",
        "            raise FileNotFoundError(f\"Input CSV file not found: {input_data}\")\n",
        "        df = pd.read_csv(input_data)\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data.copy()\n",
        "    else:\n",
        "        raise TypeError(\"input_data must be a filepath or a pandas DataFrame.\")\n",
        "\n",
        "    if LABEL_COL in df.columns:\n",
        "        df = df.drop(columns=[LABEL_COL])\n",
        "\n",
        "    trained_feature_names = list(scaler.feature_names_in_)\n",
        "    X_new = df[[c for c in df.columns if c in trained_feature_names]].copy()\n",
        "\n",
        "    for col in trained_feature_names:\n",
        "        if col not in X_new.columns:\n",
        "            X_new[col] = 0\n",
        "\n",
        "    X_new = X_new[trained_feature_names]\n",
        "    return X_new\n",
        "\n",
        "# --------------------------\n",
        "#   Load Scaler and Models\n",
        "# --------------------------\n",
        "def load_scaler_and_models(output_dir):\n",
        "    \"\"\"Loads scaler, supervised models, and unsupervised models from output_dir.\"\"\"\n",
        "    check_required_files(output_dir)  # Ensure all files exist before loading\n",
        "\n",
        "    scaler = joblib.load(os.path.join(output_dir, \"scaler.joblib\"))\n",
        "    base_model = joblib.load(os.path.join(output_dir, \"rf_base.joblib\"))\n",
        "    meta_model = joblib.load(os.path.join(output_dir, \"gb_meta.joblib\"))\n",
        "\n",
        "    unsupervised_models = {}\n",
        "    for name in ['iso', 'ocsvm', 'lof', 'dbscan', 'kmeans']:\n",
        "        unsupervised_models[name] = joblib.load(os.path.join(output_dir, f\"{name}.joblib\"))\n",
        "\n",
        "    unsupervised_models['train_X'] = np.load(os.path.join(output_dir, \"train_X_scaled.npy\"))\n",
        "    unsupervised_models['dense_ae'] = load_model(os.path.join(output_dir, \"dense_autoencoder.keras\"))\n",
        "    unsupervised_models['lstm_ae'] = load_model(os.path.join(output_dir, \"lstm_autoencoder.keras\"))\n",
        "\n",
        "    return scaler, base_model, meta_model, unsupervised_models\n",
        "\n",
        "#----------------------------------\n",
        "# Encode Simulated Real Time Data\n",
        "#----------------------------------\n",
        "\n",
        "def encode_simulated_real_time_data(df_p, LABEL_COL):\n",
        "\n",
        "    df = df_p.copy()\n",
        "    fe_processed_df, loaded_label_encoders, num_fe_scaler = load_objects_from_drive()\n",
        "    features_engineering_columns = fe_processed_df.columns.tolist()\n",
        "    input_feature_column = [col for col in features_engineering_columns if col != LABEL_COL]\n",
        "    features_engineering_columns.remove(LABEL_COL)\n",
        "\n",
        "    #encode features using loaded_label_encoders and num_fe_scaler\n",
        "    for col, encoder in loaded_label_encoders.items():\n",
        "        df[col] = encoder.transform(df[col])\n",
        "\n",
        "    return df\n",
        "\n",
        "#--------------------------------\n",
        "# decode initial data features\n",
        "#----------------------------------\n",
        "def decode_initial_data_features(df_p, LABEL_COL):\n",
        "\n",
        "    df = df_p.copy()\n",
        "    fe_processed_df, loaded_label_encoders, num_fe_scaler = load_objects_from_drive()\n",
        "    features_engineering_columns = fe_processed_df.columns.tolist()\n",
        "    input_feature_column = [col for col in features_engineering_columns if col != LABEL_COL]\n",
        "    features_engineering_columns.remove(LABEL_COL)\n",
        "\n",
        "    # Decode categorical features\n",
        "    for col, encoder in loaded_label_encoders.items():\n",
        "      if col in df.columns:  # Check if the column exists in the DataFrame\n",
        "        try:\n",
        "            df[col] = encoder.inverse_transform(df[col])\n",
        "        except ValueError as e:\n",
        "            print(f\"Error decoding column '{col}': {e}\")\n",
        "            # Handle the error appropriately (e.g., skip the column or fill with a default value)\n",
        "\n",
        "    # Inverse transform numerical features\n",
        "    if features_engineering_columns:  # check if the list is not empty\n",
        "        numerical_cols = [col for col in features_engineering_columns if col in df.columns]\n",
        "        if numerical_cols: # Check if the list of numerical cols is not empty\n",
        "            try:\n",
        "                df[numerical_cols] = num_fe_scaler.inverse_transform(df[numerical_cols])\n",
        "            except ValueError as e:\n",
        "                print(f\"Error decoding numerical features: {e}\")\n",
        "    print(f\"\\nloaded_label_encoders: {loaded_label_encoders}\")\n",
        "    print(f\"\\features_engineering_columns: {features_engineering_columns}\")\n",
        "    display(df)\n",
        "    return df\n",
        "#-------------------\n",
        "# --------------------------\n",
        "#   Prediction Function\n",
        "# --------------------------\n",
        "\n",
        "def model_2SM2UAF_predict_anomaly_features_inference(encoded_df,\n",
        "                                                     y_pred,\n",
        "                                                     y_test,\n",
        "                                                     LABEL_COL,\n",
        "                                                     threashhold_perc = 95):\n",
        "\n",
        "    #encoded_df = encode_simulated_real_time_data(df, LABEL_COL)\n",
        "    #features_engineering_columns = df.columns.tolist()\n",
        "    #features_engineering_columns.remove(LABEL_COL)\n",
        "\n",
        "    #df[features_engineering_columns] = num_fe_scaler.transform(df[features_engineering_columns])\n",
        "    #threshold = np.percentile(y_probas, threashhold_perc)\n",
        "    #encoded_df[\"Pred Threat\"] = y_pred\n",
        "    #--------------------------------\n",
        "    mse = np.mean(np.power(y_test - y_pred, 2))\n",
        "    threshold = np.percentile(mse, threashhold_perc)\n",
        "    encoded_df[\"anomaly_score\"] = mse\n",
        "    encoded_df[\"is_anomaly\"] = encoded_df[\"anomaly_score\"] > threshold\n",
        "    #-----------------------------\n",
        "    #encoded_df[\"Pred Threat Probability\"] = y_probas\n",
        "    #encoded_df[\"anomaly_score\"] = y_probas\n",
        "    #ncoded_df[\"is_anomaly\"] = encoded_df[\"anomaly_score\"] > threshold\n",
        "\n",
        "    #------------------------------\n",
        "    #y_preds = model.predict(df[input_feature_column])\n",
        "    #    df[\"Pred Threat\"] = y_preds\n",
        "    #    mse = np.mean(np.power(df[input_feature_column] - y_preds, 2), axis=1)\n",
        "    #    threshold = np.percentile(mse, 95)\n",
        "    #    df[\"anomaly_score\"] = mse\n",
        "    #-----------------------------\n",
        "\n",
        "    return encoded_df\n",
        "\n",
        "\n",
        "def predict_new_data(input_data, LABEL_COL, model_dir=MODELS_DIR):\n",
        "\n",
        "\n",
        "    \"\"\"Predicts on new data and evaluates if LABEL_COL exists.\"\"\"\n",
        "    log(\"Loading scaler and models for inference...\")\n",
        "    scaler, base_model, meta_model, unsupervised_models = load_scaler_and_models(model_dir)\n",
        "\n",
        "    #if isinstance(input_data, str):\n",
        "    #    df_raw = pd.read_csv(input_data)\n",
        "    #elif isinstance(input_data, pd.DataFrame):\n",
        "    #    df_raw = input_data.copy()\n",
        "    #else:\n",
        "    #    raise TypeError(\"input_data must be a filepath or a pandas DataFrame.\")\n",
        "\n",
        "    #encoded_df_raw = encode_simulated_real_time_data(df_raw, LABEL_COL)\n",
        "    #----------------------------------------------------\n",
        "    #features_engineering_columns = encoded_df_raw.columns.tolist()\n",
        "    #features_engineering_columns.remove(LABEL_COL)\n",
        "\n",
        "    if isinstance(input_data, str):\n",
        "        augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list = data_augmentation_pipeline(\n",
        "                                    file_path=input_data,\n",
        "                                    lead_save_true_false = False)\n",
        "        encoded_df_raw = augmented_df.copy()\n",
        "    else:\n",
        "        log(\"Initial data frame is empty...\")\n",
        "        raise TypeError(\"input_data must be a filepath \")\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------\n",
        "\n",
        "    y_test = encoded_df_raw[LABEL_COL] if LABEL_COL in encoded_df_raw.columns else None\n",
        "    X_new = load_treaned_features(scaler, encoded_df_raw)\n",
        "\n",
        "    log(\"Scaling input features...\")\n",
        "    X_scaled = scaler.transform(X_new)\n",
        "\n",
        "    log(\"Extracting anomaly features...\")\n",
        "    anomaly_features = extract_anomaly_features_inference(X_scaled, unsupervised_models)\n",
        "\n",
        "    X_ext = pd.concat(\n",
        "        [pd.DataFrame(X_scaled, columns=X_new.columns).reset_index(drop=True),\n",
        "         anomaly_features.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    base_proba = base_model.predict_proba(X_ext)\n",
        "    X_stack = np.hstack([X_ext.values, base_proba])\n",
        "    y_pred = meta_model.predict(X_stack)\n",
        "    y_proba = meta_model.predict_proba(X_stack)\n",
        "\n",
        "    log(\"Prediction complete.\")\n",
        "\n",
        "    if y_test is not None:\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        #cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        #print(f\"\\nAccuracy: {acc:.4f}\")\n",
        "        print(\"\\nClassification Report:\\n\", report)\n",
        "        #print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "        df_raw_anomaly_pred = model_2SM2UAF_predict_anomaly_features_inference(encoded_df_raw,\n",
        "                                                                               y_pred,\n",
        "                                                                               y_test,\n",
        "                                                                               LABEL_COL,\n",
        "                                                                               THREASHHOLD_PERC)\n",
        "\n",
        "        model_metrics_dic = print_model_performance_report(MODEL_NAME, y_test, y_pred)\n",
        "\n",
        "        visualizing_model_performance_pipeline(\n",
        "            data=df_raw_anomaly_pred,\n",
        "            x=\"Session Duration in Second\",\n",
        "            y=\"Data Transfer MB\",\n",
        "            anomaly_score=\"anomaly_score\",  # Use model_type to construct column name\n",
        "            is_anomaly=\"is_anomaly\",  # Use model_type to construct column name\n",
        "            title=\"Model Performance Visualization\\n\"\n",
        "            )\n",
        "\n",
        "    return y_pred, y_proba\n",
        "# This cell defines a function to format and display model inference output with dynamic insights.\n",
        "\n",
        "def display_model_inference_output(preds, probs, class_names):\n",
        "    \"\"\"\n",
        "    Formats and displays model inference output (predicted classes and probabilities)\n",
        "    with dynamic explanations and business insights.\n",
        "\n",
        "    Args:\n",
        "        preds (np.ndarray): Array of predicted class labels.\n",
        "        probs (np.ndarray): Array of prediction probabilities.\n",
        "        class_names (dict): Mapping from numerical class labels to names.\n",
        "    \"\"\"\n",
        "    # --- Explanation and Business Insight ---\n",
        "    print(\"--- Model Prediction Output Analysis ---\")\n",
        "    print(\"\\nBased on the model's inference results:\")\n",
        "\n",
        "    # Display the shape of the predicted classes array.\n",
        "    # This shows the total number of instances for which a class prediction was made.\n",
        "    num_instances = preds.shape[0]\n",
        "    print(f\"\\nShape of Predicted Classes: {preds.shape}\")\n",
        "    print(f\"Business Insight: The model processed and made predictions for a total of {num_instances} instances.\")\n",
        "\n",
        "    # Display the shape of the prediction probabilities array.\n",
        "    # This shows the total number of instances and the number of classes (columns) with probability scores.\n",
        "    num_classes = probs.shape[1]\n",
        "    print(f\"\\nShape of Prediction Probabilities: {probs.shape}\")\n",
        "    print(f\"Business Insight: For each instance, the model provided a probability score for each of the {num_classes} possible threat levels.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- First 10 Predictions and Probabilities ---\")\n",
        "\n",
        "    # Display the first 10 predicted class labels, including their names.\n",
        "    print(\"\\nFirst 10 Predicted Classes (Numerical and Name):\")\n",
        "    for i, pred in enumerate(preds[:10]):\n",
        "        print(f\"Instance {i+1}: {pred} ({class_names.get(pred, 'Unknown Class')})\")\n",
        "\n",
        "\n",
        "    # Display the first 10 rows of prediction probabilities, rounded for clarity.\n",
        "    # Each row shows the probability of the instance belonging to each of the 4 classes.\n",
        "    print(\"\\nFirst 10 Prediction Probabilities:\")\n",
        "    # Create a temporary DataFrame to display with column names\n",
        "    probs_df = pd.DataFrame(probs[:10], columns=[class_names.get(i, f'Class {i}') for i in range(probs.shape[1])])\n",
        "    display(np.round(probs_df, 4))\n",
        "    print(\"Business Insight: Examining the probabilities for individual instances shows the model's confidence in its predictions for specific events.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Prediction Probability Summary Statistics ---\")\n",
        "\n",
        "    # Display the average probability across all predictions and all classes.\n",
        "    avg_prob = np.mean(probs)\n",
        "    print(f\"\\nAverage Prediction Probability (across all classes and instances): {avg_prob:.4f}\")\n",
        "    insight_avg_prob = f\"An average probability around {1/num_classes:.2f} (for {num_classes} classes) might suggest a relatively balanced distribution of predictions or model uncertainty across classes. Further analysis of the probability distribution is recommended.\" if num_classes > 0 else \"Cannot calculate average probability with 0 classes.\"\n",
        "    print(f\"Business Insight: {insight_avg_prob}\")\n",
        "\n",
        "\n",
        "    # Display the maximum probability assigned to any class for any instance.\n",
        "    max_prob = np.max(probs)\n",
        "    print(f\"\\nMaximum Prediction Probability (assigned to any class for any instance): {max_prob:.4f}\")\n",
        "    insight_max_prob = \"A maximum probability of 1.0 indicates the model is highly confident in some of its individual predictions.\" if max_prob == 1.0 else \"The maximum probability is less than 1.0, suggesting the model has some level of uncertainty even in its most confident predictions.\"\n",
        "    print(f\"Business Insight: {insight_max_prob}\")\n",
        "\n",
        "\n",
        "    # Display the minimum probability assigned to any class for any instance.\n",
        "    min_prob = np.min(probs)\n",
        "    print(f\"\\nMinimum Prediction Probability (assigned to any class for any instance): {min_prob:.4f}\")\n",
        "    insight_min_prob = \"A minimum probability of 0.0 means the model is completely certain some instances do not belong to certain classes.\" if min_prob == 0.0 else \"The minimum probability is greater than 0.0, suggesting the model assigns some non-zero probability to all classes for all instances.\"\n",
        "    print(f\"Business Insight: {insight_min_prob}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Overall Business Insight from Prediction Output ---\")\n",
        "    print(\"\"\"\n",
        "This output provides a snapshot of the model's inference phase.\n",
        "- The **shapes** confirm the total number of instances processed and the number of classes evaluated.\n",
        "- The **predicted classes** indicate the primary threat level identified for each instance, enabling prioritized operational responses.\n",
        "- The **prediction probabilities** offer a measure of the model's confidence. While high confidence in individual cases is good, the overall average probability suggests further investigation into the probability distribution and model uncertainty is valuable.\n",
        "- To gain more specific business insights, analyze the distribution of predicted threat levels across all instances and investigate instances with lower confidence scores.\n",
        "\"\"\")\n",
        "\n",
        "# --------------------------\n",
        "#   Main Execution\n",
        "# --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    preds, probs = predict_new_data(SIMULATED_REAL_TIME_DATA_FILE, LABEL_COL)\n",
        "\n",
        "    # Log shapes\n",
        "    #print(f\"\\nPredicted classes shape: {preds.shape}\")\n",
        "    #print(f\"Prediction probabilities shape: {probs.shape}\")\n",
        "\n",
        "    # Show first few predictions\n",
        "    #print(\"\\nPredicted classes (first 10):\", preds[:10])\n",
        "    #print(\"Prediction probabilities (first 10):\\n\", np.round(probs[:10], 4))\n",
        "\n",
        "    # Aggregate probability stats\n",
        "    #print(f\"\\nAverage probability: {np.mean(probs):.4f}\")\n",
        "    #print(f\"Max probability: {np.max(probs):.4f}\")\n",
        "    #print(f\"Min probability: {np.min(probs):.4f}\")\n",
        "\n",
        "    class_names = {\n",
        "        0: 'Low',\n",
        "        1: 'Medium',\n",
        "        2: 'High',\n",
        "        3: 'Critical'\n",
        "    }\n",
        "    display_model_inference_output(preds, probs, class_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}