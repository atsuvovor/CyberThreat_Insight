# -*- coding: utf-8 -*-
"""f_engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EmPDiRcSWlaSt3njzf4W1AREklpZJ1Da
"""

#CyberThreat-Insight
#Anomalous Behavior Detection in Cybersecurity Analytics using Generative AI
#---------------------------Feature Engineering---------------------------------

#Toronto, Septeber 08 2025
#Autor : Atsu Vovor

#Master of Management in Artificial Intelligence
#Consultant Data Analytics Specialist | Machine Learning |
#Data science | Quantitative Analysis |French & English Bilingual


#======================
# Importante Library
#======================
from graphviz import Digraph
from IPython.display import Image
from google.colab import drive
drive.mount('/content/drive')
import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
import shap
import tensorflow as tf
from tensorflow.keras import layers
from imblearn.over_sampling import SMOTE
from tqdm import tqdm
from IPython.display import display
from matplotlib.colors import Normalize
from matplotlib import cm
from sklearn.manifold import TSNE
import umap
#=================================
# Feature Engineering Flowshart
#=================================

def generate_flowchart():
    # Create a directed graph
    dot = Digraph(comment='Cyber Threat Insight - Feature Engineering Workflow', format='png')
    dot = Digraph("Cyber Threat Insight - Feature Engineering Workflow", format="png")


    # Feature Engineering Phases
    dot.node('Start', 'Start')
    dot.node('DataInj', 'Data Injection\n(Cholesky-Based Perturbation)', shape='box', style='filled', fillcolor='lightblue')
    dot.node('Scaling', 'Feature Normalization & Scaling\n(Min-Max, Z-score)', shape='box', style='filled', fillcolor='lightgray')
    dot.node('CorrHeat', 'Correlation Heatmap Analysis\n(Pearson/Spearman)', shape='box', style='filled', fillcolor='orange')
    dot.node('FeatImp', 'Feature Importance\n(Random Forest)', shape='box', style='filled', fillcolor='gold')
    dot.node('SHAP', 'Model Explainability\n(SHAP Values)', shape='box', style='filled', fillcolor='lightgreen')
    dot.node('PCA', 'PCA & Variance Explained\n(Scree Plot)', shape='box', style='filled', fillcolor='plum')
    dot.node('Augment', 'Data Augmentation\n(SMOTE, GAN)', shape='box', style='filled', fillcolor='lightpink')
    dot.node('End', 'Feature Set Ready for Modeling', shape='ellipse', style='filled', fillcolor='lightyellow')

    #Arrows to show workflow
    dot.edge('Start', 'DataInj')
    dot.edge('DataInj', 'Scaling')
    dot.edge('Scaling', 'CorrHeat')
    dot.edge('CorrHeat', 'FeatImp')
    dot.edge('FeatImp', 'SHAP')
    dot.edge('SHAP', 'PCA')
    dot.edge('PCA', 'Augment')
    dot.edge('Augment', 'End')

    features_engineering_flowchart = dot.render("features_engineering_flowchart", format="png", cleanup=False)
    display(Image(filename="features_engineering_flowchart.png"))
    print("Flowchart generated successfully!")


#------------------------------------------------------------------------------------------------------------
# ==================================
# Cyber Attack Simulation Pipeline
# ==================================

def get_column_dic():
    columns_dic = {
        "numerical_columns": [
            "Timestamps", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "explanatory_data_analysis_columns": [
            "Date Reported", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "user_activity_features": [
            "Risk Level", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "initial_dates_columns": ["Date Reported", "Date Resolved", "Timestamps"],
        "categorical_columns": [
            "Issue ID", "Issue Key", "Issue Name", "Category", "Severity", "Status", "Reporters",
            "Assignees", "Risk Level", "Department Affected", "Remediation Steps", "KPI/KRI",
            "User ID", "Activity Type", "User Location", "IP Location", "Threat Level",
            "Defense Action", "Color"
        ],
        "features_engineering_columns": [
            "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score", "Threat Level"
        ],
        "numerical_behavioral_features": [
            "Login Attempts", "Data Transfer MB", "CPU Usage %", "Memory Usage MB",
            "Session Duration in Second", "Num Files Accessed", "Threat Score"
        ]
    }
    return columns_dic

# ------------------------------Save df_fe, label_encoders anf numerical columns scaler to to your Google Drive------------------------
def save_objects_to_drive(df_fe,
                          cat_cols_label_encoders,
                          num_fe_scaler,
                          filepath_df="/content/drive/My Drive/Cybersecurity Data/df_fe.pkl",
                          filepath_cat_cols_label_encoders="/content/drive/My Drive/Model deployment/cat_cols_label_encoders.pkl",
                          filepath_num_fe_scaler="/content/drive/My Drive/Model deployment/ num_fe_scaler.pkl"):
    try:
        # Ensure the directory exists for df_fe
        df_directory = os.path.dirname(filepath_df)
        if not os.path.exists(df_directory):
            os.makedirs(df_directory)
            print(f"Created directory: {df_directory}")

        # Ensure the directory exists for label_encoders and scaler
        model_directory = os.path.dirname(filepath_cat_cols_label_encoders)
        if not os.path.exists(model_directory):
            os.makedirs(model_directory)
            print(f"Created directory: {model_directory}")


        with open(filepath_df, 'wb') as f:
            pickle.dump(df_fe, f)
        print(f"DataFrame saved successfully to: {filepath_df}")

        with open(filepath_cat_cols_label_encoders, 'wb') as f:
            pickle.dump(cat_cols_label_encoders, f)
        print(f"Label encoders saved successfully to: {filepath_cat_cols_label_encoders}")

        with open(filepath_num_fe_scaler, 'wb') as f:
            pickle.dump(num_fe_scaler, f)
        print(f"Label encoders saved successfully to: {filepath_num_fe_scaler}")

    except Exception as e:
        print(f"An error occurred while saving: {e}")


# ----------------Load df_fe and label_encoders from your Google Drive----------------------------------
def load_objects_from_drive(filepath_df="/content/drive/My Drive/Cybersecurity Data/df_fe.pkl",
                            filepath_cat_cols_label_encoders="/content/drive/My Drive/Model deployment/cat_cols_label_encoders.pkl",
                            filepath_num_fe_scaler="/content/drive/My Drive/Model deployment/ num_fe_scaler.pkl"):
    try:
        with open(filepath_df, 'rb') as f:
            df_fe = pickle.load(f)
        print(f"DataFrame loaded successfully from: {filepath_df}")

        with open(filepath_cat_cols_label_encoders, 'rb') as f:
            cat_cols_label_encoders = pickle.load(f)
        print(f"Label encoders loaded successfully from: {filepath_cat_cols_label_encoders}")

        with open(filepath_num_fe_scaler, 'rb') as f:
            num_fe_scaler = pickle.load(f)
        print(f"Label encoders loaded successfully from: {filepath_num_fe_scaler}")

        return df_fe, cat_cols_label_encoders, num_fe_scaler

    except Exception as e:
        print(f"An error occurred while loading: {e}")
        return None, None, None # Return None for the third value as well


#-------------Generate Synthetic Anomalies Using Cholesky-Based Perturbation-------------------

def get_files_path(
        normal_operations_file_path = "/content/drive/My Drive/Cybersecurity Data/normal_and_anomalous_cybersecurity_dataset_for_google_drive_kb.csv",
        combined_normal_and_anomaly_file_path = "/content/combined_normal_and_anomaly_output_file_for_google_drive_kb.csv"):

    return {
        "normal_operations_file_path": normal_operations_file_path,
        "combined_normal_and_anomaly_file_path":  combined_normal_and_anomaly_file_path
    }

def load_Synthetic_dataset(filepath):
    return pd.read_csv(filepath)

def scale_data(df, features):
    scaler = StandardScaler()
    features_to_scale = [f for f in features if f != 'Timestamps']
    scaled = scaler.fit_transform(df[features_to_scale].dropna())
    return scaled, scaler

def cholesky_decomposition(scaled_data):
    cov_matrix = np.cov(scaled_data, rowvar=False)
    L = np.linalg.cholesky(cov_matrix)
    return L

def generate_cholesky_anomalies(real_data, L, num_samples=1000):
    np.random.seed(42)
    normal_samples = np.random.randn(num_samples, real_data.shape[1])
    synthetic_anomalies = normal_samples @ L.T
    return synthetic_anomalies

def inverse_transform(synthetic_data, scaler):
    return scaler.inverse_transform(synthetic_data)

def create_anomaly_df(original_data, synthetic_original, features):
    df_synthetic = pd.DataFrame(synthetic_original, columns=features)

    # Create full column DataFrame for synthetic data with same structure as original
    df_synthetic_full = pd.DataFrame(columns=original_data.columns)

    # Fill known numerical features
    for col in features:
        df_synthetic_full[col] = df_synthetic[col]

    # Fill in the rest of the columns using random sampling or generation
    for col in original_data.columns:
        if col not in features:
            if original_data[col].dtype == 'object':
                df_synthetic_full[col] = np.random.choice(original_data[col].dropna().unique(), size=len(df_synthetic_full))
            elif np.issubdtype(original_data[col].dtype, np.datetime64):
                # If timestamps exist, shift a base date with random offsets
                base = pd.to_datetime("2024-01-01")
                df_synthetic_full[col] = base + pd.to_timedelta(np.random.randint(0, 90, size=len(df_synthetic_full)), unit='D')
            else:
                df_synthetic_full[col] = np.random.choice(original_data[col].dropna(), size=len(df_synthetic_full))

    df_synthetic_full["Source"] = "Synthetic"

    df_real = original_data.copy()
    df_real["Source"] = "Real"

    df_combined = pd.concat([df_real, df_synthetic_full], ignore_index=True)
    return df_combined

def save_dataset(df, path):
    df.to_csv(path, index=False)
    print(f"Saved combined dataset with synthetic anomalies to: {path}")

def data_injection_cholesky_based_perturbation(file_paths = "", save_data_true_false = True):

    print("Anomaly Injection – Cholesky-Based Perturbation...")
    if save_data_true_false == True:
        file_paths = get_files_path()
        df_real = load_Synthetic_dataset(file_paths["normal_operations_file_path"])
    else:
        df_real = load_Synthetic_dataset(file_paths)

    # Call get_column_dic here to access numerical_columns
    columns_dic = get_column_dic()
    numerical_columns = columns_dic["numerical_columns"]

    numerical_columns_for_scaling = [col for col in numerical_columns if col != "Timestamps"]

    scaled_data, scaler = scale_data(df_real, numerical_columns_for_scaling)

    L = cholesky_decomposition(scaled_data)
    synthetic_scaled = generate_cholesky_anomalies(df_real[numerical_columns_for_scaling], L, num_samples=100)
    synthetic_original = inverse_transform(synthetic_scaled, scaler)

    normal_and_combined_cholesky_based_perturbation_df = create_anomaly_df(df_real, synthetic_original, numerical_columns_for_scaling)

    if save_data_true_false == True:
        save_dataset(normal_and_combined_cholesky_based_perturbation_df, file_paths["combined_normal_and_anomaly_file_path"])

    return normal_and_combined_cholesky_based_perturbation_df


# -------------------------------Normalize numerical feature--------------------------------------
def normalize_numerical_features(df, p_numerical_columns):
    # return normalized_df, scaler
    scaler = MinMaxScaler()
    df[p_numerical_columns] = scaler.fit_transform(df[p_numerical_columns])
    return df, scaler # Return the DataFrame and scaler

def encode_dates(df, date_columns):
    """
    Extracts date components from specified columns in a DataFrame.

    Parameters:
      df (DataFrame): The DataFrame containing date columns.
      date_columns (list): List of date columns to extract components from.

    Returns:
      DataFrame: DataFrame with additional date component columns.
    """
    processed_df = df.copy()

    for date_col in date_columns:
        # Convert the column to datetime if it's not already
        processed_df[date_col] = pd.to_datetime(processed_df[date_col], errors='coerce')

        # Check if the column is a datetime column before applying .dt accessor
        if pd.api.types.is_datetime64_any_dtype(processed_df[date_col]):
            processed_df[f"year_{date_col}"] = processed_df[date_col].dt.year
            processed_df[f"month_{date_col}"] = processed_df[date_col].dt.month
            processed_df[f"day_{date_col}"] = processed_df[date_col].dt.day
            processed_df[f"day_of_week_{date_col}"] = processed_df[date_col].dt.dayofweek  # Monday=0, Sunday=6
            processed_df[f"day_of_year_{date_col}"] = processed_df[date_col].dt.dayofyear
        else:
            print(f"Warning: Column '{date_col}' is not a datetime column and will be skipped.")

    # Example of converting timestamps to seconds (if a timestamp column exists)
    if "Timestamps" in date_columns:
        processed_df["timestamp_seconds"] = processed_df["Timestamps"].astype(int) / 10**9

    return processed_df.drop(columns=date_columns)

def encode_categorical_columns(df, categorical_columns):
    """
    Applies label encoding to specified categorical columns in a DataFrame.

    Parameters:
      df (DataFrame): The DataFrame containing categorical columns.
      categorical_columns (list): List of columns to apply label encoding to.

    Returns:
      DataFrame, dict: DataFrame with encoded categorical columns and a dictionary of label encoders.
    """
    processed_df = df.copy()
    label_encoders = {}


    for column in categorical_columns:
        le = LabelEncoder()
        processed_df[column] = le.fit_transform(processed_df[column].astype(str))
        label_encoders[column] = le

    return processed_df, label_encoders

def decode_categorical_columns( df_to_decode, label_encoders):
    """
    Decodes label-encoded categorical columns in a DataFrame.

    Parameters:
      df_to_decode (DataFrame): The DataFrame containing label-encoded categorical columns.
      label_encoders (dict): Dictionary of LabelEncoders used for encoding, with column names as keys.
    """

    # initialize decoded data frame(decoded_df)
    decoded_df = [[]]
    processed_df_to_decode = df_to_decode.copy()

    for column, le in label_encoders.items():
        if column in processed_df_to_decode.columns:
            decoded_df[column] = le.inverse_transform(processed_df_to_decode[column])


def preprocess_dataframe(df, numerical_columns, date_columns, categorical_columns):
    """
    Main function to preprocess a DataFrame by encoding dates and categorical columns.

    Parameters:
      df (DataFrame): Original DataFrame to be copied and processed.
      numerical_columns (list): List of numerical columns (currently unused in this function).
      date_columns (list): List of date columns to extract components from.
      categorical_columns (list): List of categorical columns to encode.

    Returns:
      DataFrame, dict: Processed DataFrame and dictionary of label encoders.
    """

    #Normalize numerical feature
    #processed_df = normalize_numerical_features(df, numerical_columns)
    df, normalize_numerical_features_scaler = normalize_numerical_features(df, [i for i in numerical_columns if i not in ['Timestamps']])

    # Apply date encoding using the df
    processed_df = encode_dates(df, date_columns)  # Use the output of normalize_numerical_features

    # Apply categorical encoding using the processed_df, but exclude date_columns
    processed_df, categorical_columns_label_encoders = encode_categorical_columns(processed_df, [col for col in categorical_columns if col not in date_columns])

    return processed_df, categorical_columns_label_encoders, normalize_numerical_features_scaler # Return processed_df instead of df
#------------------------------------------------------------------------------------------------

# 1. Correlation Heatmap
def plot_correlation_heatmap(ax, df, method='pearson'):
    numeric_df = df.select_dtypes(include=[np.number])
    corr = numeric_df.corr(method=method)
    sns.heatmap(corr, cmap='coolwarm', annot=False, fmt='.2f', square=True, ax=ax)
    ax.set_title(f'{method.capitalize()} Correlation Heatmap')

# 2. Feature Importance
def plot_feature_importance(ax, X, y, top_n=None):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    importances = rf.feature_importances_

    if top_n is None or top_n > len(importances):
        top_n = len(importances)

    indices = np.argsort(importances)[-top_n:]
    ax.barh(range(top_n), importances[indices], align='center')
    ax.set_yticks(range(top_n))
    ax.set_yticklabels([X.columns[i] for i in indices])
    ax.set_xlabel("Feature Importance")
    ax.set_title("Top Random Forest Feature Importances")

    return rf

# 3. SHAP Summary Plot (Standalone, not in subplot)
# Function to set font properties for plot axes
def set_font_properties(ax, x_fontsize=8, y_fontsize=8,
                        labelcolor='black', mean_shap_fontsize=8,
                        font_name = 'sans-serif'):
    """
    Set the font properties for axes ticks.

    Args:
    - ax: The axes object for the plot
    - x_fontsize: Font size for x-axis labels
    - y_fontsize: Font size for y-axis labels
    - labelcolor: Color for the labels (default is 'blue')
    """
    ax.tick_params(axis='x', labelsize=x_fontsize, labelcolor=labelcolor)
    ax.tick_params(axis='y', labelsize=y_fontsize, labelcolor=labelcolor)

    # Adjust the font for the x-axis labels
    for label in ax.get_xticklabels():
        label.set_fontsize(x_fontsize)  # Set font size
        label.set_fontname(font_name)  # Default sans-serif font
        label.set_color(labelcolor)  # Set label color


    # Adjust the font for the y-axis labels
    for label in ax.get_yticklabels():

        label.set_fontsize(y_fontsize)  # Set font size
        label.set_fontname(font_name)  # Default sans-serif font
        label.set_color(labelcolor)  # Set label color

    # Adjust mean(|SHAP value|) font size (located in the text below the plot)
    for text in ax.texts:
        if 'mean(|SHAP value|)' in text.get_text():
            text.set_fontsize(mean_shap_fontsize)  # Reduce the font size for the mean(|SHAP value|) text
            text.set_fontname(font_name)  # Default sans-serif font
            text.set_color(labelcolor)  # Set label color



# Function to update plot title font
def update_title(title, fontsize=8, family='sans-serif', fontweight='normal'):
    """
    Update the title of the plot with custom font properties.

    Args:
    - title: Title of the plot
    - fontsize: Font size for the title
    - family: Font family for the title
    - fontweight: Font weight for the title
    """
    plt.title(title, fontsize=fontsize, family=family, fontweight=fontweight)

def smaller_shap_summary_plot(shap_values, X, y,
                              plot_type="bar",
                              plot_size=(30, 15),
                              title="SHAP Summary Plot",
                              class_names=None, xlabel_fontsize=8):
    """
    Generates a smaller SHAP summary plot.

    Args:
        shap_values: SHAP values (output from SHAP model explainer)
        X: The feature matrix(Sample data used for generating the SHAP plot)
        plot_type: The type of plot ("dot", "bar", etc.).
        plot_size: A tuple (width, height) specifying the plot's size in inches.
        title: Custom title for the plot (default is 'SHAP Summary Plot')
        class_names: List of class names for the legend.
        xlabel_fontsize: Font size for the x-axis label.
    """

    # If class_names are not provided, generate default ones
    if class_names is None:
        labels = sorted(list(set(y) | set(y)))
        level_mapping = {0: "Low", 1: "Medium", 2: "High", 3: "Critical"}
        class_names = [level_mapping.get(label) for label in labels]

    # Adjust the figure size for the SHAP summary plot
    plt.figure(figsize=(plot_size[0]/10, plot_size[1]/10))

    shap.summary_plot(shap_values, X, plot_type=plot_type, class_names=class_names, show=False) #prevent auto showing the plot, so we can modify it.
    #shap.summary_plot(shap_values, X, plot_type=plot_type, feature_names=list(X), class_names=class_names, show=False)
    plt.tight_layout() #reduce white space around plot.

    # Access the current axes (for summary plot)
    ax = plt.gca()

    # Change font properties for feature names and axis labels
    set_font_properties(ax, mean_shap_fontsize=8) # Set the font size for the mean(|SHAP value|) text

    # Update the title with custom font
    update_title(title)

    # Adjust legend font size
    ax.legend(fontsize=8)

    # Adjust x-axis label font size
    ax.set_xlabel(ax.get_xlabel(), fontsize=xlabel_fontsize)


    plt.show() #manually show it.


def plot_shap_summary(model, X_sample, y):
    target_column="Threat Level"
    level_mapping = {"Low": 0, "Medium": 1, "High": 2, "Critical": 3}
    class_names = list(level_mapping.keys())

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    if isinstance(shap_values, list) and len(shap_values) > 1:
        #shap.summary_plot(shap_values[1], X_sample, plot_size=(2, 2))  # Binary case
        smaller_shap_summary_plot(shap_values[1], X_sample, y, class_names=class_names)
    else:
        #shap.summary_plot(shap_values, X_sample, plot_size=(2, 2))
        # Generate summary plot with custom class names in the legend
        smaller_shap_summary_plot(shap_values, X_sample, y, class_names=class_names)

# 4. PCA Scree Plot
def plot_pca_variance(ax, X, threshold=0.95):
    pca = PCA().fit(X)
    cum_var = np.cumsum(pca.explained_variance_ratio_)
    ax.plot(cum_var, marker='o', linestyle='--', color='b')
    ax.axhline(y=threshold, color='r', linestyle='-')
    ax.set_title("PCA Scree Plot")
    ax.set_xlabel("Num Components")
    ax.set_ylabel("Cumulative Explained Variance")
    ax.grid(True)

# 5. Main Driver Function
def run_feature_analysis(df_fe, target_column="Threat Level", corr_method="pearson"):
    print("Running Feature Analysis Pipeline...")

    df_local = df_fe.copy()

    # Encode target if needed
    if df_local[target_column].dtype == 'object':
        le = LabelEncoder()
        df_local[target_column] = le.fit_transform(df_local[target_column])

    X = df_local.select_dtypes(include=[np.number]).drop(columns=[target_column], errors='ignore')
    y = df_local[target_column]

    # Create subplots (3 panels: correlation, importance, PCA)
    fig, axes = plt.subplots(1, 3, figsize=(24, 6))

    # Plot 1: Correlation Heatmap
    plot_correlation_heatmap(axes[0], df_local, method=corr_method)

    # Plot 2: Feature Importance
    model = plot_feature_importance(axes[1], X, y, top_n=15)

    # Plot 3: PCA Scree
    plot_pca_variance(axes[2], X)

    plt.tight_layout()
    plt.show()

    # Plot 4: SHAP Summary (standalone)
    print("\nSHAP Summary Plot:")
    X_sample = shap.utils.sample(X, 200, random_state=42) if len(X) > 200 else X
    plot_shap_summary(model, X_sample, y)

    print("Feature analysis complete.")

#-------------------------features_engineering_pipeline ---------------------------
def features_engineering_pipeline(file_path = None , analysis_true_false = True):

    print("Feature engineering pipeline started.")
    #get features dic
    columns_dic = get_column_dic()
    numerical_columns = columns_dic["numerical_columns"]
    features_engineering_columns = columns_dic["features_engineering_columns"]
    initial_dates_columns = columns_dic["initial_dates_columns"]
    categorical_columns = columns_dic["categorical_columns"]

    #data injection: Anomaly Injection – Cholesky-Based Perturbation
    if analysis_true_false == True:
        naccbp_df = data_injection_cholesky_based_perturbation()
    else:
        naccbp_df = data_injection_cholesky_based_perturbation(file_path, save_data_true_false = False)

    #data collectionn, Generation and Preprocessing
    df = naccbp_df.copy()

    # Convert date columns to datetime objects
    for col in initial_dates_columns:
        df[col] = pd.to_datetime(df[col])  # Convert to datetime

    # We filter the Timestamps from the columns to apply the MinMaxScaler
    df, cat_cols_label_encoders, num_fe_scaler = preprocess_dataframe(df, numerical_columns, initial_dates_columns, categorical_columns)

    #feature analysis
    df_fe = df[features_engineering_columns].copy()
    #display(df_fe.head())

    if analysis_true_false:
        # Run feature analysis
        run_feature_analysis(df_fe, target_column="Threat Level", corr_method="pearson")

        # deploy fe_processd_df and label_encoder to google drive
        save_objects_to_drive(df_fe, cat_cols_label_encoders, num_fe_scaler)

    print("Feature engineering pipeline completed.")

    return df_fe, cat_cols_label_encoders, num_fe_scaler

#-----------------------------
# SMOT & GAN
#-----------------------------
# ------------------------- SMOTE: Handle class imbalance -------------------------
def balance_data_with_smote(df, target_column="Threat Level"):
    """
    Apply SMOTE to balance minority classes in the dataset.
    Returns resampled feature set and target labels.
    """
    print("Balancing data with SMOTE...")
    X = df.drop(columns=[target_column])
    y = df[target_column]
    smote = SMOTE(sampling_strategy='minority', random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    return X_resampled, y_resampled

# ------------------- Build Generator and Discriminator for GAN -------------------
def build_gan(latent_dim, n_outputs):
    """
    Build and compile a basic GAN architecture with:
    - A generator that outputs synthetic samples
    - A discriminator that classifies real vs synthetic samples
    Returns both models.
    """
    def build_generator():
        model = tf.keras.Sequential([
            layers.Dense(128, activation="relu", input_dim=latent_dim),
            layers.Dense(256, activation="relu"),
            layers.Dense(n_outputs, activation="tanh")
        ])
        return model

    def build_discriminator():
        model = tf.keras.Sequential([
            layers.Dense(256, activation="relu", input_shape=(n_outputs,)),
            layers.Dense(128, activation="relu"),
            layers.Dense(1, activation="sigmoid")
        ])
        return model

    generator = build_generator()
    discriminator = build_discriminator()
    discriminator.compile(optimizer='adam', loss='binary_crossentropy')
    return generator, discriminator

# -------------------------- Train GAN with Logging --------------------------
def train_gan(generator, discriminator, X_real, latent_dim, epochs=1000, batch_size=64,
              plot_loss=False, early_stop_patience=50, output_dir="/content/drive/My Drive/Cybersecurity Data/"):
    """
    Train GAN using real synthetic data with optional logging, early stopping, and visualization.
    Tracks generator and discriminator losses and saves logs and plots to output_dir.
    """
    os.makedirs(output_dir, exist_ok=True)

    d_loss_real_list = []
    d_loss_fake_list = []
    g_loss_list = []

    best_g_loss = np.inf
    patience_counter = 0

    for epoch in tqdm(range(epochs), desc="Training GAN"):
        # Generate fake samples
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        gen_data = generator.predict(noise, verbose=0)

        # Sample real data
        idx = np.random.randint(0, X_real.shape[0], batch_size)
        real_data = X_real.iloc[idx].values

        # Labels for real and fake data
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # Train discriminator on real and fake data
        d_loss_real = discriminator.train_on_batch(real_data, real_labels)
        d_loss_fake = discriminator.train_on_batch(gen_data, fake_labels)

        # Train generator to fool the discriminator
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        g_loss = discriminator.train_on_batch(generator.predict(noise, verbose=0), real_labels)

        # Log losses
        d_loss_real_list.append(d_loss_real)
        d_loss_fake_list.append(d_loss_fake)
        g_loss_list.append(g_loss)

        # Early stopping logic for generator loss
        if g_loss < best_g_loss:
            best_g_loss = g_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print(f"\nEarly stopping at epoch {epoch} - No improvement in G loss for {early_stop_patience} epochs.")
                break

    # Save loss plot and CSV log
    plt.savefig(os.path.join(output_dir, "gan_loss_plot.png"))
    plt.close()

    loss_df = pd.DataFrame({
        "D_Loss_Real": d_loss_real_list,
        "D_Loss_Fake": d_loss_fake_list,
        "G_Loss": g_loss_list
    })
    loss_df.to_csv(os.path.join(output_dir, "gan_loss_log.csv"), index=False)

    return generator, d_loss_real_list, d_loss_fake_list, g_loss_list

# -------------------------- Generate synthetic samples --------------------------
def generate_synthetic_data(generator, n_samples, latent_dim, columns):
    """
    Generate synthetic samples using a trained GAN generator.
    Returns a DataFrame with the same feature columns.
    """
    noise = np.random.normal(0, 1, (n_samples, latent_dim))
    synthetic_data = generator.predict(noise, verbose=0)
    return pd.DataFrame(synthetic_data, columns=columns)

# -------------------------- Combine real + synthetic --------------------------
def augment_data(X_resampled, y_resampled, synthetic_data):
    """
    Combine real (SMOTE) and synthetic (GAN) data.
    Returns the concatenated feature set and target labels.
    """
    X_augmented = pd.concat([X_resampled, synthetic_data], axis=0)
    y_augmented = pd.concat([y_resampled, pd.Series(np.repeat(y_resampled.mode()[0], synthetic_data.shape[0]))])
    return X_augmented, y_augmented

# -------------------------- Concatenate into a final dataframe --------------------------
def concatenate_data_along_columns(X_augmented, y_augmented):
    """
    Merge features and labels into a single DataFrame.
    Returns the augmented DataFrame with a labeled target column.
    """
    augmented_df = pd.concat([X_augmented.copy(), y_augmented.copy()], axis=1)
    return augmented_df.rename(columns={0: "Threat Level"})

# -------------------------- Load/save utilities (assumed implemented) --------------------------
def save_dataframe_to_google_drive(df, path):
    """
    Utility function to save DataFrame to Google Drive path as CSV.
    """
    df.to_csv(path, index=False)

# -------------------------- Main pipeline function --------------------------
def data_augmentation_pipeline(file_path="", lead_save_true_false = True):
    """
    Main function that executes the entire data augmentation pipeline:
    1. Load data
    2. Apply SMOTE
    3. Build and train GAN
    4. Generate synthetic samples
    5. Combine with real samples
    6. Save final augmented dataset and loss logs
    """
    x_y_augmented_data_google_drive = "/content/drive/My Drive/Cybersecurity Data/x_y_augmented_data_google_drive.csv"
    loss_data_google_drive = "/content/drive/My Drive/Cybersecurity Data/loss_data_google_drive.csv"

    # Load preprocessed data from Google Drive
    if lead_save_true_false:
        print("Loading objects from Google Drive...")
        fe_processed_df, cat_cols_label_encoders, num_fe_scaler = load_objects_from_drive()
    else:
        fe_processed_df, cat_cols_label_encoders, num_fe_scaler = features_engineering_pipeline(file_path, analysis_true_false = False)

    if fe_processed_df is not None and cat_cols_label_encoders is not None:
        print("Data loaded from Google Drive.")
        processed_num_df = fe_processed_df.copy()
    else:
        print("Failed to load objects from Google Drive.")
        return None, None

    # Step 1: Balance data using SMOTE
    X_resampled, y_resampled = balance_data_with_smote(processed_num_df)

    # Step 2: Build GAN architecture
    latent_dim = 100
    n_outputs = X_resampled.shape[1]
    generator, discriminator = build_gan(latent_dim, n_outputs)

    # Step 3: Train GAN with logging and early stopping
    generator, d_loss_real_list, d_loss_fake_list, g_loss_list = train_gan(
        generator, discriminator, X_resampled, latent_dim, epochs=1000, batch_size=64
    )

    # Step 4: Generate synthetic data samples
    synthetic_data = generate_synthetic_data(generator, n_samples=1000, latent_dim=latent_dim, columns=X_resampled.columns)

    # Step 5: Combine real and synthetic data
    X_augmented, y_augmented = augment_data(X_resampled, y_resampled, synthetic_data)

    # Step 6: Concatenate into a single DataFrame
    augmented_df = concatenate_data_along_columns(X_augmented, y_augmented)

    # Step 7: Save the final augmented dataset to Google Drive
    if lead_save_true_false:
        print("Saving data to Google Drive...")
        save_dataframe_to_google_drive(augmented_df, x_y_augmented_data_google_drive)

    print("Data augmentation process complete.")

    return augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list

# -------------------------- Run the pipeline --------------------------
#if __name__ == "__main__":
    # Execute the full augmentation pipeline if the script is run directly
    #augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list = data_augmentation_pipeline()

#-------------------------------------------------------
#SMOTE and GAN augmentation models performance Analysis
#-------------------------------------------------------


# ---------------------------- #
# Apply Custom Matplotlib Style
# ---------------------------- #
def apply_custom_matplotlib_style(font_family='serif', font_size=11):
    plt.rcParams.update({
        'font.family': font_family,
        'font.size': font_size,
        'axes.titlesize': font_size + 1,
        'axes.labelsize': font_size,
        'legend.fontsize': font_size - 1,
        'xtick.labelsize': font_size - 1,
        'ytick.labelsize': font_size - 1
    })

# ---------------------------- #
# Loaders (Stub for Integration)
# ---------------------------- #
def load_dataset(filepath):
    return pd.read_csv(filepath)

# ---------------------------- #
#       Plot GAN Loss
# ---------------------------- #
def plot_loss_history(p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list):
    plt.figure(figsize=(5, 3))
    plt.plot(p_d_loss_real_list, label='D Loss Real')
    plt.plot(p_d_loss_fake_list, label='D Loss Fake')
    plt.plot(p_g_loss_list, label='G Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('GAN Training Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ----------------------------------- #
# Plot Training vs Validation Metrics
# ---------------------------- #
def plot_train_val_comparison(train_scores, val_scores, metric_name='Accuracy', title_prefix='Model Performance'):
    plt.figure(figsize=(5, 3))
    plt.plot(train_scores, label='Train')
    plt.plot(val_scores, label='Validation')
    plt.xlabel('Epoch')
    plt.ylabel(metric_name)
    plt.title(f'{title_prefix}: Train vs Validation {metric_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def plot_gan_training_metrics(p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list,
                               train_scores, val_scores, metric_name='Accuracy',
                               title_prefix='Model Performance'):
    """
    Plot GAN loss history and training vs validation metrics in a 1-row 2-column subplot.

    Parameters
    ----------
    p_d_loss_real_list : list
        Discriminator loss on real samples per epoch.
    p_d_loss_fake_list : list
        Discriminator loss on fake samples per epoch.
    p_g_loss_list : list
        Generator loss per epoch.
    train_scores : list
        Training metric values.
    val_scores : list
        Validation metric values.
    metric_name : str, optional
        Name of the evaluation metric (default is 'Accuracy').
    title_prefix : str, optional
        Prefix for the second subplot title.
    """
    fig, axes = plt.subplots(1, 2, figsize=(10, 3))

    # Plot 1: GAN Loss History
    axes[0].plot(p_d_loss_real_list, label='D Loss Real')
    axes[0].plot(p_d_loss_fake_list, label='D Loss Fake')
    axes[0].plot(p_g_loss_list, label='G Loss')
    axes[0].set_title('GAN Training Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)

    # Plot 2: Train vs Validation Metric
    axes[1].plot(train_scores, label='Train')
    axes[1].plot(val_scores, label='Validation')
    axes[1].set_title(f'{title_prefix}: Train vs Validation {metric_name}')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel(metric_name)
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()

#import matplotlib.pyplot as plt

def plot_gan_loss_and_model_performance(
    p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list,
    train_scores, val_scores,
    metric_name='Accuracy', title_prefix='Model Performance'
):
    """
    Plot GAN loss and model performance in subplots.

    Parameters
    ----------
    p_d_loss_real_list : list
    p_d_loss_fake_list : list
    p_g_loss_list : list
    train_scores : list
    val_scores : list
    metric_name : str
    title_prefix : str
    """
    fig, axs = plt.subplots(1, 2, figsize=(10, 3))

    # Subplot 1: GAN Training Loss
    axs[0].plot(p_d_loss_real_list, label='D Loss Real')
    axs[0].plot(p_d_loss_fake_list, label='D Loss Fake')
    axs[0].plot(p_g_loss_list, label='G Loss')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].set_title('GAN Training Loss')
    axs[0].legend()
    axs[0].grid(True)

    # Subplot 2: Train vs Validation Scores
    axs[1].plot(train_scores, label='Train')
    axs[1].plot(val_scores, label='Validation')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel(metric_name)
    axs[1].set_title(f'{title_prefix}: Train vs Validation {metric_name}')
    axs[1].legend()
    axs[1].grid(True)

    plt.tight_layout()
    plt.show()

# ---------------------------- #
# 3D Histogram Comparison
# ---------------------------- #
def plot_3d_histogram_comparison(y_before, y_augmented, ax, target_column='Threat Level'):
    bins = np.histogram_bin_edges(np.concatenate([y_before, y_augmented]), bins='auto')
    hist_before, _ = np.histogram(y_before, bins=bins, density=True)
    hist_aug, _ = np.histogram(y_augmented, bins=bins, density=True)

    xpos = (bins[:-1] + bins[1:]) / 2
    ypos_before = np.zeros_like(xpos)
    ypos_aug = np.ones_like(xpos)

    dx = dy = 0.3
    norm = Normalize(vmin=0, vmax=max(hist_before.max(), hist_aug.max()))
    cmap = cm.get_cmap('coolwarm')

    ax.bar3d(xpos, ypos_before, np.zeros_like(hist_before), dx, dy, hist_before,
             color=cmap(norm(hist_before)), alpha=0.8)
    ax.bar3d(xpos, ypos_aug, np.zeros_like(hist_aug), dx, dy, hist_aug,
             color=cmap(norm(hist_aug)), alpha=0.8)

    ax.set_xticks(xpos[::max(1, len(xpos)//10)])
    ax.set_xticklabels([f"{val:.1f}" for val in xpos[::max(1, len(xpos)//10)]], rotation=45)
    ax.set_yticks([0, 1])
    ax.set_yticklabels(['Original', 'Augmented'])
    ax.set_xlabel(target_column)
    ax.set_ylabel("Data Type")
    ax.set_zlabel("Density")
    ax.set_title(f"3D Histogram\n{target_column}", pad=10)

# ---------------------------- #
# Combined 2D & 3D Projection
# ---------------------------- #
def plot_combined_analysis_2d_3d(fe_processed_df, X_augmented, y_augmented, features_engineering_columns, target_column='Threat Level'):
    x_features = [col for col in features_engineering_columns if col != target_column]
    X_real = fe_processed_df[x_features].values
    X_generated = X_augmented[x_features].values

    X_combined = np.vstack((X_real, X_generated))
    labels = ['Real'] * len(X_real) + ['Generated'] * len(X_generated)
    colors = ['blue' if l == 'Real' else 'red' for l in labels]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_combined)
    y_before = fe_processed_df[target_column]

    fig, axes = plt.subplots(1, 4, figsize=(26, 6))
    fig.suptitle('2D Projections: Real vs Synthetic', fontsize=14)
    plt.subplots_adjust(wspace=0.4)


    sns.histplot(y_before, label='Original', color='blue', kde=True, stat="density", ax=axes[0])
    sns.histplot(y_augmented, label='Augmented', color='red', kde=True, stat="density", ax=axes[0])
    axes[0].set_title('Class Distribution')
    axes[0].legend()
    axes[0].set_xlabel(target_column)
    axes[0].set_ylabel("Density")

    X_pca = PCA(n_components=2).fit_transform(X_scaled)
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[1])
    axes[1].set_title('PCA (2D)')

    X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_scaled)
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[2])
    axes[2].set_title('t-SNE (2D)')

    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    X_umap = reducer.fit_transform(X_scaled)
    sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[3])
    axes[3].set_title('UMAP (2D)')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    print("\n plotting 3D Real VS Generated\n")
    fig_3d = plt.figure(figsize=(26, 6))
    fig_3d.suptitle('3D Projections: Real vs Synthetic', fontsize=14)

    plot_3d_histogram_comparison(y_before, y_augmented, fig_3d.add_subplot(1, 4, 1, projection='3d'), target_column)

    ax_pca = fig_3d.add_subplot(1, 4, 2, projection='3d')
    X_pca_3d = PCA(n_components=3).fit_transform(X_scaled)
    ax_pca.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=colors, alpha=0.6)
    ax_pca.set_title('PCA (3D)')

    ax_tsne = fig_3d.add_subplot(1, 4, 3, projection='3d')
    X_tsne_3d = TSNE(n_components=3, perplexity=30, random_state=42).fit_transform(X_scaled)
    ax_tsne.scatter(X_tsne_3d[:, 0], X_tsne_3d[:, 1], X_tsne_3d[:, 2], c=colors, alpha=0.6)
    ax_tsne.set_title('t-SNE (3D)')

    ax_umap = fig_3d.add_subplot(1, 4, 4, projection='3d')
    reducer_3d = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)
    X_umap_3d = reducer_3d.fit_transform(X_scaled)
    ax_umap.scatter(X_umap_3d[:, 0], X_umap_3d[:, 1], X_umap_3d[:, 2], c=colors, alpha=0.6)
    ax_umap.set_title('UMAP (3D)')

    plt.show()

# ---------------------------- #
# Main Pipeline
# ---------------------------- #
def SMOTE_GANs_evaluation_pipeline():
    data_augmentation_pipeline()

    loss_df = load_dataset("/content/drive/My Drive/Cybersecurity Data/gan_loss_log.csv")
    augmented_df = load_dataset("/content/drive/My Drive/Cybersecurity Data/x_y_augmented_data_google_drive.csv")
    fe_processed_df, loaded_label_encoders, num_fe_scaler = load_objects_from_drive()

    X_augmented = augmented_df.drop(columns=["Threat Level"])
    y_augmented = augmented_df["Threat Level"]

    features_engineering_columns = X_augmented.columns

    d_loss_real_list = loss_df["D_Loss_Real"]
    d_loss_fake_list = loss_df["D_Loss_Fake"]
    g_loss_list = loss_df["G_Loss"]

    # Optional: Replace with actual tracking results
    train_accuracy = np.linspace(0.65, 0.95, len(g_loss_list)) #train_scores
    val_accuracy = np.linspace(0.60, 0.93, len(g_loss_list)) #val_scores

    #print("\nApplying Custom Matplotlib Style\n")
    apply_custom_matplotlib_style()
    plot_combined_analysis_2d_3d(fe_processed_df, X_augmented, y_augmented, features_engineering_columns)

    #print("\n plotting gan_training_metrics\n")
    plot_gan_training_metrics(d_loss_real_list, d_loss_fake_list, g_loss_list,
                              train_accuracy, val_accuracy, metric_name='Accuracy',
                              title_prefix='GAN Performance')

def main_features_engineering_pipeline():

    print("\n =================================")
    print("   Feature Engineering Flowshart")
    print("   =================================")
    generate_flowchart()

    print("\n =================================")
    print("   Cyber Attack Simulation Pipeline")
    print("   =================================")
    fe_processed_df, cat_cols_label_encoders, num_fe_scaler = features_engineering_pipeline()

    print("\n =================================")
    print("          SMOT & GAN ")
    print("   =================================")
    # Execute the full augmentation pipeline if the script is run directly
    augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list = data_augmentation_pipeline()

    print("\n ========================================================")
    print("    SMOTE and GAN augmentation models performance Analysis ")
    print("   =========================================================")
    SMOTE_GANs_evaluation_pipeline()

if __name__ == "__main__":
    main_features_engineering_pipeline()