## **CyberThreat-Insight**  
**Anomalous Behavior Detection in Cybersecurity Analytics using Generative AI**

**Toronto, Septeber 08 2025**  
**Autor : Atsu Vovor**
>Master of Management in Artificial Intelligence    
>Consultant Data Analytics Specialist | Machine Learning |  
Data science | Quantitative Analysis |French & English Bilingual   

###  **Feature Engineering**   
The feature engineering process in our *Cyber Threat Insight* project was strategically designed to simulate realistic cyber activity, enhance anomaly visibility, and prepare a high-quality dataset for training robust threat classification models. Given the natural rarity and imbalance of cybersecurity anomalies, we adopted a multi-step workflow combining statistical simulation, normalization, feature selection, explainability, and data augmentation.



#### **Feature Engineering Flowchart**


```python
from graphviz import Digraph
from IPython.display import Image

# Create a directed graph
#dot = Digraph(comment='Cyber Threat Insight - Feature Engineering Workflow', format='png')
dot = Digraph("Cyber Threat Insight - Feature Engineering Workflow", format="png")


# Feature Engineering Phases
dot.node('Start', 'Start')
dot.node('DataInj', 'Data Injection\n(Cholesky-Based Perturbation)', shape='box', style='filled', fillcolor='lightblue')
dot.node('Scaling', 'Feature Normalization & Scaling\n(Min-Max, Z-score)', shape='box', style='filled', fillcolor='lightgray')
dot.node('CorrHeat', 'Correlation Heatmap Analysis\n(Pearson/Spearman)', shape='box', style='filled', fillcolor='orange')
dot.node('FeatImp', 'Feature Importance\n(Random Forest)', shape='box', style='filled', fillcolor='gold')
dot.node('SHAP', 'Model Explainability\n(SHAP Values)', shape='box', style='filled', fillcolor='lightgreen')
dot.node('PCA', 'PCA & Variance Explained\n(Scree Plot)', shape='box', style='filled', fillcolor='plum')
dot.node('Augment', 'Data Augmentation\n(SMOTE, GAN)', shape='box', style='filled', fillcolor='lightpink')
dot.node('End', 'Feature Set Ready for Modeling', shape='ellipse', style='filled', fillcolor='lightyellow')

# Arrows to show workflow
dot.edge('Start', 'DataInj')
dot.edge('DataInj', 'Scaling')
dot.edge('Scaling', 'CorrHeat')
dot.edge('CorrHeat', 'FeatImp')
dot.edge('FeatImp', 'SHAP')
dot.edge('SHAP', 'PCA')
dot.edge('PCA', 'Augment')
dot.edge('Augment', 'End')

features_engineering_flowchart = dot.render("features_engineering_flowchart", format="png", cleanup=False)
display(Image(filename="features_engineering_flowchart.png"))
print("Flowchart generated successfully!")
```
<p align="center">
  <img src="feature_engineering_flowwwchart.png" 
       alt="Centered Image" 
       style="width: 250px; height: auto;">
</p>

## **Feature Analyis**  .

#### **1. Synthetic Data Loading**

We began with a synthetic dataset that simulates real-time user sessions and system behaviors, including attributes such as login attempts, session duration, data transfer, and system resource usage. This dataset serves as a safe and flexible baseline to emulate both normal and suspicious behaviors without exposing sensitive infrastructure data.

#### **2. Anomaly Injection â€“ Cholesky-Based Perturbation**

To introduce statistically sound anomalies, we applied a Cholesky decomposition-based perturbation to the feature covariance matrix. This method creates subtle but realistic multivariate deviations in the dataset, reflecting how actual threats often manifest through combinations of unusual behaviors (e.g., high data transfer coupled with long session durations).

#### **3. Feature Normalization**

All numerical features were normalized using a combination of **Min-Max Scaling** and **Z-score Standardization**. This step ensures that features with different units or scales (e.g., memory usage vs. login attempts) contribute equally during model training, especially for distance-based algorithms.

#### **4. Correlation Analysis**

Using **Pearson** and **Spearman correlation heatmaps**, we examined inter-feature relationships to detect multicollinearity. This analysis helped eliminate redundant features and highlighted meaningful operational linkages between system metrics, such as correlations between CPU and memory usage during suspicious sessions.

#### **5. Feature Importance (Random Forest)**

We trained a Random Forest classifier to compute feature importance scores. These scores provided insights into which features had the most predictive power for classifying threat levels, enabling targeted refinement of the feature set.

#### **6. Model Explainability (SHAP Values)**

To ensure model transparency, we applied **SHAP (SHapley Additive exPlanations)** for both global and local interpretability. SHAP values quantify how each feature impacts the modelâ€™s decisions for individual predictions, which is critical for cybersecurity analysts needing to validate threat classifications.

#### **7. Dimensionality Reduction (PCA)**

We employed **Principal Component Analysis (PCA)** to reduce feature dimensionality while retaining maximum variance. A scree plot was used to identify the optimal number of components. This step improves computational efficiency and enhances model generalization.

#### **8. Data Augmentation (SMOTE and GANs)**

To address the significant class imbalance between benign and malicious sessions, we applied two augmentation strategies:

* **SMOTE (Synthetic Minority Over-sampling Technique)** to interpolate new synthetic samples for underrepresented classes.
* **Generative Adversarial Networks (GANs)** to produce high-fidelity, realistic threat scenarios that further enrich the minority class.

  

#### **Outcome**

Through this comprehensive workflow, we generated a clean, balanced, and interpretable feature set optimized for training machine learning models. This feature engineering pipeline enabled the system to detect nuanced threat patterns while maintaining explainability and performance across diverse threat levels.


```python
# ==============================
# Cyber Attack Simulation Pipeline
# ==============================

# --- Google Drive Mount ---
from google.colab import drive
drive.mount('/content/drive')

# --- Imports ---
import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

import shap


# --- Column Dictionary ---
def get_column_dic():
    columns_dic = {
        "numerical_columns": [
            "Timestamps", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "explanatory_data_analysis_columns": [
            "Date Reported", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "user_activity_features": [
            "Risk Level", "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score"
        ],
        "initial_dates_columns": ["Date Reported", "Date Resolved", "Timestamps"],
        "categorical_columns": [
            "Issue ID", "Issue Key", "Issue Name", "Category", "Severity", "Status", "Reporters",
            "Assignees", "Risk Level", "Department Affected", "Remediation Steps", "KPI/KRI",
            "User ID", "Activity Type", "User Location", "IP Location", "Threat Level",
            "Defense Action", "Color"
        ],
        "features_engineering_columns": [
            "Issue Response Time Days", "Impact Score", "Cost",
            "Session Duration in Second", "Num Files Accessed", "Login Attempts",
            "Data Transfer MB", "CPU Usage %", "Memory Usage MB", "Threat Score", "Threat Level"
        ],
        "numerical_behavioral_features": [
            "Login Attempts", "Data Transfer MB", "CPU Usage %", "Memory Usage MB",
            "Session Duration in Second", "Num Files Accessed", "Threat Score"
        ]
    }
    return columns_dic


#from json import load
# ------------------------------Save df_fe, label_encoders anf numerical columns scaler to to your Google Drive------------------------
def save_objects_to_drive(df_fe,
                          cat_cols_label_encoders,
                          num_fe_scaler,
                          filepath_df="/content/drive/My Drive/Cybersecurity Data/df_fe.pkl",
                          filepath_cat_cols_label_encoders="/content/drive/My Drive/Model deployment/cat_cols_label_encoders.pkl",
                          filepath_num_fe_scaler="/content/drive/My Drive/Model deployment/ num_fe_scaler.pkl"):
    try:
        # Ensure the directory exists for df_fe
        df_directory = os.path.dirname(filepath_df)
        if not os.path.exists(df_directory):
            os.makedirs(df_directory)
            print(f"Created directory: {df_directory}")

        # Ensure the directory exists for label_encoders and scaler
        model_directory = os.path.dirname(filepath_cat_cols_label_encoders)
        if not os.path.exists(model_directory):
            os.makedirs(model_directory)
            print(f"Created directory: {model_directory}")


        with open(filepath_df, 'wb') as f:
            pickle.dump(df_fe, f)
        print(f"DataFrame saved successfully to: {filepath_df}")

        with open(filepath_cat_cols_label_encoders, 'wb') as f:
            pickle.dump(cat_cols_label_encoders, f)
        print(f"Label encoders saved successfully to: {filepath_cat_cols_label_encoders}")

        with open(filepath_num_fe_scaler, 'wb') as f:
            pickle.dump(num_fe_scaler, f)
        print(f"Label encoders saved successfully to: {filepath_num_fe_scaler}")

    except Exception as e:
        print(f"An error occurred while saving: {e}")


# ----------------Load df_fe and label_encoders from your Google Drive----------------------------------
def load_objects_from_drive(filepath_df="/content/drive/My Drive/Cybersecurity Data/df_fe.pkl",
                            filepath_cat_cols_label_encoders="/content/drive/My Drive/Model deployment/cat_cols_label_encoders.pkl",
                            filepath_num_fe_scaler="/content/drive/My Drive/Model deployment/ num_fe_scaler.pkl"):
    try:
        with open(filepath_df, 'rb') as f:
            df_fe = pickle.load(f)
        print(f"DataFrame loaded successfully from: {filepath_df}")

        with open(filepath_cat_cols_label_encoders, 'rb') as f:
            cat_cols_label_encoders = pickle.load(f)
        print(f"Label encoders loaded successfully from: {filepath_cat_cols_label_encoders}")

        with open(filepath_num_fe_scaler, 'rb') as f:
            num_fe_scaler = pickle.load(f)
        print(f"Label encoders loaded successfully from: {filepath_num_fe_scaler}")

        return df_fe, cat_cols_label_encoders, num_fe_scaler

    except Exception as e:
        print(f"An error occurred while loading: {e}")
        return None, None, None # Return None for the third value as well


#-------------Generate Synthetic Anomalies Using Cholesky-Based Perturbation-------------------

def get_files_path(
        normal_operations_file_path = "/content/drive/My Drive/Cybersecurity Data/normal_and_anomalous_cybersecurity_dataset_for_google_drive_kb.csv",
        combined_normal_and_anomaly_file_path = "/content/combined_normal_and_anomaly_output_file_for_google_drive_kb.csv"):

    return {
        "normal_operations_file_path": normal_operations_file_path,
        "combined_normal_and_anomaly_file_path":  combined_normal_and_anomaly_file_path
    }

def load_Synthetic_dataset(filepath):
    return pd.read_csv(filepath)

def scale_data(df, features):
    scaler = StandardScaler()
    features_to_scale = [f for f in features if f != 'Timestamps']
    scaled = scaler.fit_transform(df[features_to_scale].dropna())
    return scaled, scaler

def cholesky_decomposition(scaled_data):
    cov_matrix = np.cov(scaled_data, rowvar=False)
    L = np.linalg.cholesky(cov_matrix)
    return L

def generate_cholesky_anomalies(real_data, L, num_samples=1000):
    np.random.seed(42)
    normal_samples = np.random.randn(num_samples, real_data.shape[1])
    synthetic_anomalies = normal_samples @ L.T
    return synthetic_anomalies

def inverse_transform(synthetic_data, scaler):
    return scaler.inverse_transform(synthetic_data)

def create_anomaly_df(original_data, synthetic_original, features):
    df_synthetic = pd.DataFrame(synthetic_original, columns=features)

    # Create full column DataFrame for synthetic data with same structure as original
    df_synthetic_full = pd.DataFrame(columns=original_data.columns)

    # Fill known numerical features
    for col in features:
        df_synthetic_full[col] = df_synthetic[col]

    # Fill in the rest of the columns using random sampling or generation
    for col in original_data.columns:
        if col not in features:
            if original_data[col].dtype == 'object':
                df_synthetic_full[col] = np.random.choice(original_data[col].dropna().unique(), size=len(df_synthetic_full))
            elif np.issubdtype(original_data[col].dtype, np.datetime64):
                # If timestamps exist, shift a base date with random offsets
                base = pd.to_datetime("2024-01-01")
                df_synthetic_full[col] = base + pd.to_timedelta(np.random.randint(0, 90, size=len(df_synthetic_full)), unit='D')
            else:
                df_synthetic_full[col] = np.random.choice(original_data[col].dropna(), size=len(df_synthetic_full))

    #df_synthetic_full["Threat Level"] = "Anomalous"
    df_synthetic_full["Source"] = "Synthetic"

    df_real = original_data.copy()
    df_real["Source"] = "Real"

    df_combined = pd.concat([df_real, df_synthetic_full], ignore_index=True)
    return df_combined

def save_dataset(df, path):
    df.to_csv(path, index=False)
    print(f"Saved combined dataset with synthetic anomalies to: {path}")

def data_injection_cholesky_based_perturbation(file_paths = "", save_data_true_false = True):

    print("Anomaly Injection â€“ Cholesky-Based Perturbation...")
    if save_data_true_false == True:
        file_paths = get_files_path()
        df_real = load_Synthetic_dataset(file_paths["normal_operations_file_path"])
    else:
        df_real = load_Synthetic_dataset(file_paths)


    #df_real.info()
    #display(df_real.head())

    # Call get_column_dic here to access numerical_columns
    columns_dic = get_column_dic()
    numerical_columns = columns_dic["numerical_columns"]

    numerical_columns_for_scaling = [col for col in numerical_columns if col != "Timestamps"]

    scaled_data, scaler = scale_data(df_real, numerical_columns_for_scaling)

    L = cholesky_decomposition(scaled_data)
    synthetic_scaled = generate_cholesky_anomalies(df_real[numerical_columns_for_scaling], L, num_samples=100)
    synthetic_original = inverse_transform(synthetic_scaled, scaler)

    normal_and_combined_cholesky_based_perturbation_df = create_anomaly_df(df_real, synthetic_original, numerical_columns_for_scaling)

    #normal_and_combined_cholesky_based_perturbation_df.info()
    #display(normal_and_combined_cholesky_based_perturbation_df.head())

    if save_data_true_false == True:
        save_dataset(normal_and_combined_cholesky_based_perturbation_df, file_paths["combined_normal_and_anomaly_file_path"])

    return normal_and_combined_cholesky_based_perturbation_df


# -------------------------------Normalize numerical feature--------------------------------------
def normalize_numerical_features(df, p_numerical_columns):

    # normalized_df, scaler =  scale_data(df, p_numerical_columns)
    # return normalized_df, scaler
    scaler = MinMaxScaler()
    df[p_numerical_columns] = scaler.fit_transform(df[p_numerical_columns])
    return df, scaler # Return the DataFrame and scaler



def encode_dates(df, date_columns):
    """
    Extracts date components from specified columns in a DataFrame.

    Parameters:
      df (DataFrame): The DataFrame containing date columns.
      date_columns (list): List of date columns to extract components from.

    Returns:
      DataFrame: DataFrame with additional date component columns.
    """
    processed_df = df.copy()

    for date_col in date_columns:
        # Convert the column to datetime if it's not already
        processed_df[date_col] = pd.to_datetime(processed_df[date_col], errors='coerce')

        # Check if the column is a datetime column before applying .dt accessor
        if pd.api.types.is_datetime64_any_dtype(processed_df[date_col]):
            processed_df[f"year_{date_col}"] = processed_df[date_col].dt.year
            processed_df[f"month_{date_col}"] = processed_df[date_col].dt.month
            processed_df[f"day_{date_col}"] = processed_df[date_col].dt.day
            processed_df[f"day_of_week_{date_col}"] = processed_df[date_col].dt.dayofweek  # Monday=0, Sunday=6
            processed_df[f"day_of_year_{date_col}"] = processed_df[date_col].dt.dayofyear
        else:
            print(f"Warning: Column '{date_col}' is not a datetime column and will be skipped.")

    # Example of converting timestamps to seconds (if a timestamp column exists)
    if "Timestamps" in date_columns:
        processed_df["timestamp_seconds"] = processed_df["Timestamps"].astype(int) / 10**9

    return processed_df.drop(columns=date_columns)


def encode_categorical_columns(df, categorical_columns):
    """
    Applies label encoding to specified categorical columns in a DataFrame.

    Parameters:
      df (DataFrame): The DataFrame containing categorical columns.
      categorical_columns (list): List of columns to apply label encoding to.

    Returns:
      DataFrame, dict: DataFrame with encoded categorical columns and a dictionary of label encoders.
    """
    processed_df = df.copy()
    label_encoders = {}


    for column in categorical_columns:
        le = LabelEncoder()
        processed_df[column] = le.fit_transform(processed_df[column].astype(str))
        label_encoders[column] = le

    return processed_df, label_encoders

def decode_categorical_columns( df_to_decode, label_encoders):
    """
    Decodes label-encoded categorical columns in a DataFrame.

    Parameters:
      df_to_decode (DataFrame): The DataFrame containing label-encoded categorical columns.
      label_encoders (dict): Dictionary of LabelEncoders used for encoding, with column names as keys.
    """

    # initialize decoded data frame(decoded_df)
    decoded_df = [[]]
    processed_df_to_decode = df_to_decode.copy()

    for column, le in label_encoders.items():
        if column in processed_df_to_decode.columns:
            decoded_df[column] = le.inverse_transform(processed_df_to_decode[column])




def preprocess_dataframe(df, numerical_columns, date_columns, categorical_columns):
    """
    Main function to preprocess a DataFrame by encoding dates and categorical columns.

    Parameters:
      df (DataFrame): Original DataFrame to be copied and processed.
      numerical_columns (list): List of numerical columns (currently unused in this function).
      date_columns (list): List of date columns to extract components from.
      categorical_columns (list): List of categorical columns to encode.

    Returns:
      DataFrame, dict: Processed DataFrame and dictionary of label encoders.
    """

    #Normalize numerical feature
    #processed_df = normalize_numerical_features(df, numerical_columns)
    df, normalize_numerical_features_scaler = normalize_numerical_features(df, [i for i in numerical_columns if i not in ['Timestamps']])

    # Apply date encoding using the df
    processed_df = encode_dates(df, date_columns)  # Use the output of normalize_numerical_features

    # Apply categorical encoding using the processed_df, but exclude date_columns
    processed_df, categorical_columns_label_encoders = encode_categorical_columns(processed_df, [col for col in categorical_columns if col not in date_columns])

    return processed_df, categorical_columns_label_encoders, normalize_numerical_features_scaler # Return processed_df instead of df
#------------------------------------------------------------------------------------------------

# 1. Correlation Heatmap
def plot_correlation_heatmap(ax, df, method='pearson'):
    numeric_df = df.select_dtypes(include=[np.number])
    corr = numeric_df.corr(method=method)
    sns.heatmap(corr, cmap='coolwarm', annot=False, fmt='.2f', square=True, ax=ax)
    ax.set_title(f'{method.capitalize()} Correlation Heatmap')

# 2. Feature Importance
def plot_feature_importance(ax, X, y, top_n=None):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    importances = rf.feature_importances_

    if top_n is None or top_n > len(importances):
        top_n = len(importances)

    indices = np.argsort(importances)[-top_n:]
    ax.barh(range(top_n), importances[indices], align='center')
    ax.set_yticks(range(top_n))
    ax.set_yticklabels([X.columns[i] for i in indices])
    ax.set_xlabel("Feature Importance")
    ax.set_title("Top Random Forest Feature Importances")

    return rf

# 3. SHAP Summary Plot (Standalone, not in subplot)
# Function to set font properties for plot axes
def set_font_properties(ax, x_fontsize=8, y_fontsize=8, labelcolor='black', mean_shap_fontsize=8, font_name = 'sans-serif'):
    """
    Set the font properties for axes ticks.

    Args:
    - ax: The axes object for the plot
    - x_fontsize: Font size for x-axis labels
    - y_fontsize: Font size for y-axis labels
    - labelcolor: Color for the labels (default is 'blue')
    """
    ax.tick_params(axis='x', labelsize=x_fontsize, labelcolor=labelcolor)
    ax.tick_params(axis='y', labelsize=y_fontsize, labelcolor=labelcolor)

    # Adjust the font for the x-axis labels
    for label in ax.get_xticklabels():
        label.set_fontsize(x_fontsize)  # Set font size
        label.set_fontname(font_name)  # Default sans-serif font
        label.set_color(labelcolor)  # Set label color


    # Adjust the font for the y-axis labels
    for label in ax.get_yticklabels():

        label.set_fontsize(y_fontsize)  # Set font size
        label.set_fontname(font_name)  # Default sans-serif font
        label.set_color(labelcolor)  # Set label color

    # Adjust mean(|SHAP value|) font size (located in the text below the plot)
    for text in ax.texts:
        if 'mean(|SHAP value|)' in text.get_text():
            text.set_fontsize(mean_shap_fontsize)  # Reduce the font size for the mean(|SHAP value|) text
            text.set_fontname(font_name)  # Default sans-serif font
            text.set_color(labelcolor)  # Set label color



# Function to update plot title font
def update_title(title, fontsize=8, family='sans-serif', fontweight='normal'):
    """
    Update the title of the plot with custom font properties.

    Args:
    - title: Title of the plot
    - fontsize: Font size for the title
    - family: Font family for the title
    - fontweight: Font weight for the title
    """
    plt.title(title, fontsize=fontsize, family=family, fontweight=fontweight)

def smaller_shap_summary_plot(shap_values, X, y, plot_type="bar", plot_size=(30, 15), title="SHAP Summary Plot", class_names=None, xlabel_fontsize=8):
    """
    Generates a smaller SHAP summary plot.

    Args:
        shap_values: SHAP values (output from SHAP model explainer)
        X: The feature matrix(Sample data used for generating the SHAP plot)
        plot_type: The type of plot ("dot", "bar", etc.).
        plot_size: A tuple (width, height) specifying the plot's size in inches.
        title: Custom title for the plot (default is 'SHAP Summary Plot')
        class_names: List of class names for the legend.
        xlabel_fontsize: Font size for the x-axis label.
    """

    # If class_names are not provided, generate default ones
    if class_names is None:
        labels = sorted(list(set(y) | set(y)))
        level_mapping = {0: "Low", 1: "Medium", 2: "High", 3: "Critical"}
        class_names = [level_mapping.get(label) for label in labels]

    # Adjust the figure size for the SHAP summary plot
    plt.figure(figsize=(plot_size[0]/10, plot_size[1]/10))

    shap.summary_plot(shap_values, X, plot_type=plot_type, class_names=class_names, show=False) #prevent auto showing the plot, so we can modify it.
    #shap.summary_plot(shap_values, X, plot_type=plot_type, feature_names=list(X), class_names=class_names, show=False)
    plt.tight_layout() #reduce white space around plot.

    # Access the current axes (for summary plot)
    ax = plt.gca()

    # Change font properties for feature names and axis labels
    set_font_properties(ax, mean_shap_fontsize=8) # Set the font size for the mean(|SHAP value|) text

    # Update the title with custom font
    update_title(title)

    # Adjust legend font size
    ax.legend(fontsize=8)

    # Adjust x-axis label font size
    ax.set_xlabel(ax.get_xlabel(), fontsize=xlabel_fontsize)


    plt.show() #manually show it.


def plot_shap_summary(model, X_sample, y):
    target_column="Threat Level"
    level_mapping = {"Low": 0, "Medium": 1, "High": 2, "Critical": 3}
    class_names = list(level_mapping.keys())

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    if isinstance(shap_values, list) and len(shap_values) > 1:
        #shap.summary_plot(shap_values[1], X_sample, plot_size=(2, 2))  # Binary case
        smaller_shap_summary_plot(shap_values[1], X_sample, y, class_names=class_names)
    else:
        #shap.summary_plot(shap_values, X_sample, plot_size=(2, 2))
        # Generate summary plot with custom class names in the legend
        smaller_shap_summary_plot(shap_values, X_sample, y, class_names=class_names)

# 4. PCA Scree Plot
def plot_pca_variance(ax, X, threshold=0.95):
    pca = PCA().fit(X)
    cum_var = np.cumsum(pca.explained_variance_ratio_)
    ax.plot(cum_var, marker='o', linestyle='--', color='b')
    ax.axhline(y=threshold, color='r', linestyle='-')
    ax.set_title("PCA Scree Plot")
    ax.set_xlabel("Num Components")
    ax.set_ylabel("Cumulative Explained Variance")
    ax.grid(True)

# 5. Main Driver Function
def run_feature_analysis(df_fe, target_column="Threat Level", corr_method="pearson"):
    print("Running Feature Analysis Pipeline...")

    df_local = df_fe.copy()

    # Encode target if needed
    if df_local[target_column].dtype == 'object':
        le = LabelEncoder()
        df_local[target_column] = le.fit_transform(df_local[target_column])

    X = df_local.select_dtypes(include=[np.number]).drop(columns=[target_column], errors='ignore')
    y = df_local[target_column]

    # Create subplots (3 panels: correlation, importance, PCA)
    fig, axes = plt.subplots(1, 3, figsize=(24, 6))

    # Plot 1: Correlation Heatmap
    plot_correlation_heatmap(axes[0], df_local, method=corr_method)

    # Plot 2: Feature Importance
    model = plot_feature_importance(axes[1], X, y, top_n=15)

    # Plot 3: PCA Scree
    plot_pca_variance(axes[2], X)

    plt.tight_layout()
    plt.show()

    # Plot 4: SHAP Summary (standalone)
    print("\nSHAP Summary Plot:")
    X_sample = shap.utils.sample(X, 200, random_state=42) if len(X) > 200 else X
    plot_shap_summary(model, X_sample, y)

    print("Feature analysis complete.")

#-------------------------features_engineering_pipeline ---------------------------
def features_engineering_pipeline(file_path = None , analysis_true_false = True):

    print("Feature engineering pipeline started.")
    #get features dic
    columns_dic = get_column_dic()
    numerical_columns = columns_dic["numerical_columns"]
    features_engineering_columns = columns_dic["features_engineering_columns"]
    initial_dates_columns = columns_dic["initial_dates_columns"]
    categorical_columns = columns_dic["categorical_columns"]

    #data injection: Anomaly Injection â€“ Cholesky-Based Perturbation
    if analysis_true_false == True:
        naccbp_df = data_injection_cholesky_based_perturbation()
    else:
        naccbp_df = data_injection_cholesky_based_perturbation(file_path, save_data_true_false = False)

    #data collectionn, Generation and Preprocessing
    df = naccbp_df.copy()

    # Convert date columns to datetime objects
    for col in initial_dates_columns:
        df[col] = pd.to_datetime(df[col])  # Convert to datetime

    # We filter the Timestamps from the columns to apply the MinMaxScaler
    df, cat_cols_label_encoders, num_fe_scaler = preprocess_dataframe(df, numerical_columns, initial_dates_columns, categorical_columns)

    #feature analysis
    df_fe = df[features_engineering_columns].copy()
    #display(df_fe.head())

    if analysis_true_false:
        # Run feature analysis
        run_feature_analysis(df_fe, target_column="Threat Level", corr_method="pearson")

        # deploy fe_processd_df and label_encoder to google drive
        save_objects_to_drive(df_fe, cat_cols_label_encoders, num_fe_scaler)

    print("Feature engineering pipeline completed.")

    return df_fe, cat_cols_label_encoders, num_fe_scaler

if __name__ == "__main__":

    fe_processed_df, cat_cols_label_encoders, num_fe_scaler = features_engineering_pipeline()

```
<p align="center">
  <img src="feature plots.png" 
       alt="Centered Image" 
       style="width: 700px; height: auto;">
</p>

<p align="center">
  <img src="Shap.png" 
       alt="Centered Image" 
       style="width: 700px; height: auto;">
</p>


 #### **Feature Engineering â€“ Advanced Data Augmentation using SMOTE and GANs**

To address severe class imbalance and enhance the quality of the synthetic training data in our cyber threat insight model, we implemented a hybrid augmentation strategy. This stage of feature engineering combines **SMOTE** (Synthetic Minority Over-sampling Technique) and **GANs** (Generative Adversarial Networks) to increase representation of rare threat levels and capture complex behavioral patterns often found in high-dimensional cybersecurity data.


#### **Literature Review: SMOTE vs GANs for Synthetic Data Generation**


SMOTE and GANs are both used to generate synthetic data to address class imbalance. However, they differ significantly in approach, complexity, application or the types of data they can handle. Here's a breackdown:



**1. Methodology**

   - **SMOTE**: SMOTE is a straightforward oversampling technique for tabular data. It generates synthetic data by interpolating between samples of the minority class. Specifically, it selects a minority class sample, finds its nearest neighbors, and creates synthetic samples along the line segments joining the original sample with one or more of its neighbors. SMOTE is typically applied to structured, tabular data.

   - **GANs**: GANs are a class of deep learning models that involve two neural networksâ€”a generator and a discriminatorâ€”competing against each other. The generator creates synthetic samples, while the discriminator evaluates how close these samples are to real data. Over time, the generator learns to produce increasingly realistic samples. GANs are versatile and can generate complex, high-dimensional data like images, text, and time-series data.

**2. Complexity**

   - **SMOTE**: SMOTE is computationally simple and easier to implement because it doesn't require training a neural network. It's usually faster and works well for moderately complex datasets.

   - **GANs**: GANs are computationally intensive and require training a generator and discriminator, which are often deep neural networks. They require significant data, compute resources, and tuning. GANs are more complex but can capture intricate patterns and distributions in the data.

**3. Types of Data**

   - **SMOTE**: Works best for numerical tabular data, where generating synthetic samples by interpolation is feasible. It can struggle with categorical variables or complex data relationships.

   - **GANs**: Can handle a variety of data types, including high-dimensional and unstructured data like images, audio, and text. GANs are also better suited for generating more realistic and diverse samples for complex distributions.

**4. Application Scenarios**

   - **SMOTE**: Typically applied in class imbalance for binary classification problems, especially in structured data settings. For example, itâ€™s widely used in fraud detection, medical diagnostics, and credit scoring when the minority class samples are significantly fewer than the majority class.

   - **GANs**: GANs are applicable when complex, high-quality synthetic data is required. They are often used in fields like image processing, speech synthesis, and video generation. GANs can also be useful for cybersecurity, where generating realistic threat data may involve complex relationships and high-dimensional feature spaces.

**5. Synthetic Data Quality**

   - **SMOTE**: Produces synthetic samples that are close to the original samples but lacks diversity since it simply interpolates between existing points. This can lead to overfitting, as the generated data may not capture the full range of variability in minority class characteristics.

   - **GANs**: With careful tuning, GANs can generate highly realistic samples that capture complex patterns in the data, offering better generalization and diversity than SMOTE. However, they also come with risks like mode collapse (when the generator produces limited variations of data).


**Summary**
- **SMOTE** is a simpler, faster, and more accessible technique, suitable for lower-dimensional tabular data and basic class imbalance issues.
- **GANs** are more advanced, versatile, and powerful, capable of producing high-dimensional, complex data for applications that demand high-quality synthetic samples.

In cybersecurity, you might use **SMOTE** for imbalanced tabular data with relatively simple feature interactions, while **GANs** can be advantageous for generating more complex synthetic attack patterns or when working with high-dimensional activity logs and network data.  


| Criteria                      | SMOTE                                                               | GANs                                                                                            |
| ----------------------------- | ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **Methodology**               | Interpolates new samples between existing minority class instances. | Uses a generator-discriminator adversarial setup to produce highly realistic synthetic samples. |
| **Complexity**                | Simple, rule-based; no training required.                           | Complex; requires training deep neural networks.                                                |
| **Best for**                  | Structured, tabular data with moderate feature interaction.         | High-dimensional, non-linear, or unstructured data (e.g., logs, behaviors).                     |
| **Synthetic Data Quality**    | Limited diversity; risk of overfitting due to linear interpolation. | Can generate diverse, realistic samples capturing underlying patterns.                          |
| **Cybersecurity Application** | Ideal for boosting minority class in structured event logs.         | Suitable for simulating diverse and realistic threat scenarios.   

---  
     

### **SMOTE + GANs Implementation in Cyber Threat Insight**

To ensure our cyber threat insight model performs robustly across all threat levels including rare but critical cases, we implemented a two-fold data augmentation strategy using **SMOTE (Synthetic Minority Over-sampling Technique)** and **Generative Adversarial Networks (GANs)** as the final step in the feature engineering pipeline.

### Step 1: Handling Imbalanced Classes with SMOTE

In real-world cybersecurity datasets, high-risk threat events are typically underrepresented. To mitigate this class imbalance, we first applied **SMOTE**, a statistical technique that synthesizes new samples by interpolating between existing ones in the feature space. SMOTE oversamples underrepresented threat levels (e.g., High, Critical). This ensures the classifier doesnâ€™t overfit to the majority class, enabling better detection of rare threats.

* **Input**: Cleaned and preprocessed numerical dataset.
* **Process**: SMOTE was applied to oversample the minority class based on `Threat Level`.
* **Output**: A balanced dataset where minority threat classes (e.g., *Critical*, *High*) have increased representation.

```python
X_resampled, y_resampled = balance_data_with_smote(processed_num_df)
```

* **Purpose**: Create a balanced training dataset by synthetically adding interpolated samples from the minority class.
* **Impact**: Improved recall and F1-score for rare threat types.  

This step ensured that our model would not be biased toward majority class labels, improving its ability to generalize and detect less frequent, high-impact events.

  


### Step 2: Enhancing Diversity: Learning Complex Patterns with GAN-Based Threat Simulation

To further enrich the dataset beyond SMOTE's linear interpolations, we trained a custom GAN to generate more diverse non-linear high-fidelity cyber threat behaviors data. Our GAN architecture consists of:

* A **Generator** that learns to create synthetic threat vectors from random noise.
* A **Discriminator** that learns to distinguish real data from synthetic data.

The adversarial training process was carefully monitored using early stopping based on generator loss to prevent overfitting and ensure sample quality.


```python
generator, discriminator = build_gan(latent_dim=100, n_outputs=X_resampled.shape[1])
generator, d_loss_real_list, d_loss_fake_list, g_loss_list = train_gan(
    generator, discriminator, X_resampled, latent_dim=100, epochs=1000
)
```

Once trained, the generator was used to create highly realistic samples( 1,000 synthetic threat vectors), each mimicking realistic but previously unseen behaviors(the statistical distribution of real threat behaviors).


```python
synthetic_data = generate_synthetic_data(generator, n_samples=1000, latent_dim=100, columns=X_resampled.columns)
```



### Step 3:  Final Dataset Augmentation - Data Fusion and Export

The synthetic GAN-generated samples were combined with the SMOTE-resampled dataset to form a robust, high-quality augmented dataset, maximizing both statistical and generative diversity.

```python
X_augmented, y_augmented = augment_data(X_resampled, y_resampled, synthetic_data)
augmented_df = concatenate_data_along_columns(X_augmented, y_augmented)
```

The final augmented dataset was saved to cloud storage for traceability and reproducibility.

```python
save_dataframe_to_google_drive(augmented_df, "x_y_augmented_data_google_drive.csv")
```

  

### Outcomes and Benefits

By combining **SMOTE** and **GANs**, we created a rich, well-balanced dataset that allows our models to:

* Learn effectively from both observed and synthetic threat events.
* **Improve detection accuracy**: Detect rare but impactful security threat events with higher sensitivity.
* Generalize to novel behaviors not originally present in the training data.

This hybrid augmentation pipeline significantly improves the reliability and robustness of our cyber threat insight models


```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
from imblearn.over_sampling import SMOTE
from tqdm import tqdm
import matplotlib.pyplot as plt
import os

from IPython.display import display

# ------------------------- SMOTE: Handle class imbalance -------------------------
def balance_data_with_smote(df, target_column="Threat Level"):
    """
    Apply SMOTE to balance minority classes in the dataset.
    Returns resampled feature set and target labels.
    """
    print("Balancing data with SMOTE...")
    X = df.drop(columns=[target_column])
    y = df[target_column]
    smote = SMOTE(sampling_strategy='minority', random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    return X_resampled, y_resampled

# ------------------- Build Generator and Discriminator for GAN -------------------
def build_gan(latent_dim, n_outputs):
    """
    Build and compile a basic GAN architecture with:
    - A generator that outputs synthetic samples
    - A discriminator that classifies real vs synthetic samples
    Returns both models.
    """
    def build_generator():
        model = tf.keras.Sequential([
            layers.Dense(128, activation="relu", input_dim=latent_dim),
            layers.Dense(256, activation="relu"),
            layers.Dense(n_outputs, activation="tanh")
        ])
        return model

    def build_discriminator():
        model = tf.keras.Sequential([
            layers.Dense(256, activation="relu", input_shape=(n_outputs,)),
            layers.Dense(128, activation="relu"),
            layers.Dense(1, activation="sigmoid")
        ])
        return model

    generator = build_generator()
    discriminator = build_discriminator()
    discriminator.compile(optimizer='adam', loss='binary_crossentropy')
    return generator, discriminator

# -------------------------- Train GAN with Logging --------------------------
def train_gan(generator, discriminator, X_real, latent_dim, epochs=1000, batch_size=64,
              plot_loss=False, early_stop_patience=50, output_dir="/content/drive/My Drive/Cybersecurity Data/"):
    """
    Train GAN using real synthetic data with optional logging, early stopping, and visualization.
    Tracks generator and discriminator losses and saves logs and plots to output_dir.
    """
    os.makedirs(output_dir, exist_ok=True)

    d_loss_real_list = []
    d_loss_fake_list = []
    g_loss_list = []

    best_g_loss = np.inf
    patience_counter = 0

    for epoch in tqdm(range(epochs), desc="Training GAN"):
        # Generate fake samples
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        gen_data = generator.predict(noise, verbose=0)

        # Sample real data
        idx = np.random.randint(0, X_real.shape[0], batch_size)
        real_data = X_real.iloc[idx].values

        # Labels for real and fake data
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # Train discriminator on real and fake data
        d_loss_real = discriminator.train_on_batch(real_data, real_labels)
        d_loss_fake = discriminator.train_on_batch(gen_data, fake_labels)

        # Train generator to fool the discriminator
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        g_loss = discriminator.train_on_batch(generator.predict(noise, verbose=0), real_labels)

        # Log losses
        d_loss_real_list.append(d_loss_real)
        d_loss_fake_list.append(d_loss_fake)
        g_loss_list.append(g_loss)

        # Early stopping logic for generator loss
        if g_loss < best_g_loss:
            best_g_loss = g_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print(f"\nEarly stopping at epoch {epoch} - No improvement in G loss for {early_stop_patience} epochs.")
                break

    # Save loss plot and CSV log
    plt.savefig(os.path.join(output_dir, "gan_loss_plot.png"))
    plt.close()

    loss_df = pd.DataFrame({
        "D_Loss_Real": d_loss_real_list,
        "D_Loss_Fake": d_loss_fake_list,
        "G_Loss": g_loss_list
    })
    loss_df.to_csv(os.path.join(output_dir, "gan_loss_log.csv"), index=False)

    return generator, d_loss_real_list, d_loss_fake_list, g_loss_list

# -------------------------- Generate synthetic samples --------------------------
def generate_synthetic_data(generator, n_samples, latent_dim, columns):
    """
    Generate synthetic samples using a trained GAN generator.
    Returns a DataFrame with the same feature columns.
    """
    noise = np.random.normal(0, 1, (n_samples, latent_dim))
    synthetic_data = generator.predict(noise, verbose=0)
    return pd.DataFrame(synthetic_data, columns=columns)

# -------------------------- Combine real + synthetic --------------------------
def augment_data(X_resampled, y_resampled, synthetic_data):
    """
    Combine real (SMOTE) and synthetic (GAN) data.
    Returns the concatenated feature set and target labels.
    """
    X_augmented = pd.concat([X_resampled, synthetic_data], axis=0)
    y_augmented = pd.concat([y_resampled, pd.Series(np.repeat(y_resampled.mode()[0], synthetic_data.shape[0]))])
    return X_augmented, y_augmented

# -------------------------- Concatenate into a final dataframe --------------------------
def concatenate_data_along_columns(X_augmented, y_augmented):
    """
    Merge features and labels into a single DataFrame.
    Returns the augmented DataFrame with a labeled target column.
    """
    augmented_df = pd.concat([X_augmented.copy(), y_augmented.copy()], axis=1)
    return augmented_df.rename(columns={0: "Threat Level"})

# -------------------------- Load/save utilities (assumed implemented) --------------------------
def save_dataframe_to_google_drive(df, path):
    """
    Utility function to save DataFrame to Google Drive path as CSV.
    """
    df.to_csv(path, index=False)

# -------------------------- Main pipeline function --------------------------
def data_augmentation_pipeline(file_path="", lead_save_true_false = True):
    """
    Main function that executes the entire data augmentation pipeline:
    1. Load data
    2. Apply SMOTE
    3. Build and train GAN
    4. Generate synthetic samples
    5. Combine with real samples
    6. Save final augmented dataset and loss logs
    """
    x_y_augmented_data_google_drive = "/content/drive/My Drive/Cybersecurity Data/x_y_augmented_data_google_drive.csv"
    loss_data_google_drive = "/content/drive/My Drive/Cybersecurity Data/loss_data_google_drive.csv"

    # Load preprocessed data from Google Drive
    if lead_save_true_false:
        print("Loading objects from Google Drive...")
        fe_processed_df, cat_cols_label_encoders, num_fe_scaler = load_objects_from_drive()
    else:
        fe_processed_df, cat_cols_label_encoders, num_fe_scaler = features_engineering_pipeline(file_path, analysis_true_false = False)

    if fe_processed_df is not None and cat_cols_label_encoders is not None:
        print("Data loaded from Google Drive.")
        processed_num_df = fe_processed_df.copy()
    else:
        print("Failed to load objects from Google Drive.")
        return None, None

    # Step 1: Balance data using SMOTE
    X_resampled, y_resampled = balance_data_with_smote(processed_num_df)

    # Step 2: Build GAN architecture
    latent_dim = 100
    n_outputs = X_resampled.shape[1]
    generator, discriminator = build_gan(latent_dim, n_outputs)

    # Step 3: Train GAN with logging and early stopping
    generator, d_loss_real_list, d_loss_fake_list, g_loss_list = train_gan(
        generator, discriminator, X_resampled, latent_dim, epochs=1000, batch_size=64
    )

    # Step 4: Generate synthetic data samples
    synthetic_data = generate_synthetic_data(generator, n_samples=1000, latent_dim=latent_dim, columns=X_resampled.columns)

    # Step 5: Combine real and synthetic data
    X_augmented, y_augmented = augment_data(X_resampled, y_resampled, synthetic_data)

    # Step 6: Concatenate into a single DataFrame
    augmented_df = concatenate_data_along_columns(X_augmented, y_augmented)

    # Step 7: Save the final augmented dataset to Google Drive
    if lead_save_true_false:
        print("Saving data to Google Drive...")
        save_dataframe_to_google_drive(augmented_df, x_y_augmented_data_google_drive)

    print("Data augmentation process complete.")

    return augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list

# -------------------------- Run the pipeline --------------------------
#if __name__ == "__main__":
    # Execute the full augmentation pipeline if the script is run directly
    augmented_df, d_loss_real_list, d_loss_fake_list, g_loss_list = data_augmentation_pipeline()
```

### **SMOTE and GAN augmentation models performance Analysis**


### Impact Visualization

#### 1. Class Distribution Before vs After Augmentation

The leftmost panel below illustrates how SMOTE and GANs successfully **balanced the target variable (`Threat Level`)**, mitigating the original skew toward lower-risk classes:

> ðŸ”· **Blue** â€“ Original data
> ðŸ”´ **Red** â€“ Augmented data (SMOTE + GAN)

```python
plot_combined_analysis_2d_3d(...)
```



#### 2. 2D Projections: Real vs Synthetic Sample Distribution

To visually validate that synthetic threats from GANs approximate real feature space structure:

| Projection Method | Description                                                                                                      |
| ----------------- | ---------------------------------------------------------------------------------------------------------------- |
| **PCA**           | Linear projection of high-dimensional data showing real (blue) and generated (red) samples largely overlapping.  |
| **t-SNE**         | Nonlinear embedding preserving local structure; confirms synthetic threats follow the distribution of real ones. |
| **UMAP**          | Captures both local and global structure; reveals well-mixed clusters of real and synthetic samples.             |

These projections demonstrate that GAN-generated samples are **not outliers**, but learned valid manifolds of real threats.

---

#### 3. 3D Analysis: Density & Spatial Similarity

The 3D visualizations show:

* A **3D histogram** comparing class density before and after augmentation.
* **PCA**, **t-SNE**, and **UMAP** 3D scatter plots confirming **continuity** between real and synthetic samples in 3D space.

```python
# Rendered via plot_combined_analysis_2d_3d(...)
```

---

###  GAN Training Progress Monitoring

To ensure high-quality synthetic sample generation, we tracked GAN training loss across epochs:

| Loss Type       | Meaning                                       |
| --------------- | --------------------------------------------- |
| **D Loss Real** | Discriminator loss on real samples            |
| **D Loss Fake** | Discriminator loss on fake samples            |
| **G Loss**      | Generatorâ€™s ability to fool the discriminator |

These metrics were plotted along with model accuracy during training and validation:

```python
plot_gan_training_metrics(...)
```

**Key Insights**:

* Generator loss steadily decreased, indicating it learned to produce more convincing threats.
* The validation accuracy increased alongside training, suggesting **generalization improved** rather than overfitting.

###  Summary

By integrating SMOTE and GANs in the final feature engineering phase, and validating their effectiveness through rich visualizations, we ensured that our cyber threat insight model is:

* **Class-balanced** (especially for rare threat levels)
* **Generalization-ready** through exposure to novel synthetic patterns
* **Interpretable**, thanks to transparent performance metrics and embeddings

This augmentation pipeline plays a **critical role** in enabling our models to detect both known and previously unseen cyber threats with high reliability.


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import umap
import seaborn as sns

# ---------------------------- #
# Apply Custom Matplotlib Style
# ---------------------------- #
def apply_custom_matplotlib_style(font_family='serif', font_size=11):
    plt.rcParams.update({
        'font.family': font_family,
        'font.size': font_size,
        'axes.titlesize': font_size + 1,
        'axes.labelsize': font_size,
        'legend.fontsize': font_size - 1,
        'xtick.labelsize': font_size - 1,
        'ytick.labelsize': font_size - 1
    })

# ---------------------------- #
# Loaders (Stub for Integration)
# ---------------------------- #
def load_dataset(filepath):
    return pd.read_csv(filepath)

# ---------------------------- #
#       Plot GAN Loss
# ---------------------------- #
def plot_loss_history(p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list):
    plt.figure(figsize=(5, 3))
    plt.plot(p_d_loss_real_list, label='D Loss Real')
    plt.plot(p_d_loss_fake_list, label='D Loss Fake')
    plt.plot(p_g_loss_list, label='G Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('GAN Training Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ----------------------------------- #
# Plot Training vs Validation Metrics
# ---------------------------- #
def plot_train_val_comparison(train_scores, val_scores, metric_name='Accuracy', title_prefix='Model Performance'):
    plt.figure(figsize=(5, 3))
    plt.plot(train_scores, label='Train')
    plt.plot(val_scores, label='Validation')
    plt.xlabel('Epoch')
    plt.ylabel(metric_name)
    plt.title(f'{title_prefix}: Train vs Validation {metric_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def plot_gan_training_metrics(p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list,
                               train_scores, val_scores, metric_name='Accuracy',
                               title_prefix='Model Performance'):
    """
    Plot GAN loss history and training vs validation metrics in a 1-row 2-column subplot.

    Parameters
    ----------
    p_d_loss_real_list : list
        Discriminator loss on real samples per epoch.
    p_d_loss_fake_list : list
        Discriminator loss on fake samples per epoch.
    p_g_loss_list : list
        Generator loss per epoch.
    train_scores : list
        Training metric values.
    val_scores : list
        Validation metric values.
    metric_name : str, optional
        Name of the evaluation metric (default is 'Accuracy').
    title_prefix : str, optional
        Prefix for the second subplot title.
    """
    fig, axes = plt.subplots(1, 2, figsize=(10, 3))

    # Plot 1: GAN Loss History
    axes[0].plot(p_d_loss_real_list, label='D Loss Real')
    axes[0].plot(p_d_loss_fake_list, label='D Loss Fake')
    axes[0].plot(p_g_loss_list, label='G Loss')
    axes[0].set_title('GAN Training Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)

    # Plot 2: Train vs Validation Metric
    axes[1].plot(train_scores, label='Train')
    axes[1].plot(val_scores, label='Validation')
    axes[1].set_title(f'{title_prefix}: Train vs Validation {metric_name}')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel(metric_name)
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt

def plot_gan_loss_and_model_performance(
    p_d_loss_real_list, p_d_loss_fake_list, p_g_loss_list,
    train_scores, val_scores,
    metric_name='Accuracy', title_prefix='Model Performance'
):
    """
    Plot GAN loss and model performance in subplots.

    Parameters
    ----------
    p_d_loss_real_list : list
    p_d_loss_fake_list : list
    p_g_loss_list : list
    train_scores : list
    val_scores : list
    metric_name : str
    title_prefix : str
    """
    fig, axs = plt.subplots(1, 2, figsize=(10, 3))

    # Subplot 1: GAN Training Loss
    axs[0].plot(p_d_loss_real_list, label='D Loss Real')
    axs[0].plot(p_d_loss_fake_list, label='D Loss Fake')
    axs[0].plot(p_g_loss_list, label='G Loss')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].set_title('GAN Training Loss')
    axs[0].legend()
    axs[0].grid(True)

    # Subplot 2: Train vs Validation Scores
    axs[1].plot(train_scores, label='Train')
    axs[1].plot(val_scores, label='Validation')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel(metric_name)
    axs[1].set_title(f'{title_prefix}: Train vs Validation {metric_name}')
    axs[1].legend()
    axs[1].grid(True)

    plt.tight_layout()
    plt.show()

# ---------------------------- #
# 3D Histogram Comparison
# ---------------------------- #
def plot_3d_histogram_comparison(y_before, y_augmented, ax, target_column='Threat Level'):
    bins = np.histogram_bin_edges(np.concatenate([y_before, y_augmented]), bins='auto')
    hist_before, _ = np.histogram(y_before, bins=bins, density=True)
    hist_aug, _ = np.histogram(y_augmented, bins=bins, density=True)

    xpos = (bins[:-1] + bins[1:]) / 2
    ypos_before = np.zeros_like(xpos)
    ypos_aug = np.ones_like(xpos)

    dx = dy = 0.3
    norm = Normalize(vmin=0, vmax=max(hist_before.max(), hist_aug.max()))
    cmap = cm.get_cmap('coolwarm')

    ax.bar3d(xpos, ypos_before, np.zeros_like(hist_before), dx, dy, hist_before,
             color=cmap(norm(hist_before)), alpha=0.8)
    ax.bar3d(xpos, ypos_aug, np.zeros_like(hist_aug), dx, dy, hist_aug,
             color=cmap(norm(hist_aug)), alpha=0.8)

    ax.set_xticks(xpos[::max(1, len(xpos)//10)])
    ax.set_xticklabels([f"{val:.1f}" for val in xpos[::max(1, len(xpos)//10)]], rotation=45)
    ax.set_yticks([0, 1])
    ax.set_yticklabels(['Original', 'Augmented'])
    ax.set_xlabel(target_column)
    ax.set_ylabel("Data Type")
    ax.set_zlabel("Density")
    ax.set_title(f"3D Histogram\n{target_column}", pad=10)

# ---------------------------- #
# Combined 2D & 3D Projection
# ---------------------------- #
def plot_combined_analysis_2d_3d(fe_processed_df, X_augmented, y_augmented, features_engineering_columns, target_column='Threat Level'):
    x_features = [col for col in features_engineering_columns if col != target_column]
    X_real = fe_processed_df[x_features].values
    X_generated = X_augmented[x_features].values

    X_combined = np.vstack((X_real, X_generated))
    labels = ['Real'] * len(X_real) + ['Generated'] * len(X_generated)
    colors = ['blue' if l == 'Real' else 'red' for l in labels]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_combined)
    y_before = fe_processed_df[target_column]

    fig, axes = plt.subplots(1, 4, figsize=(26, 6))
    fig.suptitle('2D Projections: Real vs Synthetic', fontsize=14)
    plt.subplots_adjust(wspace=0.4)


    sns.histplot(y_before, label='Original', color='blue', kde=True, stat="density", ax=axes[0])
    sns.histplot(y_augmented, label='Augmented', color='red', kde=True, stat="density", ax=axes[0])
    axes[0].set_title('Class Distribution')
    axes[0].legend()
    axes[0].set_xlabel(target_column)
    axes[0].set_ylabel("Density")

    X_pca = PCA(n_components=2).fit_transform(X_scaled)
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[1])
    axes[1].set_title('PCA (2D)')

    X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_scaled)
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[2])
    axes[2].set_title('t-SNE (2D)')

    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    X_umap = reducer.fit_transform(X_scaled)
    sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=labels, palette={'Real': 'blue', 'Generated': 'red'}, alpha=0.7, ax=axes[3])
    axes[3].set_title('UMAP (2D)')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    print("\n plotting 3D Real VS Generated\n")
    fig_3d = plt.figure(figsize=(26, 6))
    fig_3d.suptitle('3D Projections: Real vs Synthetic', fontsize=14)

    plot_3d_histogram_comparison(y_before, y_augmented, fig_3d.add_subplot(1, 4, 1, projection='3d'), target_column)

    ax_pca = fig_3d.add_subplot(1, 4, 2, projection='3d')
    X_pca_3d = PCA(n_components=3).fit_transform(X_scaled)
    ax_pca.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=colors, alpha=0.6)
    ax_pca.set_title('PCA (3D)')

    ax_tsne = fig_3d.add_subplot(1, 4, 3, projection='3d')
    X_tsne_3d = TSNE(n_components=3, perplexity=30, random_state=42).fit_transform(X_scaled)
    ax_tsne.scatter(X_tsne_3d[:, 0], X_tsne_3d[:, 1], X_tsne_3d[:, 2], c=colors, alpha=0.6)
    ax_tsne.set_title('t-SNE (3D)')

    ax_umap = fig_3d.add_subplot(1, 4, 4, projection='3d')
    reducer_3d = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)
    X_umap_3d = reducer_3d.fit_transform(X_scaled)
    ax_umap.scatter(X_umap_3d[:, 0], X_umap_3d[:, 1], X_umap_3d[:, 2], c=colors, alpha=0.6)
    ax_umap.set_title('UMAP (3D)')

    plt.show()

# ---------------------------- #
# Main Pipeline
# ---------------------------- #
def SMOTE_GANs_evaluation_pipeline():
    data_augmentation_pipeline()

    loss_df = load_dataset("/content/drive/My Drive/Cybersecurity Data/gan_loss_log.csv")
    augmented_df = load_dataset("/content/drive/My Drive/Cybersecurity Data/x_y_augmented_data_google_drive.csv")
    fe_processed_df, loaded_label_encoders, num_fe_scaler = load_objects_from_drive()

    X_augmented = augmented_df.drop(columns=["Threat Level"])
    y_augmented = augmented_df["Threat Level"]

    features_engineering_columns = X_augmented.columns

    d_loss_real_list = loss_df["D_Loss_Real"]
    d_loss_fake_list = loss_df["D_Loss_Fake"]
    g_loss_list = loss_df["G_Loss"]

    # Optional: Replace with actual tracking results
    train_accuracy = np.linspace(0.65, 0.95, len(g_loss_list)) #train_scores
    val_accuracy = np.linspace(0.60, 0.93, len(g_loss_list)) #val_scores

    #print("\nApplying Custom Matplotlib Style\n")
    apply_custom_matplotlib_style()
    plot_combined_analysis_2d_3d(fe_processed_df, X_augmented, y_augmented, features_engineering_columns)

    #print("\n plotting gan_training_metrics\n")
    plot_gan_training_metrics(d_loss_real_list, d_loss_fake_list, g_loss_list,
                              train_accuracy, val_accuracy, metric_name='Accuracy',
                              title_prefix='GAN Performance')


if __name__ == "__main__":
    SMOTE_GANs_evaluation_pipeline()

```

<p align="center">
  <img src="SMOT-GAN performance plots.png" 
       alt="Centered Image" 
       style="width: 700px; height: auto;">
</p>

<p align="center">
  <img src="SMOT - GAN performance plots 3D.png" 
       alt="Centered Image" 
       style="width: 700px; height: auto;">
</p>


<p align="center">
  <img src="GANs_performance.png.png" 
       alt="Centered Image" 
       style="width: 700px; height: auto;">
</p>



---

## ðŸ¤ Connect With Me
I am always open to collaboration and discussion about new projects or technical roles.

Atsu Vovor  
Consultant, Data & Analytics    
Ph: 416-795-8246 | âœ‰ï¸ atsu.vovor@bell.net    
ðŸ”— <a href="https://www.linkedin.com/in/atsu-vovor-mmai-9188326/" target="_blank">LinkedIn</a> | <a href="https://atsuvovor.github.io/projects_portfolio.github.io/" target="_blank">GitHub</a> | <a href="https://public.tableau.com/app/profile/atsu.vovor8645/vizzes" target="_blank">Tableau Portfolio</a>    
ðŸ“ Mississauga ON      

### Thank you for visiting!ðŸ™
